{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f478d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TRAINING-ONLY: Robust, time-aware LightGBM + Optuna pipeline for multiple targets.\n",
    "\n",
    "Outputs per target:\n",
    "- tuning_summary.json: best params, best CV MAE (Optuna)\n",
    "- cv_metrics.csv: per-fold metrics (MAE, RMSE, R2, MAPE, MedAE)\n",
    "- cv_metrics_mean.json: mean/std of metrics across folds\n",
    "- timings.json: timing for tuning and final CV runs (seconds)\n",
    "- feature_importance.csv: LightGBM gain-based importances on final refit (optional)\n",
    "- optuna_study.sqlite (optional): full Optuna study (set SAVE_STUDY=True)\n",
    "\n",
    "Requires:\n",
    "  pip install pandas numpy scikit-learn lightgbm optuna joblib\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error as MAE,\n",
    "    mean_squared_error as MSE,\n",
    "    r2_score as R2,\n",
    "    median_absolute_error as MedAE,\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# =====================\n",
    "# CONFIG\n",
    "# =====================\n",
    "DATE_COL = \"date_utc\"  # used ONLY for time ordering / CV, not as a feature\n",
    "RAW_TIME_COLS_TO_EXCLUDE = [\n",
    "    DATE_COL, \"season\", \"game_datetime_utc\", \"game_datetime_local\",\n",
    "    \"date_home\", \"date_away\"  # in case daily ratings were merged earlier\n",
    "]\n",
    "ID_COLS = [\"game_id\", \"home_team_id\", \"away_team_id\"]\n",
    "CATEGORICAL_COLS = [\"conf_home\", \"conf_away\"]\n",
    "TARGETS = [\n",
    "    \"home_1h\",\"away_1h\",\"home_2h\",\"away_2h\",\"home_score\",\"away_score\",\n",
    "    \"total\",\"1h_total\",\"2h_total\",\"margin\",\"1h_margin\",\"2h_margin\"\n",
    "]\n",
    "\n",
    "# CV / tuning knobs\n",
    "TSCV_SPLITS = 5\n",
    "OPTUNA_TRIALS = 30     # raise for stronger tuning\n",
    "RANDOM_STATE = 42\n",
    "USE_GPU = False        # True if LightGBM GPU build installed\n",
    "NUM_BOOST_ROUND = 20000\n",
    "EARLY_STOPPING_ROUNDS = 200\n",
    "N_JOBS = -1\n",
    "\n",
    "# Outputs\n",
    "OUT_DIR = \"training_reports\"\n",
    "SAVE_STUDY = True\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# =====================\n",
    "# HELPERS (SAFE BY DESIGN)\n",
    "# =====================\n",
    "def _parse_dates(df: pd.DataFrame, date_col: str = DATE_COL) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[date_col] = pd.to_datetime(out[date_col], errors=\"coerce\", utc=True)\n",
    "    return out.loc[out[date_col].notna()].copy()\n",
    "\n",
    "def _make_ohe():\n",
    "    \"\"\"OneHotEncoder compatible with sklearn<=1.3 and >=1.4.\"\"\"\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)  # sklearn >= 1.4\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=True)         # sklearn <= 1.3\n",
    "\n",
    "def _build_feature_frame(df: pd.DataFrame, target: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a frame with ONLY model features + [target, DATE_COL] for CV ordering.\n",
    "    Drops:\n",
    "      - all ID columns\n",
    "      - ALL targets (including the current one)\n",
    "      - any raw time/grouping columns (date_utc, season, etc.)\n",
    "    \"\"\"\n",
    "    exclude = set(ID_COLS) | set(TARGETS) | set(RAW_TIME_COLS_TO_EXCLUDE)\n",
    "    feat_cols = [c for c in df.columns if c not in exclude]\n",
    "    # safety: current target should never slip into features\n",
    "    if target in feat_cols:\n",
    "        feat_cols.remove(target)\n",
    "    # return features + current target + date for time-aware CV\n",
    "    return df[feat_cols + [target, DATE_COL]].copy()\n",
    "\n",
    "def _preprocessor_for_df(X: pd.DataFrame) -> ColumnTransformer:\n",
    "    \"\"\"\n",
    "    Builds a ColumnTransformer from dtypes:\n",
    "      - numeric pipeline gets only numeric dtypes\n",
    "      - categorical pipeline gets the declared CATEGORICAL_COLS (if present)\n",
    "      - anything else (e.g., datetime/object columns not in cat list) is dropped (remainder=\"drop\")\n",
    "    This guarantees no Timestamp columns reach the model.\n",
    "    \"\"\"\n",
    "    cat_cols = [c for c in CATEGORICAL_COLS if c in X.columns]\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.difference(cat_cols).tolist()\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
    "            (\"cat\", _make_ohe(), cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "    pre.set_output(transform=\"default\")\n",
    "    return pre\n",
    "\n",
    "def _suggest_params(trial: optuna.Trial) -> Dict:\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 255, log=True),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 0, 12),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 300, log=True),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 12),\n",
    "        \"n_jobs\": N_JOBS,\n",
    "        \"objective\": \"mae\",\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "    }\n",
    "    if USE_GPU:\n",
    "        params[\"device_type\"] = \"gpu\"   # lightgbm>=4\n",
    "    return params\n",
    "\n",
    "def _fold_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str,float]:\n",
    "    rmse = float(np.sqrt(MSE(y_true, y_pred)))\n",
    "    mae = float(MAE(y_true, y_pred))\n",
    "    r2  = float(R2(y_true, y_pred))\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), 1e-8, None))))\n",
    "    medae = float(MedAE(y_true, y_pred))\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2, \"MAPE\": mape, \"MedAE\": medae}\n",
    "\n",
    "def _summarize_metrics(rows: List[Dict[str,float]]) -> Dict[str,Dict[str,float]]:\n",
    "    keys = rows[0].keys()\n",
    "    arr = {k: np.array([r[k] for r in rows], dtype=float) for k in keys}\n",
    "    return {k: {\"mean\": float(arr[k].mean()), \"std\": float(arr[k].std(ddof=1) if len(arr[k])>1 else 0.0)} for k in keys}\n",
    "\n",
    "\n",
    "# =====================\n",
    "# CORE TRAINING LOGIC\n",
    "# =====================\n",
    "def tune_with_optuna(X: pd.DataFrame, y: pd.Series, dates: pd.Series) -> Tuple[Dict, float, float]:\n",
    "    \"\"\"Tune hyperparameters using time-aware CV (objective: MAE). Returns (best_params, best_score, elapsed_sec).\"\"\"\n",
    "    order = np.argsort(dates.values)\n",
    "    X, y, dates = X.iloc[order], y.iloc[order], dates.iloc[order]\n",
    "\n",
    "    t0 = time.time()\n",
    "    tscv = TimeSeriesSplit(n_splits=TSCV_SPLITS)\n",
    "\n",
    "    def objective(trial: optuna.Trial) -> float:\n",
    "        params = _suggest_params(trial)\n",
    "        fold_maes = []\n",
    "\n",
    "        for tr_idx, va_idx in tscv.split(X):\n",
    "            Xtr, Xva = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "            ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "            pre = _preprocessor_for_df(Xtr)\n",
    "            Xtr_t = pre.fit_transform(Xtr)\n",
    "            Xva_t = pre.transform(Xva)\n",
    "\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators=NUM_BOOST_ROUND)\n",
    "            model.fit(\n",
    "                Xtr_t, ytr,\n",
    "                eval_set=[(Xva_t, yva)],\n",
    "                callbacks=[lgb.early_stopping(EARLY_STOPPING_ROUNDS, verbose=False)],\n",
    "            )\n",
    "            pred = model.predict(Xva_t)\n",
    "            fold_maes.append(MAE(yva, pred))\n",
    "\n",
    "        return float(np.mean(fold_maes))\n",
    "\n",
    "    if SAVE_STUDY:\n",
    "        study = optuna.create_study(\n",
    "            direction=\"minimize\",\n",
    "            storage=f\"sqlite:///{os.path.join(OUT_DIR, 'optuna_study.sqlite')}\",\n",
    "            study_name=\"cbb_study\",\n",
    "            load_if_exists=True\n",
    "        )\n",
    "    else:\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "    study.optimize(objective, n_trials=OPTUNA_TRIALS, show_progress_bar=False)\n",
    "    elapsed = time.time() - t0\n",
    "    return study.best_params, float(study.best_value), float(elapsed)\n",
    "\n",
    "\n",
    "def crossval_report(X: pd.DataFrame, y: pd.Series, dates: pd.Series, best_params: Dict) -> Tuple[pd.DataFrame, Dict, float]:\n",
    "    \"\"\"Run full time-aware CV using best params and return (per_fold_df, summary_json, elapsed_sec).\"\"\"\n",
    "    order = np.argsort(dates.values)\n",
    "    X, y, dates = X.iloc[order], y.iloc[order], dates.iloc[order]\n",
    "\n",
    "    t0 = time.time()\n",
    "    tscv = TimeSeriesSplit(n_splits=TSCV_SPLITS)\n",
    "\n",
    "    fold_rows = []\n",
    "    for fold, (tr_idx, va_idx) in enumerate(tscv.split(X), start=1):\n",
    "        Xtr, Xva = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "        pre = _preprocessor_for_df(Xtr)\n",
    "        Xtr_t = pre.fit_transform(Xtr)\n",
    "        Xva_t = pre.transform(Xva)\n",
    "\n",
    "        model = lgb.LGBMRegressor(**best_params, n_estimators=NUM_BOOST_ROUND, objective=\"mae\", random_state=RANDOM_STATE)\n",
    "        model.fit(\n",
    "            Xtr_t, ytr,\n",
    "            eval_set=[(Xva_t, yva)],\n",
    "            callbacks=[lgb.early_stopping(EARLY_STOPPING_ROUNDS, verbose=False)],\n",
    "        )\n",
    "        ypred = model.predict(Xva_t)\n",
    "        fold_rows.append({\n",
    "            \"fold\": fold,\n",
    "            **_fold_metrics(yva.values, ypred),\n",
    "            \"best_iteration\": int(getattr(model, \"best_iteration_\", model.n_estimators)),\n",
    "            \"n_samples_train\": int(len(tr_idx)),\n",
    "            \"n_samples_val\": int(len(va_idx)),\n",
    "        })\n",
    "\n",
    "    per_fold = pd.DataFrame(fold_rows)\n",
    "    summary = _summarize_metrics(per_fold[[\"MAE\",\"RMSE\",\"R2\",\"MAPE\",\"MedAE\"]].to_dict(orient=\"records\"))\n",
    "    elapsed = time.time() - t0\n",
    "    return per_fold, summary, elapsed\n",
    "\n",
    "\n",
    "def train_targets_training_only(df: pd.DataFrame):\n",
    "    # Ensure time column is parsed\n",
    "    df = _parse_dates(df, DATE_COL)\n",
    "\n",
    "    all_reports = []\n",
    "\n",
    "    for target in TARGETS:\n",
    "        print(f\"\\n==============================\")\n",
    "        print(f\"Training target: {target}\")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        # --- Build leak-safe working frame\n",
    "        work = _build_feature_frame(df, target).dropna(subset=[target]).copy()\n",
    "\n",
    "        # Split into X / y / dates\n",
    "        X = work.drop(columns=[target, DATE_COL])\n",
    "        y = work[target].astype(float)\n",
    "        dates = work[DATE_COL]\n",
    "\n",
    "        # OPTIONAL sanity print (first run): confirm no targets/dates in X\n",
    "        # print(\"n_features:\", X.shape[1])\n",
    "\n",
    "        # --- Tuning with Optuna (time-aware CV)\n",
    "        best_params, best_mae, tune_sec = tune_with_optuna(X, y, dates)\n",
    "        print(f\"Best MAE (Optuna CV) for {target}: {best_mae:.5f}\")\n",
    "\n",
    "        # Persist tuning summary\n",
    "        tgt_dir = os.path.join(OUT_DIR, target)\n",
    "        os.makedirs(tgt_dir, exist_ok=True)\n",
    "        with open(os.path.join(tgt_dir, \"tuning_summary.json\"), \"w\") as f:\n",
    "            json.dump({\"target\": target, \"best_params\": best_params, \"cv_mae\": best_mae, \"tuning_seconds\": tune_sec}, f, indent=2)\n",
    "\n",
    "        # --- Full CV report with best params (richer metrics)\n",
    "        per_fold, summary, cv_sec = crossval_report(X, y, dates, best_params)\n",
    "        per_fold.to_csv(os.path.join(tgt_dir, \"cv_metrics.csv\"), index=False)\n",
    "        with open(os.path.join(tgt_dir, \"cv_metrics_mean.json\"), \"w\") as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        with open(os.path.join(tgt_dir, \"timings.json\"), \"w\") as f:\n",
    "            json.dump({\"tuning_seconds\": tune_sec, \"cv_seconds\": cv_sec}, f, indent=2)\n",
    "\n",
    "        # --- Optional: final refit on full data for importance snapshot\n",
    "        pre = _preprocessor_for_df(X)\n",
    "        X_all = pre.fit_transform(X)\n",
    "        final_model = lgb.LGBMRegressor(\n",
    "            **best_params,\n",
    "            n_estimators=NUM_BOOST_ROUND,\n",
    "            objective=\"mae\",\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "        final_model.fit(X_all, y)  # <-- no early stopping here\n",
    "        try:\n",
    "            importances = final_model.booster_.feature_importance(importance_type=\"gain\")\n",
    "            imp_df = pd.DataFrame({\"feature_index\": np.arange(len(importances)), \"gain_importance\": importances})\n",
    "            imp_df.sort_values(\"gain_importance\", ascending=False).to_csv(os.path.join(tgt_dir, \"feature_importance.csv\"), index=False)\n",
    "        except Exception as e:\n",
    "            with open(os.path.join(tgt_dir, \"feature_importance_error.txt\"), \"w\") as f:\n",
    "                f.write(str(e))\n",
    "        # --- Save final model + preprocessor for inference\n",
    "        import joblib\n",
    "        model_artifact = {\n",
    "            \"preprocessor\": pre,          # ColumnTransformer fitted on training data\n",
    "            \"model\": final_model,         # fitted LightGBMRegressor\n",
    "            \"features\": X.columns.tolist(),\n",
    "            \"best_params\": best_params,\n",
    "            \"target\": target,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        }\n",
    "\n",
    "        joblib.dump(model_artifact, os.path.join(tgt_dir, f\"{target}_final_model.joblib\"))\n",
    "        print(f\"Saved model artifact: {os.path.join(tgt_dir, f'{target}_final_model.joblib')}\")\n",
    "\n",
    "        # Aggregate summary to a master report\n",
    "        report_row = {\n",
    "            \"target\": target,\n",
    "            \"optuna_cv_mae\": best_mae,\n",
    "            \"tuning_seconds\": tune_sec,\n",
    "            \"cv_seconds\": cv_sec,\n",
    "            **{f\"mean_{k}\": v[\"mean\"] for k, v in summary.items()},\n",
    "            **{f\"std_{k}\": v[\"std\"] for k, v in summary.items()},\n",
    "        }\n",
    "        all_reports.append(report_row)\n",
    "\n",
    "    summary_df = pd.DataFrame(all_reports)\n",
    "    summary_path = os.path.join(OUT_DIR, \"overall_summary.csv\")\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"\\nSaved overall training summary to {summary_path}\")\n",
    "    print(\"Per-target reports saved under:\", os.path.abspath(OUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd340e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"s3://collegebasketballinsiders/train/train.csv\", index_col=0)  # your assembled 27k x ~400 frame\n",
    "train_targets_training_only(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97658cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d85861b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAILY GAME IDS\n",
    "\n",
    "- grabs games based on date\n",
    "- date, time, home, away, neutral, conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATE = \"20251103\"\n",
    "csv_files = glob.glob(\"data/boxscores/game-info-2026/*.csv\")\n",
    "combined_df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "\n",
    "team_map = pd.read_csv(\"data/teams/map.csv\")[[\"team\", \"espn\"]]\n",
    "\n",
    "combined_df = combined_df.merge(team_map, left_on=\"home_team\", right_on=\"espn\", how=\"left\").merge(team_map, left_on=\"away_team\", right_on=\"espn\", how=\"left\")\n",
    "\n",
    "combined_df['home'] = combined_df['team_x']\n",
    "combined_df['away'] = combined_df['team_y']\n",
    "\n",
    "combined_df['date'] = pd.to_datetime(combined_df['date_utc']).dt.strftime('%Y%m%d')\n",
    "combined_df['date_key'] = pd.to_numeric(combined_df['date'], errors='coerce').astype('Int64')\n",
    "\n",
    "combined_df['date'] = pd.to_datetime(combined_df['date_utc']).dt.strftime('%Y%m%d')\n",
    "combined_df['date_key'] = pd.to_numeric(combined_df['date'], errors='coerce').astype('Int64')\n",
    "conferences = pd.concat([pd.read_csv(\"barttorvik_2024_all.csv\")[[\"Team\", \"Conf\"]], pd.read_csv(\"barttorvik_2025_all.csv\")[[\"Team\", \"Conf\"]], pd.read_csv(\"barttorvik_2026_all.csv\")[[\"Team\", \"Conf\"]]], axis=0)\n",
    "conferences = conferences[conferences['Team'] != \"Team\"]\n",
    "conferences['Team'] = conferences['Team'].str.extract(r'^([A-Za-z\\s.&]+)')[0].str.strip()\n",
    "conferences = conferences.drop_duplicates(subset=\"Team\")\n",
    "\n",
    "# --- normalize team names (strip seeds/suffixes) ---\n",
    "name_pat = r'^([A-Za-z\\s.&\\'-]+)'\n",
    "def clean_team(s):\n",
    "    if pd.isna(s): return s\n",
    "    m = re.match(name_pat, str(s))\n",
    "    base = m.group(1) if m else str(s)\n",
    "    return re.sub(r'\\s+', ' ', base).strip()\n",
    "\n",
    "combined_df['home_key'] = combined_df['home'].map(clean_team)\n",
    "combined_df['away_key'] = combined_df['away'].map(clean_team)\n",
    "conferences['team_key'] = conferences['Team'].map(clean_team)\n",
    "\n",
    "right = conferences.drop_duplicates(['team_key']).copy()\n",
    "\n",
    "# --- Build HOME version of the right table ---\n",
    "home_cols = [c for c in right.columns if c not in ['date_key', 'team_key']]\n",
    "torvik_home = right.rename(columns={'team_key': 'home_key', **{c: f'{c}_home' for c in home_cols}})\n",
    "\n",
    "# --- Merge HOME ---\n",
    "combined_df = combined_df.merge(\n",
    "    torvik_home,\n",
    "    on='home_key',\n",
    "    how='left',\n",
    "    validate='many_to_one'\n",
    ")\n",
    "\n",
    "# --- Build AWAY version of the right table ---\n",
    "away_cols = [c for c in right.columns if c not in ['date_key', 'team_key']]\n",
    "torvik_away = right.rename(columns={'team_key': 'away_key', **{c: f'{c}_away' for c in away_cols}})\n",
    "\n",
    "# --- Merge AWAY ---\n",
    "combined_df = combined_df.merge(\n",
    "    torvik_away,\n",
    "    on='away_key',\n",
    "    how='left',\n",
    "    validate='many_to_one'\n",
    ")\n",
    "\n",
    "combined_df['season'] = 2026\n",
    "combined_df['neutral_site'] = np.where(combined_df['neutral_site'] == True, 1, 0)\n",
    "\n",
    "combined_df = combined_df[[\"game_id\", \"date\", \"date_key\", \"date_utc\", \"time_utc\", \"neutral_site\", \"home\", \"away\", \"Conf_home\", \"Conf_away\"]]\n",
    "combined_df.columns = [\"game_id\", \"date\", \"date_key\", \"date_utc\", \"time_utc\", \"neutral_site\", \"home\", \"away\", \"conf_home\", \"conf_away\"]\n",
    "\n",
    "csv_files = glob.glob(f\"daily-box-score-ids/{DATE}/*.csv\")\n",
    "game_id_df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "\n",
    "game_ids = list(game_id_df['game_id'])\n",
    "combined_df[combined_df['game_id'].isin(game_ids)].to_csv(\"daily-games/daily.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEASON GAME INFORMATION AND TEAM STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "csv_files = glob.glob(\"data/boxscores/game-info-2026/*.csv\")\n",
    "combined_df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "\n",
    "team_map = pd.read_csv(\"data/teams/map.csv\")[[\"team\", \"espn\"]]\n",
    "\n",
    "combined_df = combined_df.merge(team_map, left_on=\"home_team\", right_on=\"espn\", how=\"left\").merge(team_map, left_on=\"away_team\", right_on=\"espn\", how=\"left\")\n",
    "\n",
    "combined_df['home'] = combined_df['team_x']\n",
    "combined_df['away'] = combined_df['team_y']\n",
    "\n",
    "combined_df = combined_df.dropna(subset=\"home\").dropna(subset=\"away\").dropna(subset=\"home_1h\")\n",
    "combined_df = combined_df[['game_id', 'date_utc', 'time_utc', 'neutral_site', 'home',\n",
    "       'away', 'home_1h', 'away_1h', 'home_2h', 'away_2h', 'home_score',\n",
    "       'away_score']]\n",
    "\n",
    "csv_files = glob.glob(\"data/boxscores/team-stats-2026/*.csv\")\n",
    "team_combined_df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "\n",
    "combined_df.sort_values(\"date_utc\")\n",
    "\n",
    "df = team_combined_df.sort_values(['displayOrder'])\n",
    "\n",
    "# create a helper column to pair home/away rows by game id\n",
    "# (if you don't already have a game_id column)\n",
    "# Split into home and away\n",
    "home_df = df[df['homeAway'] == 'home'].copy()\n",
    "away_df = df[df['homeAway'] == 'away'].copy()\n",
    "\n",
    "# Columns we don't want duplicated (they’ll be renamed anyway)\n",
    "cols_to_remove = ['homeAway', 'displayOrder', 'abbreviation', 'team_id']\n",
    "\n",
    "# Rename columns to indicate home/away\n",
    "home_df = home_df.drop(columns=cols_to_remove).add_suffix('_home')\n",
    "away_df = away_df.drop(columns=cols_to_remove).add_suffix('_away')\n",
    "\n",
    "# Merge back together on the shared game_id\n",
    "# (keep original game_id)\n",
    "final_df = pd.merge(\n",
    "    home_df,\n",
    "    away_df,\n",
    "    left_on='game_id_home',\n",
    "    right_on='game_id_away',\n",
    "    suffixes=('', ''),\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Keep just one copy of game_id\n",
    "final_df['game_id'] = final_df['game_id_home']\n",
    "final_df = final_df.drop(columns=['game_id_home', 'game_id_away'])\n",
    "\n",
    "# Optional: reorder columns to have game_id first\n",
    "cols = ['game_id'] + [c for c in final_df.columns if c != 'game_id']\n",
    "final_df = final_df[cols]\n",
    "\n",
    "final_df = final_df[['game_id', 'assists_home', 'defensiveRebounds_home', 'freeThrowPct_home',\n",
    "       'threePointFieldGoalsMade-threePointFieldGoalsAttempted_home',\n",
    "       'fouls_home', 'totalRebounds_home', 'threePointFieldGoalPct_home',\n",
    "       'teamTurnovers_home', 'pointsInPaint_home', 'technicalFouls_home',\n",
    "       'totalTechnicalFouls_home', 'largestLead_home',\n",
    "       'offensiveRebounds_home', 'fieldGoalPct_home',\n",
    "       'totalTurnovers_home', 'turnoverPoints_home', 'flagrantFouls_home',\n",
    "       'freeThrowsMade-freeThrowsAttempted_home', 'steals_home',\n",
    "       'fieldGoalsMade-fieldGoalsAttempted_home', 'blocks_home',\n",
    "       'fastBreakPoints_home', 'turnovers_home',  'assists_away',\n",
    "       'defensiveRebounds_away', 'freeThrowPct_away',\n",
    "       'threePointFieldGoalsMade-threePointFieldGoalsAttempted_away',\n",
    "       'fouls_away', 'totalRebounds_away', 'threePointFieldGoalPct_away',\n",
    "       'teamTurnovers_away', 'pointsInPaint_away', 'technicalFouls_away',\n",
    "       'totalTechnicalFouls_away', 'largestLead_away',\n",
    "       'offensiveRebounds_away', 'fieldGoalPct_away',\n",
    "       'totalTurnovers_away', 'turnoverPoints_away', 'flagrantFouls_away',\n",
    "       'freeThrowsMade-freeThrowsAttempted_away', 'steals_away',\n",
    "       'fieldGoalsMade-fieldGoalsAttempted_away', 'blocks_away',\n",
    "       'fastBreakPoints_away', 'turnovers_away']]\n",
    "\n",
    "combined_df = combined_df.merge(final_df, on=\"game_id\")\n",
    "\n",
    "csv_files = glob.glob(\"data/boxscores/officials-2026/*.csv\")\n",
    "officials_df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "\n",
    "# create a rank/order number per game_id\n",
    "officials_df['official_number'] = officials_df.groupby('game_id').cumcount() + 1\n",
    "\n",
    "# pivot to wide format\n",
    "flat_officials = (\n",
    "    officials_df.pivot(index='game_id', columns='official_number', values='official_name')\n",
    "    .rename(columns=lambda x: f'official_{x}')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "flat_officials = flat_officials[[\"game_id\", \"official_1\", \"official_2\", \"official_3\"]]\n",
    "\n",
    "combined_df = combined_df.merge(flat_officials, on=\"game_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "team_map = pd.read_csv(\"data/teams/map.csv\")[[\"team\", \"espn\"]]\n",
    "\n",
    "game_df = pd.read_csv(\"data/train/game-info-2026.csv\", index_col=0)\n",
    "game_df['date'] = pd.to_datetime(game_df['date_utc']).dt.strftime('%Y%m%d')\n",
    "game_df['home_margin'] = game_df['home_score'] - game_df['away_score']\n",
    "game_df['away_margin'] = game_df['away_score'] - game_df['home_score']\n",
    "\n",
    "csv_files_2026 = glob.glob(\"daily_csvs_2026/*.csv\")\n",
    "daily_torvik_2026_df = pd.concat((pd.read_csv(f) for f in csv_files_2026), ignore_index=True)\n",
    "daily_torvik_2026_df = daily_torvik_2026_df[daily_torvik_2026_df['Team'] != \"Team\"]\n",
    "daily_torvik_2026_df['Team'] = daily_torvik_2026_df['Team'].str.extract(r'^([A-Za-z\\s.&]+)')[0].str.strip()\n",
    "daily_torvik_2026_df['WAB'] = daily_torvik_2026_df['WAB'].str.replace(\"+\",\"\", regex=False).astype(\"float\")\n",
    "daily_torvik_2026_df['season'] = 2026\n",
    "daily_torvik_2026_df = daily_torvik_2026_df[['season','Date', 'Team', 'Rk', 'Conf', 'G', 'AdjOE', 'AdjDE', 'Barthag',\n",
    "       'EFG%', 'EFGD%', 'TOR', 'TORD', 'ORB', 'DRB', 'FTR', 'FTRD', '2P%',\n",
    "       '2P%D', '3P%', '3P%D', '3PR', '3PRD', 'Adj T.', 'WAB']].sort_values([\"Date\",\"Team\"], ascending=True)\n",
    "daily_torvik_2026_df.columns = ['season', 'date', 'team', 'rank', 'conf', 'games', 'adj_off_eff', 'adj_def_eff', 'barthag',\n",
    "       'efg_pct', 'efgd_pct', 'tor', 'tord', 'orb', 'drb', 'ftr', 'ftrd', 'two_pt_pct',\n",
    "       'two_pt_def_pct', 'three_pt_pct', 'three_pt_def_pct', 'three_pt_rt', 'three_pt_def_rt', 'adj_tempo', 'wab']\n",
    "\n",
    "assert len(set(daily_torvik_2026_df['team']) - set(team_map['team'])) == 0\n",
    "\n",
    "daily_torvik_df = daily_torvik_2026_df\n",
    "\n",
    "\n",
    "import re\n",
    "game_df['date_key'] = pd.to_numeric(game_df['date'], errors='coerce').astype('Int64')\n",
    "daily_torvik_df['date_key'] = pd.to_numeric(daily_torvik_df['date'], errors='coerce').astype('Int64')\n",
    "\n",
    "# --- normalize team names (strip seeds/suffixes) ---\n",
    "name_pat = r'^([A-Za-z\\s.&\\'-]+)'\n",
    "def clean_team(s):\n",
    "    if pd.isna(s): return s\n",
    "    m = re.match(name_pat, str(s))\n",
    "    base = m.group(1) if m else str(s)\n",
    "    return re.sub(r'\\s+', ' ', base).strip()\n",
    "\n",
    "game_df['home_key'] = game_df['home'].map(clean_team)\n",
    "game_df['away_key'] = game_df['away'].map(clean_team)\n",
    "daily_torvik_df['team_key'] = daily_torvik_df['team'].map(clean_team)\n",
    "\n",
    "right = daily_torvik_df.drop_duplicates(['date_key', 'team_key']).copy()\n",
    "\n",
    "# --- Build HOME version of the right table ---\n",
    "home_cols = [c for c in right.columns if c not in ['date_key', 'team_key']]\n",
    "torvik_home = right.rename(columns={'team_key': 'home_key', **{c: f'{c}_home' for c in home_cols}})\n",
    "\n",
    "# --- Merge HOME ---\n",
    "merged_df = game_df.merge(\n",
    "    torvik_home,\n",
    "    on=['date_key', 'home_key'],\n",
    "    how='left',\n",
    "    validate='many_to_one'\n",
    ")\n",
    "\n",
    "# --- Build AWAY version of the right table ---\n",
    "away_cols = [c for c in right.columns if c not in ['date_key', 'team_key']]\n",
    "torvik_away = right.rename(columns={'team_key': 'away_key', **{c: f'{c}_away' for c in away_cols}})\n",
    "\n",
    "# --- Merge AWAY ---\n",
    "merged_df = merged_df.merge(\n",
    "    torvik_away,\n",
    "    on=['date_key', 'away_key'],\n",
    "    how='left',\n",
    "    validate='many_to_one'\n",
    ")\n",
    "\n",
    "merged_df['season'] = 2026\n",
    "merged_df['neutral_site'] = np.where(merged_df['neutral_site'] == True, 1, 0)\n",
    "\n",
    "merged_df = merged_df[[\n",
    "    'game_id',\n",
    "    'season',\n",
    "    'date',\n",
    "    'date_utc',\n",
    "    'time_utc',\n",
    "    'neutral_site',\n",
    "    'home',\n",
    "    'away',\n",
    "    'home_1h',\n",
    "    'away_1h',\n",
    "    'home_2h',\n",
    "    'away_2h',\n",
    "    'home_score',\n",
    "    'away_score',\n",
    "    'home_margin',\n",
    "    'away_margin',\n",
    "    'assists_home',\n",
    "    'fouls_home',\n",
    "    'technicalFouls_home',\n",
    "    'flagrantFouls_home',\n",
    "    'totalRebounds_home',\n",
    "    'offensiveRebounds_home',\n",
    "    'defensiveRebounds_home',\n",
    "    'pointsInPaint_home',\n",
    "    'turnovers_home',\n",
    "    'turnoverPoints_home',\n",
    "    'steals_home',\n",
    "    'blocks_home',\n",
    "    'fastBreakPoints_home',\n",
    "    'assists_away',\n",
    "    'fouls_away',\n",
    "    'technicalFouls_away',\n",
    "    'flagrantFouls_away',\n",
    "    'totalRebounds_away',\n",
    "    'offensiveRebounds_away',\n",
    "    'defensiveRebounds_away',\n",
    "    'pointsInPaint_away',\n",
    "    'turnovers_away',\n",
    "    'turnoverPoints_away',\n",
    "    'steals_away',\n",
    "    'blocks_away',\n",
    "    'fastBreakPoints_away',\n",
    "    'official_1',\n",
    "    'official_2',\n",
    "    'official_3',  \n",
    "    'rank_home',\n",
    "    'conf_home',\n",
    "    'games_home',\n",
    "    'adj_off_eff_home',\n",
    "    'adj_def_eff_home',\n",
    "    'barthag_home',\n",
    "    'efg_pct_home',\n",
    "    'efgd_pct_home',\n",
    "    'tor_home',\n",
    "    'tord_home',\n",
    "    'orb_home',\n",
    "    'drb_home',\n",
    "    'ftr_home',\n",
    "    'ftrd_home',\n",
    "    'two_pt_pct_home',\n",
    "    'two_pt_def_pct_home',\n",
    "    'three_pt_pct_home',\n",
    "    'three_pt_def_pct_home',\n",
    "    'three_pt_rt_home',\n",
    "    'three_pt_def_rt_home',\n",
    "    'adj_tempo_home',\n",
    "    'wab_home',\n",
    "    'rank_away',\n",
    "    'conf_away',\n",
    "    'games_away',\n",
    "    'adj_off_eff_away',\n",
    "    'adj_def_eff_away',\n",
    "    'barthag_away',\n",
    "    'efg_pct_away',\n",
    "    'efgd_pct_away',\n",
    "    'tor_away',\n",
    "    'tord_away',\n",
    "    'orb_away',\n",
    "    'drb_away',\n",
    "    'ftr_away',\n",
    "    'ftrd_away',\n",
    "    'two_pt_pct_away',\n",
    "    'two_pt_def_pct_away',\n",
    "    'three_pt_pct_away',\n",
    "    'three_pt_def_pct_away',\n",
    "    'three_pt_rt_away',\n",
    "    'three_pt_def_rt_away',\n",
    "    'adj_tempo_away',\n",
    "    'wab_away']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SET 1 USING DAILY.CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_time_officials_conference.py\n",
    "from __future__ import annotations\n",
    "import json\n",
    "from typing import Dict, Optional, Iterable, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Configuration ----\n",
    "OFFICIAL_COLS = ['official_1', 'official_2', 'official_3']\n",
    "LOCAL_TZ = 'America/New_York'  # Eastern time\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def _ensure_date_key_str(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalize YYYYMMDD to 8-char string from any input series.\"\"\"\n",
    "    return s.astype(str).str.extract(r'(\\d{8})')[0]\n",
    "\n",
    "def _build_tipoff_utc(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Build timezone-aware UTC datetime from (date_key + time_utc like '21:00Z').\n",
    "    Requires: 'date_key' (YYYYMMDD) and 'time_utc' ('HH:MMZ' or 'HH:MM').\n",
    "    \"\"\"\n",
    "    if 'date_utc' not in df.columns:\n",
    "        raise KeyError(\"Expected 'date_key' (YYYYMMDD).\")\n",
    "    if 'time_utc' not in df.columns:\n",
    "        raise KeyError(\"Expected 'time_utc' like '21:00Z' or '21:00'.\")\n",
    "\n",
    "    # use date_key (not 'date'); coerce invalids to NaT\n",
    "    date_key = _ensure_date_key_str(df['date'])\n",
    "    t = df['time_utc'].astype(str).str.strip()\n",
    "    t = np.where(t.str.endswith('Z'), t, t + 'Z')  # ensure trailing Z\n",
    "    iso_date = pd.to_datetime(date_key, format='%Y%m%d', errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "    iso = iso_date + ' ' + t\n",
    "    tipoff_utc = pd.to_datetime(iso, utc=True, errors='coerce', infer_datetime_format=True)\n",
    "    return tipoff_utc\n",
    "\n",
    "def _time_features_from_dt(dt: pd.Series, prefix: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    From a timezone-aware datetime series, produce:\n",
    "      - {prefix}_hour, {prefix}_minute, {prefix}_second\n",
    "      - {prefix}_seconds_since_midnight\n",
    "      - {prefix}_hour_sin, {prefix}_hour_cos (cyclical)\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame(index=dt.index)\n",
    "    out[f'{prefix}_hour'] = dt.dt.hour.fillna(0).astype('int16')\n",
    "    out[f'{prefix}_minute'] = dt.dt.minute.fillna(0).astype('int16')\n",
    "    out[f'{prefix}_second'] = dt.dt.second.fillna(0).astype('int16')\n",
    "    out[f'{prefix}_seconds_since_midnight'] = (\n",
    "        out[f'{prefix}_hour'] * 3600 + out[f'{prefix}_minute'] * 60 + out[f'{prefix}_second']\n",
    "    ).astype('int32')\n",
    "\n",
    "    two_pi = 2 * np.pi\n",
    "    out[f'{prefix}_hour_sin'] = np.sin(two_pi * out[f'{prefix}_hour'] / 24.0)\n",
    "    out[f'{prefix}_hour_cos'] = np.cos(two_pi * out[f'{prefix}_hour'] / 24.0)\n",
    "    return out\n",
    "\n",
    "def _add_day_flags(local_dt: pd.Series, base_df: pd.DataFrame, prefix: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Day-of-week flags on LOCAL time:\n",
    "      - {prefix}_is_weekend (Sat/Sun)\n",
    "      - {prefix}_is_primetime (18:00–22:59)\n",
    "      - {prefix}_daypart_* one-hots: morning(5–11), afternoon(12–16), evening(17–21), late(other)\n",
    "    \"\"\"\n",
    "    out = base_df.copy()\n",
    "    dow = local_dt.dt.dayofweek  # Mon=0..Sun=6\n",
    "    out[f'{prefix}_is_weekend'] = dow.isin([5, 6]).fillna(False).astype('int8')\n",
    "\n",
    "    hour = local_dt.dt.hour.fillna(0).astype(int)\n",
    "    out[f'{prefix}_is_primetime'] = ((hour >= 18) & (hour <= 22)).astype('int8')\n",
    "\n",
    "    def _daypart(h):\n",
    "        if 5 <= h <= 11:  return 'morning'\n",
    "        if 12 <= h <= 16: return 'afternoon'\n",
    "        if 17 <= h <= 21: return 'evening'\n",
    "        return 'late'\n",
    "\n",
    "    dp = hour.map(_daypart).astype('category')\n",
    "    dummies = pd.get_dummies(dp, prefix=f'{prefix}_daypart', dtype='int8')\n",
    "    out = pd.concat([out, dummies], axis=1)\n",
    "    return out\n",
    "\n",
    "# ---------- Transform (apply saved maps) ----------\n",
    "def transform_officials_with_map(df: pd.DataFrame, mapping: Dict[str, int],\n",
    "                                 official_cols: Iterable[str] = OFFICIAL_COLS) -> pd.DataFrame:\n",
    "    \"\"\"Apply the shared mapping to each official* column, creating *_code columns.\"\"\"\n",
    "    out = df.copy()\n",
    "    unk = mapping.get('UNK', 0)\n",
    "    for c in official_cols:\n",
    "        if c in out.columns:\n",
    "            s = out[c].astype('string')\n",
    "            out[f'{c}_code'] = s.map(mapping).fillna(unk).astype('int32')\n",
    "        else:\n",
    "            out[f'{c}_code'] = unk\n",
    "    return out\n",
    "\n",
    "def transform_with_map(series: pd.Series, mapping: Dict[str, int], fill_value: str = 'UNK') -> pd.Series:\n",
    "    \"\"\"Transform using a prefit mapping, unknowns go to code for fill_value (default 0).\"\"\"\n",
    "    return series.astype('string').fillna(fill_value).map(mapping).fillna(mapping.get(fill_value, 0)).astype('int32')\n",
    "\n",
    "# ---------- Public Inference Entry ----------\n",
    "def load_enc_maps(path: str) -> Dict[str, Dict[str, int]]:\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def build_time_officials_conference_features_inference(\n",
    "    df: pd.DataFrame,\n",
    "    enc_maps: Dict[str, Dict[str, int]],\n",
    "    *,\n",
    "    add_et_features: bool = True,\n",
    "    make_conference_dummies: bool = False\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Dict[str, int]]]:\n",
    "    \"\"\"\n",
    "    INFERENCE version:\n",
    "      - Uses prefit maps in `enc_maps` to transform officials + conferences\n",
    "      - Builds UTC/ET time features + flags\n",
    "      - Does NOT refit any encoder\n",
    "    Returns: (features_df, enc_maps) for convenience\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # Ensure date_key exists for time parsing\n",
    "    if 'date_key' not in out.columns:\n",
    "        if 'date' in out.columns:\n",
    "            out['date_key'] = _ensure_date_key_str(out['date'])\n",
    "        else:\n",
    "            raise KeyError(\"Expected 'date' or 'date_key' in inference dataframe.\")\n",
    "\n",
    "    # Build UTC time + time features\n",
    "    out['tipoff_utc'] = _build_tipoff_utc(out)\n",
    "    utc_feats = _time_features_from_dt(out['tipoff_utc'], prefix='utc')\n",
    "    out = pd.concat([out, utc_feats], axis=1)\n",
    "\n",
    "    # Local (ET) features + day flags\n",
    "    if add_et_features:\n",
    "        tipoff_et = out['tipoff_utc'].dt.tz_convert(LOCAL_TZ)\n",
    "        et_feats = _time_features_from_dt(tipoff_et, prefix='et')\n",
    "        out = pd.concat([out, et_feats], axis=1)\n",
    "        out = _add_day_flags(tipoff_et, out, prefix='et')\n",
    "\n",
    "    # Officials (shared map)\n",
    "    official_map = enc_maps.get('official_map', {'UNK': 0})\n",
    "    out = transform_officials_with_map(out, official_map, OFFICIAL_COLS)\n",
    "\n",
    "    # Conferences\n",
    "    if 'conf_home' in out.columns:\n",
    "        conf_home_map = enc_maps.get('conf_home_map', {'UNK': 0})\n",
    "        out['conf_home_code'] = transform_with_map(out['conf_home'], conf_home_map)\n",
    "        if make_conference_dummies:\n",
    "            dummies = pd.get_dummies(out['conf_home'].astype('string').fillna('UNK'),\n",
    "                                     prefix='conf_home', dtype='int8')\n",
    "            out = pd.concat([out, dummies], axis=1)\n",
    "\n",
    "    if 'conf_away' in out.columns:\n",
    "        conf_away_map = enc_maps.get('conf_away_map', {'UNK': 0})\n",
    "        out['conf_away_code'] = transform_with_map(out['conf_away'], conf_away_map)\n",
    "        if make_conference_dummies:\n",
    "            dummies = pd.get_dummies(out['conf_away'].astype('string').fillna('UNK'),\n",
    "                                     prefix='conf_away', dtype='int8')\n",
    "            out = pd.concat([out, dummies], axis=1)\n",
    "\n",
    "    return out, enc_maps\n",
    "\n",
    "# ---------- Optional: align to training feature set ----------\n",
    "def align_to_training_features(df_features: pd.DataFrame, train_feature_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reindex to the exact training feature set:\n",
    "      - add any missing columns (filled with 0),\n",
    "      - drop any extra columns,\n",
    "      - keep the same ordering as training.\n",
    "    Ensures numeric dtype for all features.\n",
    "    \"\"\"\n",
    "    X = df_features.reindex(columns=train_feature_cols, fill_value=0)\n",
    "    for c in X.columns:\n",
    "        if not np.issubdtype(X[c].dtype, np.number):\n",
    "            X[c] = pd.to_numeric(X[c], errors='coerce').fillna(0)\n",
    "    return X\n",
    "\n",
    "enc_maps = load_enc_maps(\"data/train/enc_maps.json\")\n",
    "\n",
    "# 2) Apply to new games dataframe (must have: 'date' or 'date_key', and 'time_utc')\n",
    "features_1, _ = build_time_officials_conference_features_inference(\n",
    "    pd.read_csv(\"daily-games/daily.csv\"), enc_maps,\n",
    "    add_et_features=True,\n",
    "    make_conference_dummies=True\n",
    ")\n",
    "features_1 = features_1[['game_id', 'utc_seconds_since_midnight', 'utc_hour_sin', 'utc_hour_cos',\n",
    "       'et_hour', 'et_minute', 'et_second', 'et_seconds_since_midnight',\n",
    "       'et_hour_sin', 'et_hour_cos', 'et_is_weekend', 'et_is_primetime',\n",
    "       'et_daypart_afternoon', 'et_daypart_evening', 'et_daypart_late',\n",
    "       'et_daypart_morning', 'official_1_code', 'official_2_code',\n",
    "       'official_3_code', 'conf_home_code', 'conf_away_code']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SET 2/3 USING 2026 GAME INFORMATION AND TORVVIK RATINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- assume your dataframe is named df ---\n",
    "df = merged_df[['game_id', 'season', 'date', 'date_utc', 'time_utc',\n",
    "       'neutral_site', 'home', 'away', 'home_1h', 'away_1h', 'home_2h',\n",
    "       'away_2h', 'home_score', 'away_score', 'home_margin',\n",
    "       'away_margin','rank_home', 'games_home',\n",
    "       'adj_off_eff_home', 'adj_def_eff_home', 'barthag_home',\n",
    "       'efg_pct_home', 'efgd_pct_home', 'tor_home', 'tord_home',\n",
    "       'orb_home', 'drb_home', 'ftr_home', 'ftrd_home', 'two_pt_pct_home',\n",
    "       'two_pt_def_pct_home', 'three_pt_pct_home',\n",
    "       'three_pt_def_pct_home', 'three_pt_rt_home',\n",
    "       'three_pt_def_rt_home', 'adj_tempo_home', 'wab_home', 'rank_away',\n",
    "       'games_away', 'adj_off_eff_away', 'adj_def_eff_away',\n",
    "       'barthag_away', 'efg_pct_away', 'efgd_pct_away', 'tor_away',\n",
    "       'tord_away', 'orb_away', 'drb_away', 'ftr_away', 'ftrd_away',\n",
    "       'two_pt_pct_away', 'two_pt_def_pct_away', 'three_pt_pct_away',\n",
    "       'three_pt_def_pct_away', 'three_pt_rt_away',\n",
    "       'three_pt_def_rt_away', 'adj_tempo_away', 'wab_away']].copy()\n",
    "\n",
    "# --- 1. Create a datetime for chronological sorting ---\n",
    "if 'date_utc' in df.columns:\n",
    "    if 'time_utc' in df.columns:\n",
    "        df['game_dt'] = pd.to_datetime(\n",
    "            df['date_utc'].astype(str).str.strip() + ' ' +\n",
    "            df['time_utc'].fillna('00:00:00').astype(str).str.strip(),\n",
    "            errors='coerce'\n",
    "        )\n",
    "    else:\n",
    "        df['game_dt'] = pd.to_datetime(df['date_utc'], errors='coerce')\n",
    "else:\n",
    "    df['game_dt'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# --- 2. Identify feature bases (shared between home/away) ---\n",
    "suffix_cols = [c for c in df.columns if c.endswith('_home') or c.endswith('_away')]\n",
    "bases = sorted({c.rsplit('_', 1)[0] for c in suffix_cols})\n",
    "\n",
    "# --- 3. Map column names for home/away to team/opponent versions ---\n",
    "home_to_team = {f'{b}_home': b for b in bases}\n",
    "away_to_team = {f'{b}_away': b for b in bases}\n",
    "home_to_opp  = {f'{b}_away': f'opp_{b}' for b in bases}\n",
    "away_to_opp  = {f'{b}_home': f'opp_{b}' for b in bases}\n",
    "\n",
    "# --- 4. Create long-format team-game table (home + away) ---\n",
    "id_cols = ['game_id', 'season', 'game_dt', 'neutral_site']\n",
    "id_cols = [c for c in id_cols if c in df.columns]\n",
    "\n",
    "# home perspective\n",
    "home_view = df[id_cols + ['home', 'away'] + suffix_cols].copy()\n",
    "home_view = home_view.rename(columns={'home': 'team', 'away': 'opponent'})\n",
    "home_view = home_view.rename(columns={**home_to_team, **home_to_opp})\n",
    "home_view['is_home'] = True\n",
    "\n",
    "# away perspective\n",
    "away_view = df[id_cols + ['home', 'away'] + suffix_cols].copy()\n",
    "away_view = away_view.rename(columns={'away': 'team', 'home': 'opponent'})\n",
    "away_view = away_view.rename(columns={**away_to_team, **away_to_opp})\n",
    "away_view['is_home'] = False\n",
    "\n",
    "# combine both\n",
    "team_games = pd.concat([home_view, away_view], ignore_index=True, sort=False)\n",
    "\n",
    "# --- 5. Sort games chronologically per team/season ---\n",
    "team_games = team_games.sort_values(['team', 'season', 'game_dt', 'game_id'], ignore_index=True)\n",
    "\n",
    "team_feature_cols = [c for c in bases if c in team_games.columns]\n",
    "opp_feature_cols  = [f'opp_{b}' for b in bases if f'opp_{b}' in team_games.columns]\n",
    "\n",
    "# 2) Compute current values (NO shift) and store under the same names you use at training (`lag1_*`)\n",
    "#    because for the *next* game these become lag1_*.\n",
    "if team_feature_cols:\n",
    "    current_team_vals = (\n",
    "        team_games.groupby(['team','season'], dropna=False)[team_feature_cols]\n",
    "        .transform(lambda s: s)  # identity, just to keep shape\n",
    "    )\n",
    "    current_team_vals.columns = [f'lag1_{c}' for c in current_team_vals.columns]\n",
    "    team_games = pd.concat([team_games, current_team_vals], axis=1)\n",
    "\n",
    "if opp_feature_cols:\n",
    "    current_opp_vals = (\n",
    "        team_games.groupby(['team','season'], dropna=False)[opp_feature_cols]\n",
    "        .transform(lambda s: s)\n",
    "    )\n",
    "    current_opp_vals.columns = [f'lag1_{c}' for c in current_opp_vals.columns]\n",
    "    team_games = pd.concat([team_games, current_opp_vals], axis=1)\n",
    "\n",
    "# --- B) CURRENT (no-shift) versions of your rolling/EWMA features from build_cbb_features ---\n",
    "\n",
    "def _roll_no_leak(s, w):\n",
    "    return s.rolling(w, min_periods=1).mean()\n",
    "\n",
    "def _rstd_no_leak(s, w=5):\n",
    "    return s.rolling(w, min_periods=2).std()\n",
    "\n",
    "def _ewm_no_leak(s, hl):\n",
    "    return s.ewm(halflife=hl, min_periods=1, adjust=False).mean()\n",
    "\n",
    "# full_bases is what you used in build_cbb_features after standardizing\n",
    "full_bases = [b for b in bases if b in team_games.columns]  # reuse your 'bases' from earlier\n",
    "\n",
    "# windows and halflife consistent with your earlier function\n",
    "windows = (1, 3, 5, 10)\n",
    "ewm_halflife = 5\n",
    "\n",
    "# Compute CURRENT versions (NO shift) but KEEP the SAME column names (ra_*, ra_allowed_*, rstd_*, ewm_*).\n",
    "for b in full_bases:\n",
    "    # ra_<b>_w*\n",
    "    for w in windows:\n",
    "        team_games[f'ra_{b}_w{w}'] = (\n",
    "            team_games.groupby(['team','season'])[b].transform(lambda s: _roll_no_leak(s, w))\n",
    "        )\n",
    "    # rstd_*, ewm_*\n",
    "    team_games[f'rstd_{b}_w5'] = team_games.groupby(['team','season'])[b].transform(lambda s: _rstd_no_leak(s, 5))\n",
    "    team_games[f'ewm_{b}_hl{ewm_halflife}'] = team_games.groupby(['team','season'])[b].transform(lambda s: _ewm_no_leak(s, ewm_halflife))\n",
    "\n",
    "    # Allowed versions from opponent columns if present\n",
    "    opp_b = f'opp_{b}'\n",
    "    if opp_b in team_games.columns:\n",
    "        for w in windows:\n",
    "            team_games[f'ra_allowed_{b}_w{w}'] = (\n",
    "                team_games.groupby(['team','season'])[opp_b].transform(lambda s: _roll_no_leak(s, w))\n",
    "            )\n",
    "        team_games[f'rstd_allowed_{b}_w5'] = team_games.groupby(['team','season'])[opp_b].transform(lambda s: _rstd_no_leak(s, 5))\n",
    "        team_games[f'ewm_allowed_{b}_hl{ewm_halflife}'] = team_games.groupby(['team','season'])[opp_b].transform(lambda s: _ewm_no_leak(s, ewm_halflife))\n",
    "\n",
    "# 1H/2H points current rolling\n",
    "for b in ['points_1h', 'points_2h']:\n",
    "    if b in team_games.columns:\n",
    "        for w in windows:\n",
    "            team_games[f'ra_{b}_w{w}'] = team_games.groupby(['team','season'])[b].transform(lambda s: _roll_no_leak(s, w))\n",
    "        opp_b = f'opp_{b}'\n",
    "        if opp_b in team_games.columns:\n",
    "            for w in windows:\n",
    "                team_games[f'ra_allowed_{b}_w{w}'] = team_games.groupby(['team','season'])[opp_b].transform(lambda s: _roll_no_leak(s, w))\n",
    "\n",
    "# Venue-dependent margin current rolling\n",
    "if 'team_margin' in team_games.columns:\n",
    "    for w in windows:\n",
    "        team_games[f'ra_margin_homeonly_w{w}'] = (\n",
    "            team_games.groupby(['team','season','is_home'])['team_margin']\n",
    "            .transform(lambda s: _roll_no_leak(s.shift(0), w))  # no shift\n",
    "        )\n",
    "    for w in windows:\n",
    "        team_games[f'ra_margin_w{w}'] = team_games.groupby(['team','season'])['team_margin'].transform(lambda s: _roll_no_leak(s, w))\n",
    "\n",
    "# Recent scoring (points for/against) current rolling\n",
    "if 'team_score' in team_games.columns:\n",
    "    for w in windows:\n",
    "        pf = team_games.groupby(['team','season'])['team_score'].transform(lambda s: _roll_no_leak(s, w))\n",
    "        pa = team_games.groupby(['team','season'])['opp_score'].transform(lambda s: _roll_no_leak(s, w))\n",
    "        team_games[f'ra_points_for_w{w}'] = pf\n",
    "        team_games[f'ra_points_against_w{w}'] = pa\n",
    "        team_games[f'ra_point_diff_w{w}'] = pf - pa\n",
    "\n",
    "# Rest days current rolling (rest_days already computed)\n",
    "for w in windows:\n",
    "    team_games[f'ra_rest_days_w{w}'] = (\n",
    "        team_games.groupby(['team','season'])['rest_days'].transform(lambda s: _roll_no_leak(s, w))\n",
    "    )\n",
    "\n",
    "# --- C) Opponent rank cumulative – for NEXT game you want history INCLUDING last opponent\n",
    "#     So use the \"inclusive\" version at the last row.\n",
    "if 'opp_rank' in team_games.columns:\n",
    "    g = team_games.groupby(['team','season'], dropna=False)['opp_rank']\n",
    "    team_games['opp_rank_cummean_incl'] = g.cumsum() / (g.cumcount() + 1)\n",
    "    # Keep your leakage-safe version too if you still need it elsewhere:\n",
    "    cum_sum_prev = g.cumsum().shift(1)\n",
    "    cnt_prev = g.cumcount()\n",
    "    team_games['opp_rank_cummean_pre'] = cum_sum_prev / cnt_prev.replace(0, np.nan)\n",
    "\n",
    "# --- D) Finally, extract the ONE most recent (latest) row per team & season ---\n",
    "latest_snapshot = (\n",
    "    team_games\n",
    "    .sort_values(['team','season','game_dt','game_id'])\n",
    "    .groupby(['team','season'], as_index=False, sort=False)\n",
    "    .tail(1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- E) Select the same feature sets you use downstream ---\n",
    "\n",
    "# features_2-style current snapshot (renamed to lag1_* where relevant)\n",
    "features_2_current = latest_snapshot[['game_id', 'date', 'team', 'opponent'] + [\n",
    "    # team-side lag1_* (already created above as current values but under lag1_*)\n",
    "    'lag1_adj_def_eff','lag1_adj_off_eff','lag1_adj_tempo','lag1_barthag','lag1_drb','lag1_efg_pct',\n",
    "    'lag1_efgd_pct','lag1_ftr','lag1_ftrd','lag1_games','lag1_orb','lag1_rank',\n",
    "    'lag1_three_pt_def_pct','lag1_three_pt_def_rt','lag1_three_pt_pct','lag1_three_pt_rt',\n",
    "    'lag1_tor','lag1_tord','lag1_two_pt_def_pct','lag1_two_pt_pct','lag1_wab',\n",
    "    # opp-side lag1_* (current opponent aggregates)\n",
    "    'lag1_opp_adj_def_eff','lag1_opp_adj_off_eff','lag1_opp_adj_tempo','lag1_opp_barthag','lag1_opp_drb',\n",
    "    'lag1_opp_efg_pct','lag1_opp_efgd_pct','lag1_opp_ftr','lag1_opp_ftrd','lag1_opp_games','lag1_opp_orb',\n",
    "    'lag1_opp_rank','lag1_opp_three_pt_def_pct','lag1_opp_three_pt_def_rt','lag1_opp_three_pt_pct',\n",
    "    'lag1_opp_three_pt_rt','lag1_opp_tor','lag1_opp_tord','lag1_opp_two_pt_def_pct','lag1_opp_two_pt_pct',\n",
    "    'lag1_opp_wab',\n",
    "    # cum means\n",
    "    'opp_rank_cummean_incl','opp_rank_cummean_pre'\n",
    "]].copy()\n",
    "\n",
    "# features_3-style current snapshot (rolling ra_* etc. with NO shift)\n",
    "keep_cols_f3 = ['game_id','date','team','rest_days',\n",
    "    'ra_rest_days_w1','ra_rest_days_w3','ra_rest_days_w5','ra_rest_days_w10',\n",
    "    'ra_assists_w1','ra_allowed_assists_w1','ra_assists_w3','ra_allowed_assists_w3',\n",
    "    'ra_assists_w5','ra_allowed_assists_w5','ra_assists_w10','ra_allowed_assists_w10',\n",
    "    'rstd_assists_w5','ewm_assists_hl5','rstd_allowed_assists_w5','ewm_allowed_assists_hl5',\n",
    "    'ra_blocks_w1','ra_allowed_blocks_w1','ra_blocks_w3','ra_allowed_blocks_w3','ra_blocks_w5',\n",
    "    'ra_allowed_blocks_w5','ra_blocks_w10','ra_allowed_blocks_w10','rstd_blocks_w5','ewm_blocks_hl5',\n",
    "    'rstd_allowed_blocks_w5','ewm_allowed_blocks_hl5',\n",
    "    'ra_defensiveRebounds_w1','ra_allowed_defensiveRebounds_w1','ra_defensiveRebounds_w3',\n",
    "    'ra_allowed_defensiveRebounds_w3','ra_defensiveRebounds_w5','ra_allowed_defensiveRebounds_w5',\n",
    "    'ra_defensiveRebounds_w10','ra_allowed_defensiveRebounds_w10','rstd_defensiveRebounds_w5',\n",
    "    'ewm_defensiveRebounds_hl5','rstd_allowed_defensiveRebounds_w5','ewm_allowed_defensiveRebounds_hl5',\n",
    "    'ra_fastBreakPoints_w1','ra_allowed_fastBreakPoints_w1','ra_fastBreakPoints_w3',\n",
    "    'ra_allowed_fastBreakPoints_w3','ra_fastBreakPoints_w5','ra_allowed_fastBreakPoints_w5',\n",
    "    'ra_fastBreakPoints_w10','ra_allowed_fastBreakPoints_w10','rstd_fastBreakPoints_w5','ewm_fastBreakPoints_hl5',\n",
    "    'rstd_allowed_fastBreakPoints_w5','ewm_allowed_fastBreakPoints_hl5',\n",
    "    'ra_flagrantFouls_w1','ra_allowed_flagrantFouls_w1','ra_flagrantFouls_w3','ra_allowed_flagrantFouls_w3',\n",
    "    'ra_flagrantFouls_w5','ra_allowed_flagrantFouls_w5','ra_flagrantFouls_w10','ra_allowed_flagrantFouls_w10',\n",
    "    'rstd_flagrantFouls_w5','ewm_flagrantFouls_hl5','rstd_allowed_flagrantFouls_w5','ewm_allowed_flagrantFouls_hl5',\n",
    "    'ra_fouls_w1','ra_allowed_fouls_w1','ra_fouls_w3','ra_allowed_fouls_w3','ra_fouls_w5','ra_allowed_fouls_w5',\n",
    "    'ra_fouls_w10','ra_allowed_fouls_w10','rstd_fouls_w5','ewm_fouls_hl5','rstd_allowed_fouls_w5','ewm_allowed_fouls_hl5',\n",
    "    'ra_offensiveRebounds_w1','ra_allowed_offensiveRebounds_w1','ra_offensiveRebounds_w3',\n",
    "    'ra_allowed_offensiveRebounds_w3','ra_offensiveRebounds_w5','ra_allowed_offensiveRebounds_w5',\n",
    "    'ra_offensiveRebounds_w10','ra_allowed_offensiveRebounds_w10','rstd_offensiveRebounds_w5',\n",
    "    'ewm_offensiveRebounds_hl5','rstd_allowed_offensiveRebounds_w5','ewm_allowed_offensiveRebounds_hl5',\n",
    "    'ra_pointsInPaint_w1','ra_allowed_pointsInPaint_w1','ra_pointsInPaint_w3','ra_allowed_pointsInPaint_w3',\n",
    "    'ra_pointsInPaint_w5','ra_allowed_pointsInPaint_w5','ra_pointsInPaint_w10','ra_allowed_pointsInPaint_w10',\n",
    "    'rstd_pointsInPaint_w5','ewm_pointsInPaint_hl5','rstd_allowed_pointsInPaint_w5','ewm_allowed_pointsInPaint_hl5',\n",
    "    'ra_steals_w1','ra_allowed_steals_w1','ra_steals_w3','ra_allowed_steals_w3','ra_steals_w5','ra_allowed_steals_w5',\n",
    "    'ra_steals_w10','ra_allowed_steals_w10','rstd_steals_w5','ewm_steals_hl5','rstd_allowed_steals_w5','ewm_allowed_steals_hl5',\n",
    "    'ra_technicalFouls_w1','ra_allowed_technicalFouls_w1','ra_technicalFouls_w3','ra_allowed_technicalFouls_w3',\n",
    "    'ra_technicalFouls_w5','ra_allowed_technicalFouls_w5','ra_technicalFouls_w10','ra_allowed_technicalFouls_w10',\n",
    "    'rstd_technicalFouls_w5','ewm_technicalFouls_hl5','rstd_allowed_technicalFouls_w5','ewm_allowed_technicalFouls_hl5',\n",
    "    'ra_totalRebounds_w1','ra_allowed_totalRebounds_w1','ra_totalRebounds_w3','ra_allowed_totalRebounds_w3',\n",
    "    'ra_totalRebounds_w5','ra_allowed_totalRebounds_w5','ra_totalRebounds_w10','ra_allowed_totalRebounds_w10',\n",
    "    'rstd_totalRebounds_w5','ewm_totalRebounds_hl5','rstd_allowed_totalRebounds_w5','ewm_allowed_totalRebounds_hl5',\n",
    "    'ra_turnoverPoints_w1','ra_allowed_turnoverPoints_w1','ra_turnoverPoints_w3','ra_allowed_turnoverPoints_w3',\n",
    "    'ra_turnoverPoints_w5','ra_allowed_turnoverPoints_w5','ra_turnoverPoints_w10','ra_allowed_turnoverPoints_w10',\n",
    "    'rstd_turnoverPoints_w5','ewm_turnoverPoints_hl5','rstd_allowed_turnoverPoints_w5','ewm_allowed_turnoverPoints_hl5',\n",
    "    'ra_turnovers_w1','ra_allowed_turnovers_w1','ra_turnovers_w3','ra_allowed_turnovers_w3','ra_turnovers_w5',\n",
    "    'ra_allowed_turnovers_w5','ra_turnovers_w10','ra_allowed_turnovers_w10','rstd_turnovers_w5','ewm_turnovers_hl5',\n",
    "    'rstd_allowed_turnovers_w5','ewm_allowed_turnovers_hl5',\n",
    "    'ra_points_1h_w1','ra_points_1h_w3','ra_points_1h_w5','ra_points_1h_w10',\n",
    "    'ra_allowed_points_1h_w1','ra_allowed_points_1h_w3','ra_allowed_points_1h_w5','ra_allowed_points_1h_w10',\n",
    "    'ra_points_2h_w1','ra_points_2h_w3','ra_points_2h_w5','ra_points_2h_w10',\n",
    "    'ra_allowed_points_2h_w1','ra_allowed_points_2h_w3','ra_allowed_points_2h_w5','ra_allowed_points_2h_w10',\n",
    "    'ra_margin_homeonly_w1','ra_margin_homeonly_w3','ra_margin_homeonly_w5','ra_margin_homeonly_w10',\n",
    "    'ra_points_for_w1','ra_points_against_w1','ra_point_diff_w1',\n",
    "    'ra_points_for_w3','ra_points_against_w3','ra_point_diff_w3',\n",
    "    'ra_points_for_w5','ra_points_against_w5','ra_point_diff_w5',\n",
    "    'ra_points_for_w10','ra_points_against_w10','ra_point_diff_w10',\n",
    "    'ra_margin_w1','ra_margin_w3','ra_margin_w5','ra_margin_w10'\n",
    "]\n",
    "# keep only columns present\n",
    "keep_cols_f3 = [c for c in keep_cols_f3 if c in latest_snapshot.columns]\n",
    "features_3_current = latest_snapshot[keep_cols_f3].copy()\n",
    "\n",
    "# --- F) Optional: if you want exactly ONE per team (across seasons), keep the latest overall ---\n",
    "# (You already did something similar with sort_values+drop_duplicates)\n",
    "features_2_current = features_2_current.sort_values([\"date\",\"team\"]).drop_duplicates(subset=\"team\", keep=\"last\")\n",
    "features_3_current = features_3_current.sort_values([\"date\",\"team\"]).drop_duplicates(subset=\"team\", keep=\"last\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFERENCE DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_info = pd.read_csv(\"daily-games/daily.csv\")\n",
    "game_info = game_info.merge(features_1, on=\"game_id\", how=\"left\")\n",
    "\n",
    "home_merge = features_2.copy()\n",
    "home_merge = home_merge.rename(columns=lambda c: f\"{c}_home\" if c not in [\"game_id\", \"team\", \"opponent\"] else c)\n",
    "merged_home = game_info.merge(\n",
    "    home_merge,\n",
    "    left_on=[\"game_id\", \"home\"],\n",
    "    right_on=[\"game_id\", \"team\"],\n",
    "    how=\"left\",\n",
    "    validate=\"1:1\"\n",
    ").drop(columns=[\"team\", \"opponent\"])\n",
    "\n",
    "# --- AWAY TEAM MERGE ---\n",
    "away_merge = features_2.copy()\n",
    "away_merge = away_merge.rename(columns=lambda c: f\"{c}_away\" if c not in [\"game_id\", \"team\", \"opponent\"] else c)\n",
    "game_info = merged_home.merge(\n",
    "    away_merge,\n",
    "    left_on=[\"game_id\", \"away\"],\n",
    "    right_on=[\"game_id\", \"team\"],\n",
    "    how=\"left\",\n",
    "    validate=\"1:1\"\n",
    ").drop(columns=[\"team\", \"opponent\"])\n",
    "\n",
    "key_cols = ['game_id', 'team']\n",
    "\n",
    "# --- HOME merge ---\n",
    "home_feats = features_3.copy()\n",
    "home_feats = home_feats.rename(columns=lambda c: f\"{c}_home\" if c not in key_cols else c)\n",
    "\n",
    "out = game_info.merge(\n",
    "    home_feats,\n",
    "    left_on=['game_id', 'home'],\n",
    "    right_on=['game_id', 'team'],\n",
    "    how='left',\n",
    "    validate='1:1'\n",
    ").drop(columns=['team'])\n",
    "\n",
    "# --- AWAY merge ---\n",
    "away_feats = features_3.copy()\n",
    "away_feats = away_feats.rename(columns=lambda c: f\"{c}_away\" if c not in key_cols else c)\n",
    "\n",
    "game_info = out.merge(\n",
    "    away_feats,\n",
    "    left_on=['game_id', 'away'],\n",
    "    right_on=['game_id', 'team'],\n",
    "    how='left',\n",
    "    validate='1:1'\n",
    ").drop(columns=['team'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03eca27aa3e5b0c2bf98348f6751bc7dc08663828d24c367008019d5f5934307"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

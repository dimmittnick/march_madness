{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  20251103: saved 225 rows to daily_csvs_2026/barttorvik_20251103.csv\n",
      "✔️  20251104: saved 267 rows to daily_csvs_2026/barttorvik_20251104.csv\n",
      "✔️  20251105: saved 284 rows to daily_csvs_2026/barttorvik_20251105.csv\n",
      "✔️  20251106: saved 284 rows to daily_csvs_2026/barttorvik_20251106.csv\n",
      "\n",
      "✅ Done! 1060 total rows saved across days.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Daily Barttovik Ratings ----------\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright, TimeoutError as PWTimeout\n",
    "\n",
    "BASE_URL = (\n",
    "    \"https://barttorvik.com/trank.php\"\n",
    "    \"?year={year}&sort=&hteam=&t2value=&conlimit=All&state=All\"\n",
    "    \"&begin={begin}&end={end}&top=0&revquad=0&quad=5&venue=All&type=All&mingames=0#\"\n",
    ")\n",
    "\n",
    "# ---------- HTML fetch ----------\n",
    "\n",
    "async def goto_and_get_html(page, url: str, table_selector: str = \"table\", timeout_ms: int = 30000) -> str:\n",
    "    await page.goto(url, wait_until=\"domcontentloaded\", timeout=timeout_ms)\n",
    "    try:\n",
    "        await page.wait_for_selector(table_selector, timeout=20000)\n",
    "    except PWTimeout:\n",
    "        await page.wait_for_load_state(\"networkidle\", timeout=10000)\n",
    "\n",
    "    # if we’re still on the verification page, wait a bit\n",
    "    for _ in range(6):\n",
    "        html = await page.content()\n",
    "        if \"Verifying your browser\" not in html and \"js_test_submitted\" not in html:\n",
    "            break\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "    return await page.content()\n",
    "\n",
    "# ---------- Table parsing ----------\n",
    "\n",
    "def parse_first_table(html: str) -> List[List[str]]:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.select_one(\"table\")\n",
    "    if not table:\n",
    "        return []\n",
    "    rows = []\n",
    "    for tr in table.select(\"tr\"):\n",
    "        cells = [c.get_text(strip=True) for c in tr.select(\"th, td\")]\n",
    "        if cells:\n",
    "            rows.append(cells)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def rows_to_dataframe(rows: List[List[str]]) -> pd.DataFrame:\n",
    "    \"\"\"Convert raw scraped rows into a DataFrame\"\"\"\n",
    "    if len(rows) < 3:\n",
    "        return pd.DataFrame()\n",
    "    columns = rows[1]  # second row = headers\n",
    "    data = rows[2:]\n",
    "    max_len = len(columns)\n",
    "    norm = [r[:max_len] + ([\"\"] * (max_len - len(r))) for r in data]\n",
    "    df = pd.DataFrame(norm, columns=columns)\n",
    "    return df\n",
    "\n",
    "# ---------- Orchestrator ----------\n",
    "\n",
    "async def scrape_barttorvik_daily(\n",
    "    year: int = 2021,\n",
    "    begin: str = \"20201101\",\n",
    "    end: str = \"20210313\",\n",
    "    output_dir: str = \"daily_csvs\",\n",
    "    master_csv: str = \"barttorvik_2021_all.csv\",\n",
    "    table_selector: str = \"table\",\n",
    "    headless: bool = True,\n",
    "    pause_sec: float = 3.8\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    start_dt = datetime.strptime(begin, \"%Y%m%d\")\n",
    "    final_dt = datetime.strptime(end, \"%Y%m%d\")\n",
    "\n",
    "    first_write = not os.path.exists(master_csv)\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(channel=\"chrome\", headless=headless)\n",
    "        context = await browser.new_context(\n",
    "            user_agent=(\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                        \"Chrome/119.0.0.0 Safari/537.36\"),\n",
    "            locale=\"en-US\",\n",
    "            timezone_id=\"America/New_York\",\n",
    "        )\n",
    "        page = await context.new_page()\n",
    "\n",
    "        dt = start_dt\n",
    "        total_rows = 0\n",
    "        while dt <= final_dt:\n",
    "            end_str = dt.strftime(\"%Y%m%d\")\n",
    "            url = BASE_URL.format(year=year, begin=begin, end=end_str)\n",
    "\n",
    "            try:\n",
    "                html = await goto_and_get_html(page, url, table_selector=table_selector)\n",
    "                rows = parse_first_table(html)\n",
    "                df = rows_to_dataframe(rows)\n",
    "\n",
    "                if not df.empty:\n",
    "                    df.insert(0, \"Date\", end_str)\n",
    "\n",
    "                    # write individual daily file\n",
    "                    daily_path = os.path.join(output_dir, f\"barttorvik_{end_str}.csv\")\n",
    "                    df.to_csv(daily_path, index=False)\n",
    "                    print(f\"✔️  {end_str}: saved {len(df)} rows to {daily_path}\")\n",
    "\n",
    "                    # append to master CSV\n",
    "                    if first_write:\n",
    "                        df.to_csv(master_csv, index=False)\n",
    "                        first_write = False\n",
    "                    else:\n",
    "                        df.to_csv(master_csv, mode=\"a\", header=False, index=False)\n",
    "\n",
    "                    total_rows += len(df)\n",
    "                else:\n",
    "                    print(f\"⚠️  {end_str}: no data (empty table)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {end_str}: ERROR {e}\")\n",
    "\n",
    "            await asyncio.sleep(pause_sec)\n",
    "            dt += timedelta(days=1)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    print(f\"\\n✅ Done! {total_rows} total rows saved across days.\")\n",
    "\n",
    "# ---------- Run ----------\n",
    "# In Jupyter or async environment:\n",
    "await scrape_barttorvik_daily(\n",
    "    year=2026,\n",
    "    begin=\"20251103\",\n",
    "    end=\"20251106\",\n",
    "    output_dir=\"daily_csvs_2026\",\n",
    "    master_csv=\"barttorvik_2026_all.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️  20251103: daily-box-score-ids/20251103/game_ids.csv exists, skipping (set OVERWRITE=True to redo)\n",
      "⏭️  20251104: daily-box-score-ids/20251104/game_ids.csv exists, skipping (set OVERWRITE=True to redo)\n",
      "⏭️  20251105: daily-box-score-ids/20251105/game_ids.csv exists, skipping (set OVERWRITE=True to redo)\n",
      "✅ 20251106: saved 44 games → daily-box-score-ids/20251106/game_ids.csv and daily-box-score-ids/20251106/game_ids.txt\n",
      "\n",
      "Done. Wrote 44 games across 1 days into 'daily-box-score-ids/<YYYYMMDD>/' folders.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Game Ids (dated subfolders) ----------\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "START_DATE = \"20251103\"  # inclusive\n",
    "END_DATE   = \"20251106\"  # inclusive\n",
    "GROUP = 50               # 50 = NCAA Division I\n",
    "OUT_DIR = \"daily-box-score-ids\"\n",
    "OVERWRITE = False        # True to overwrite existing daily files\n",
    "PAUSE_SECONDS = 3.4      # be polite (optional)\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/119.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "def fetch_games_api(date_yyyymmdd: str, group: int = GROUP) -> pd.DataFrame:\n",
    "    \"\"\"Preferred: ESPN public JSON API (no HTML parsing).\"\"\"\n",
    "    url = (\n",
    "        \"https://site.api.espn.com/apis/v2/sports/basketball/mens-college-basketball/\"\n",
    "        f\"scoreboard?dates={date_yyyymmdd}&groups={group}\"\n",
    "    )\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    rows = []\n",
    "    for e in data.get(\"events\", []):\n",
    "        gid = e.get(\"id\")\n",
    "        comp = (e.get(\"competitions\") or [{}])[0]\n",
    "        comps = comp.get(\"competitors\", [])\n",
    "        home = next((c for c in comps if c.get(\"homeAway\") == \"home\"), {})\n",
    "        away = next((c for c in comps if c.get(\"homeAway\") == \"away\"), {})\n",
    "        rows.append({\n",
    "            \"game_id\": gid,\n",
    "            \"home_team\": (home.get(\"team\") or {}).get(\"displayName\"),\n",
    "            \"away_team\": (away.get(\"team\") or {}).get(\"displayName\"),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def fetch_scoreboard_html(date_yyyymmdd: str, group: int = GROUP) -> str:\n",
    "    \"\"\"Fallback: fetch the scoreboard HTML for the date/group.\"\"\"\n",
    "    url = f\"https://www.espn.com/mens-college-basketball/scoreboard/_/date/{date_yyyymmdd}/group/{group}\"\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def parse_games_from_html(html: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse game blocks from server-rendered DOM.\n",
    "    <section class=\"Scoreboard\" id=\"<game_id>\">…</section>\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    games = []\n",
    "\n",
    "    # Primary: section blocks with game id\n",
    "    for sec in soup.select(\"section.Scoreboard[id]\"):\n",
    "        gid = sec.get(\"id\")\n",
    "        away = sec.select_one(\".ScoreboardScoreCell__Item--away .ScoreCell__TeamName--shortDisplayName\")\n",
    "        home = sec.select_one(\".ScoreboardScoreCell__Item--home .ScoreCell__TeamName--shortDisplayName\")\n",
    "        games.append({\n",
    "            \"game_id\": gid,\n",
    "            \"home_team\": home.get_text(strip=True) if home else None,\n",
    "            \"away_team\": away.get_text(strip=True) if away else None,\n",
    "        })\n",
    "\n",
    "    # Secondary: backup to anchor pattern if nothing found\n",
    "    if not games:\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            m = re.search(r\"/mens-college-basketball/game/_/gameId/(\\d+)\", a[\"href\"])\n",
    "            if m:\n",
    "                games.append({\"game_id\": m.group(1), \"home_team\": None, \"away_team\": None})\n",
    "\n",
    "    return pd.DataFrame(games)\n",
    "\n",
    "def get_games_for_date(date_yyyymmdd: str, group: int = GROUP) -> pd.DataFrame:\n",
    "    \"\"\"Try API first; if empty/error, fall back to HTML.\"\"\"\n",
    "    try:\n",
    "        df_api = fetch_games_api(date_yyyymmdd, group=group)\n",
    "        if not df_api.empty:\n",
    "            df_api.insert(0, \"date\", date_yyyymmdd)\n",
    "            return df_api\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        html = fetch_scoreboard_html(date_yyyymmdd, group=group)\n",
    "        df_html = parse_games_from_html(html)\n",
    "        if not df_html.empty:\n",
    "            df_html.insert(0, \"date\", date_yyyymmdd)\n",
    "        return df_html\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def daterange(start_yyyymmdd: str, end_yyyymmdd: str):\n",
    "    start = datetime.strptime(start_yyyymmdd, \"%Y%m%d\")\n",
    "    end = datetime.strptime(end_yyyymmdd, \"%Y%m%d\")\n",
    "    d = start\n",
    "    while d <= end:\n",
    "        yield d.strftime(\"%Y%m%d\")\n",
    "        d += timedelta(days=1)\n",
    "\n",
    "def main():\n",
    "    total_days = 0\n",
    "    total_games = 0\n",
    "\n",
    "    for day in daterange(START_DATE, END_DATE):\n",
    "        # Make a dated subfolder like: daily-box-score-ids/20251103/\n",
    "        day_dir = os.path.join(OUT_DIR, day)\n",
    "        os.makedirs(day_dir, exist_ok=True)\n",
    "\n",
    "        # Files we’ll write inside that folder\n",
    "        csv_path = os.path.join(day_dir, \"game_ids.csv\")\n",
    "        txt_path = os.path.join(day_dir, \"game_ids.txt\")\n",
    "\n",
    "        if (not OVERWRITE) and os.path.exists(csv_path):\n",
    "            print(f\"⏭️  {day}: {csv_path} exists, skipping (set OVERWRITE=True to redo)\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = get_games_for_date(day, group=GROUP)\n",
    "            if df is None or df.empty:\n",
    "                print(f\"— {day}: no games\")\n",
    "            else:\n",
    "                # Normalize columns/order\n",
    "                df = df[[\"date\", \"game_id\", \"home_team\", \"away_team\"]]\n",
    "\n",
    "                # Write CSV in dated folder\n",
    "                df.to_csv(csv_path, index=False)\n",
    "\n",
    "                # Also write a plain text list of IDs (one per line)\n",
    "                with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    for gid in df[\"game_id\"].astype(str):\n",
    "                        f.write(gid + \"\\n\")\n",
    "\n",
    "                total_days += 1\n",
    "                total_games += len(df)\n",
    "                print(f\"✅ {day}: saved {len(df)} games → {csv_path} and {txt_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {day}: ERROR {e}\")\n",
    "\n",
    "        # be polite to the server\n",
    "        if PAUSE_SECONDS:\n",
    "            try:\n",
    "                time.sleep(PAUSE_SECONDS)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    print(f\"\\nDone. Wrote {total_games} games across {total_days} days into '{OUT_DIR}/<YYYYMMDD>/' folders.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "# ---------- Game Box Scores ----------\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "SUMMARY_URL = \"https://site.api.espn.com/apis/site/v2/sports/basketball/mens-college-basketball/summary?event={gid}\"\n",
    "UA_HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# ----------------------------\n",
    "# Fetch\n",
    "# ----------------------------\n",
    "def _get_summary(game_id: str) -> Dict[str, Any]:\n",
    "    r = requests.get(SUMMARY_URL.format(gid=game_id), headers=UA_HEADERS, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _first_comp(summary: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    return (summary.get(\"header\", {}).get(\"competitions\") or [{}])[0]\n",
    "\n",
    "def _competitors(summary: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    return _first_comp(summary).get(\"competitors\") or []\n",
    "\n",
    "def _home_comp(comp_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    return next((c for c in comp_list if c.get(\"homeAway\") == \"home\"), comp_list[0] if comp_list else {})\n",
    "\n",
    "def _away_comp(comp_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    return next((c for c in comp_list if c.get(\"homeAway\") == \"away\"), comp_list[1] if len(comp_list) > 1 else {})\n",
    "\n",
    "def _team_name(team_obj: Dict[str, Any]) -> str:\n",
    "    return team_obj.get(\"displayName\") or team_obj.get(\"name\") or team_obj.get(\"location\") or \"\"\n",
    "\n",
    "def _parse_value_to_int(val: Any) -> Optional[int]:\n",
    "    if val is None:\n",
    "        return None\n",
    "    try:\n",
    "        return int(val)\n",
    "    except Exception:\n",
    "        try:\n",
    "            # sometimes \"42\" or \"42.0\"\n",
    "            return int(str(val).split(\".\")[0])\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def _extract_period_num(item: Dict[str, Any]) -> Optional[int]:\n",
    "    # supports {\"period\": 1} or {\"period\": {\"number\": 1}}\n",
    "    p = item.get(\"period\")\n",
    "    if isinstance(p, dict):\n",
    "        return _parse_value_to_int(p.get(\"number\"))\n",
    "    return _parse_value_to_int(p)\n",
    "\n",
    "def _extract_score_from_item(item: Dict[str, Any]) -> Optional[int]:\n",
    "    # common shapes: {\"value\": 42} or {\"displayValue\": \"42\"} or {\"score\": 42}\n",
    "    for k in (\"value\", \"displayValue\", \"score\"):\n",
    "        if k in item:\n",
    "            return _parse_value_to_int(item[k])\n",
    "    return None\n",
    "\n",
    "def _half_scores(competitor: Dict[str, Any]) -> Tuple[Optional[int], Optional[int]]:\n",
    "    \"\"\"\n",
    "    Robust 1H/2H extraction from competitor lines:\n",
    "      - supports 'linescores' OR 'scoreByPeriod'\n",
    "      - supports period number under 'period' or 'period.number'\n",
    "      - supports score value under 'value', 'displayValue', or 'score'\n",
    "      - if no period numbers, falls back to first two items in order\n",
    "    \"\"\"\n",
    "    lines = competitor.get(\"linescores\") or competitor.get(\"scoreByPeriod\") or []\n",
    "    if not isinstance(lines, list) or not lines:\n",
    "        return (None, None)\n",
    "\n",
    "    by_period: Dict[int, int] = {}\n",
    "    fallback_order_vals: List[int] = []\n",
    "\n",
    "    for it in lines:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        val = _extract_score_from_item(it)\n",
    "        if val is not None:\n",
    "            fallback_order_vals.append(val)\n",
    "        pnum = _extract_period_num(it)\n",
    "        if pnum is not None and val is not None:\n",
    "            by_period[pnum] = val\n",
    "\n",
    "    # Prefer explicit period numbers 1 and 2\n",
    "    h1 = by_period.get(1)\n",
    "    h2 = by_period.get(2)\n",
    "    if h1 is not None or h2 is not None:\n",
    "        return h1, h2\n",
    "\n",
    "    # Fall back to first two entries by order\n",
    "    if len(fallback_order_vals) >= 2:\n",
    "        return fallback_order_vals[0], fallback_order_vals[1]\n",
    "\n",
    "    return (None, None)\n",
    "\n",
    "def _write_csv(path: str, rows: List[Dict[str, Any]], header: List[str] = None):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    if not rows:\n",
    "        if header:\n",
    "            with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                csv.DictWriter(f, fieldnames=header).writeheader()\n",
    "        else:\n",
    "            open(path, \"w\").close()\n",
    "        return\n",
    "    cols = header or list({k for r in rows for k in r.keys()})\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=cols)\n",
    "        w.writeheader()\n",
    "        w.writerows(rows)\n",
    "\n",
    "# ----------------------------\n",
    "# Parsers (summary-only)\n",
    "# ----------------------------\n",
    "def parse_game_info(summary: Dict[str, Any], game_id: str) -> List[Dict[str, Any]]:\n",
    "    header = summary.get(\"header\", {}) or {}\n",
    "    comp = _first_comp(summary)\n",
    "    comps = _competitors(summary)\n",
    "\n",
    "    home = _home_comp(comps)\n",
    "    away = _away_comp(comps)\n",
    "\n",
    "    # date/time (UTC ISO)\n",
    "    dt_iso = (header.get(\"competitions\", [{}])[0].get(\"date\")\n",
    "              or header.get(\"date\")\n",
    "              or \"\")\n",
    "    date_utc, time_utc = \"\", \"\"\n",
    "    if \"T\" in dt_iso:\n",
    "        date_utc, rest = dt_iso.split(\"T\", 1)\n",
    "        time_utc = rest\n",
    "    else:\n",
    "        date_utc = dt_iso\n",
    "\n",
    "    # final scores\n",
    "    home_score = _parse_value_to_int(home.get(\"score\"))\n",
    "    away_score = _parse_value_to_int(away.get(\"score\"))\n",
    "\n",
    "    # halves\n",
    "    home_1h, home_2h = _half_scores(home)\n",
    "    away_1h, away_2h = _half_scores(away)\n",
    "\n",
    "    row = {\n",
    "        \"game_id\": game_id,\n",
    "        \"date_utc\": date_utc,\n",
    "        \"time_utc\": time_utc,\n",
    "        \"neutral_site\": bool(comp.get(\"neutralSite\")),\n",
    "        \"home_team\": _team_name((home.get(\"team\") or {})),\n",
    "        \"away_team\": _team_name((away.get(\"team\") or {})),\n",
    "        \"home_1h\": home_1h,\n",
    "        \"away_1h\": away_1h,\n",
    "        \"home_2h\": home_2h,\n",
    "        \"away_2h\": away_2h,\n",
    "        \"home_score\": home_score,\n",
    "        \"away_score\": away_score,\n",
    "    }\n",
    "    return [row]\n",
    "\n",
    "def parse_team_stats(summary: Dict[str, Any], game_id: str) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    teams = (summary.get(\"boxscore\", {}) or {}).get(\"teams\") or []\n",
    "    for t in teams:\n",
    "        team_obj = t.get(\"team\", {}) or {}\n",
    "        stats = t.get(\"statistics\") or []\n",
    "        row = {\n",
    "            \"game_id\": game_id,\n",
    "            \"team_id\": team_obj.get(\"id\"),\n",
    "            \"team\": _team_name(team_obj),\n",
    "            \"abbreviation\": team_obj.get(\"abbreviation\"),\n",
    "            \"homeAway\": t.get(\"homeAway\"),\n",
    "            \"displayOrder\": t.get(\"displayOrder\"),\n",
    "        }\n",
    "        for s in stats:\n",
    "            key = s.get(\"name\") or s.get(\"label\")\n",
    "            if key:\n",
    "                row[key] = s.get(\"displayValue\")\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def parse_player_stats(summary: Dict[str, Any], game_id: str) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    players_blocks = (summary.get(\"boxscore\", {}) or {}).get(\"players\") or []\n",
    "\n",
    "    for team_block in players_blocks:\n",
    "        team_obj = team_block.get(\"team\", {}) or {}\n",
    "        team_id = team_obj.get(\"id\")\n",
    "        team_name = _team_name(team_obj)\n",
    "        team_abbr = team_obj.get(\"abbreviation\")\n",
    "\n",
    "        for stats_pack in team_block.get(\"statistics\") or []:\n",
    "            keys = stats_pack.get(\"keys\") or []\n",
    "            for ath in stats_pack.get(\"athletes\") or []:\n",
    "                athlete = ath.get(\"athlete\", {}) or {}\n",
    "                values = ath.get(\"stats\") or []\n",
    "                row = {\n",
    "                    \"game_id\": game_id,\n",
    "                    \"team_id\": team_id,\n",
    "                    \"team\": team_name,\n",
    "                    \"abbreviation\": team_abbr,\n",
    "                    \"athlete_id\": athlete.get(\"id\"),\n",
    "                    \"athlete_name\": athlete.get(\"displayName\"),\n",
    "                    \"jersey\": athlete.get(\"jersey\"),\n",
    "                    \"position\": (athlete.get(\"position\") or {}).get(\"abbreviation\") or (athlete.get(\"position\") or {}).get(\"displayName\"),\n",
    "                    \"starter\": ath.get(\"starter\"),\n",
    "                    \"didNotPlay\": ath.get(\"didNotPlay\"),\n",
    "                    \"ejected\": ath.get(\"ejected\"),\n",
    "                }\n",
    "                for k, v in zip(keys, values):\n",
    "                    row[k] = v\n",
    "                rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def parse_officials(summary: Dict[str, Any], game_id: str) -> List[Dict[str, Any]]:\n",
    "    officials = (\n",
    "        summary.get(\"officials\")\n",
    "        or summary.get(\"gameInfo\", {}).get(\"officials\")\n",
    "        or _first_comp(summary).get(\"officials\")\n",
    "        or []\n",
    "    )\n",
    "    out = []\n",
    "    for off in officials:\n",
    "        name = off.get(\"fullName\") or off.get(\"displayName\")\n",
    "        out.append({\"game_id\": game_id, \"official_name\": name})\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Public: single-game saver\n",
    "# ----------------------------\n",
    "def save_single_game(game_id: str, outdir: str = \"data/boxscores\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Writes FOUR CSVs using only the summary endpoint:\n",
    "      1) {gid}_game_info.csv      (exact fields requested incl. 1H/2H)\n",
    "      2) {gid}_team_stats.csv\n",
    "      3) {gid}_player_stats.csv\n",
    "      4) {gid}_officials.csv      (names only)\n",
    "    \"\"\"\n",
    "    summary = _get_summary(game_id)\n",
    "\n",
    "    game_info_rows  = parse_game_info(summary, game_id)\n",
    "    team_stats_rows = parse_team_stats(summary, game_id)\n",
    "    player_rows     = parse_player_stats(summary, game_id)\n",
    "    officials_rows  = parse_officials(summary, game_id)\n",
    "\n",
    "    game_info_path    = os.path.join(outdir, f\"game-info-2026/{game_id}_game_info.csv\")\n",
    "    team_stats_path   = os.path.join(outdir, f\"team-stats-2026/{game_id}_team_stats.csv\")\n",
    "    player_stats_path = os.path.join(outdir, f\"player-stats-2026/{game_id}_player_stats.csv\")\n",
    "    officials_path    = os.path.join(outdir, f\"officials-2026/{game_id}_officials.csv\")\n",
    "\n",
    "    _write_csv(\n",
    "        game_info_path,\n",
    "        game_info_rows,\n",
    "        header=[\n",
    "            \"game_id\",\"date_utc\",\"time_utc\",\"neutral_site\",\n",
    "            \"home_team\",\"away_team\",\n",
    "            \"home_1h\",\"away_1h\",\"home_2h\",\"away_2h\",\n",
    "            \"home_score\",\"away_score\",\n",
    "        ],\n",
    "    )\n",
    "    _write_csv(team_stats_path, team_stats_rows)\n",
    "    _write_csv(player_stats_path, player_rows)\n",
    "    _write_csv(officials_path, officials_rows, header=[\"game_id\",\"official_name\"])\n",
    "\n",
    "    return {\n",
    "        \"game_info_csv\": game_info_path,\n",
    "        \"team_stats_csv\": team_stats_path,\n",
    "        \"player_stats_csv\": player_stats_path,\n",
    "        \"officials_csv\": officials_path,\n",
    "    }\n",
    "\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "csv_files = glob.glob(\"daily-box-score-ids/20251106/*.csv\")\n",
    "combined_df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "\n",
    "import time\n",
    "game_ids = list(combined_df['game_id'])\n",
    "\n",
    "count = 1\n",
    "for gid in game_ids:\n",
    "    print(count)\n",
    "    save_single_game(gid)\n",
    "    time.sleep(0.5)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03eca27aa3e5b0c2bf98348f6751bc7dc08663828d24c367008019d5f5934307"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

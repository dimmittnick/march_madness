{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "team_map = pd.read_csv(\"data/teams/map.csv\")[[\"team\", \"espn\"]]\n",
    "\n",
    "game_df = pd.read_csv(\"data/train/game-info.csv\", index_col=0)\n",
    "game_df['date'] = pd.to_datetime(game_df['date_utc']).dt.strftime('%Y%m%d')\n",
    "game_df['home_margin'] = game_df['home_score'] - game_df['away_score']\n",
    "game_df['away_margin'] = game_df['away_score'] - game_df['home_score']\n",
    "\n",
    "csv_files_2021 = glob.glob(\"daily_csvs_2021/*.csv\")\n",
    "daily_torvik_2021_df = pd.concat((pd.read_csv(f) for f in csv_files_2021), ignore_index=True)\n",
    "daily_torvik_2021_df = daily_torvik_2021_df[daily_torvik_2021_df['Team'] != \"Team\"]\n",
    "daily_torvik_2021_df['Team'] = daily_torvik_2021_df['Team'].str.extract(r'^([A-Za-z\\s.&]+)')[0].str.strip()\n",
    "daily_torvik_2021_df['WAB'] = daily_torvik_2021_df['WAB'].str.replace(\"+\",\"\", regex=False).astype(\"float\")\n",
    "daily_torvik_2021_df['season'] = 2021\n",
    "daily_torvik_2021_df = daily_torvik_2021_df[['season','Date', 'Team', 'Rk', 'Conf', 'G', 'AdjOE', 'AdjDE', 'Barthag',\n",
    "       'EFG%', 'EFGD%', 'TOR', 'TORD', 'ORB', 'DRB', 'FTR', 'FTRD', '2P%',\n",
    "       '2P%D', '3P%', '3P%D', '3PR', '3PRD', 'Adj T.', 'WAB']].sort_values([\"Date\",\"Team\"], ascending=True)\n",
    "daily_torvik_2021_df.columns = ['season', 'date', 'team', 'rank', 'conf', 'games', 'adj_off_eff', 'adj_def_eff', 'barthag',\n",
    "       'efg_pct', 'efgd_pct', 'tor', 'tord', 'orb', 'drb', 'ftr', 'ftrd', 'two_pt_pct',\n",
    "       'two_pt_def_pct', 'three_pt_pct', 'three_pt_def_pct', 'three_pt_rt', 'three_pt_def_rt', 'adj_tempo', 'wab']\n",
    "\n",
    "    \n",
    "csv_files_2022 = glob.glob(\"daily_csvs_2022/*.csv\")\n",
    "daily_torvik_2022_df = pd.concat((pd.read_csv(f) for f in csv_files_2022), ignore_index=True)\n",
    "daily_torvik_2022_df = daily_torvik_2022_df[daily_torvik_2022_df['Team'] != \"Team\"]\n",
    "daily_torvik_2022_df['Team'] = daily_torvik_2022_df['Team'].str.extract(r'^([A-Za-z\\s.&]+)')[0].str.strip()\n",
    "daily_torvik_2022_df['WAB'] = daily_torvik_2022_df['WAB'].str.replace(\"+\",\"\", regex=False).astype(\"float\")\n",
    "daily_torvik_2022_df['season'] = 2022\n",
    "daily_torvik_2022_df = daily_torvik_2022_df[['season','Date', 'Team', 'Rk', 'Conf', 'G', 'AdjOE', 'AdjDE', 'Barthag',\n",
    "       'EFG%', 'EFGD%', 'TOR', 'TORD', 'ORB', 'DRB', 'FTR', 'FTRD', '2P%',\n",
    "       '2P%D', '3P%', '3P%D', '3PR', '3PRD', 'Adj T.', 'WAB']].sort_values([\"Date\",\"Team\"], ascending=True)\n",
    "daily_torvik_2022_df.columns = ['season','date', 'team', 'rank', 'conf', 'games', 'adj_off_eff', 'adj_def_eff', 'barthag',\n",
    "       'efg_pct', 'efgd_pct', 'tor', 'tord', 'orb', 'drb', 'ftr', 'ftrd', 'two_pt_pct',\n",
    "       'two_pt_def_pct', 'three_pt_pct', 'three_pt_def_pct', 'three_pt_rt', 'three_pt_def_rt', 'adj_tempo', 'wab']\n",
    "\n",
    "csv_files_2023 = glob.glob(\"daily_csvs_2023/*.csv\")\n",
    "daily_torvik_2023_df = pd.concat((pd.read_csv(f) for f in csv_files_2023), ignore_index=True)\n",
    "daily_torvik_2023_df = daily_torvik_2023_df[daily_torvik_2023_df['Team'] != \"Team\"]\n",
    "daily_torvik_2023_df['Team'] = daily_torvik_2023_df['Team'].str.extract(r'^([A-Za-z\\s.&]+)')[0].str.strip()\n",
    "daily_torvik_2023_df['WAB'] = daily_torvik_2023_df['WAB'].str.replace(\"+\",\"\", regex=False).astype(\"float\")\n",
    "daily_torvik_2023_df['season'] = 2023\n",
    "daily_torvik_2023_df = daily_torvik_2023_df[['season','Date', 'Team', 'Rk', 'Conf', 'G', 'AdjOE', 'AdjDE', 'Barthag',\n",
    "       'EFG%', 'EFGD%', 'TOR', 'TORD', 'ORB', 'DRB', 'FTR', 'FTRD', '2P%',\n",
    "       '2P%D', '3P%', '3P%D', '3PR', '3PRD', 'Adj T.', 'WAB']].sort_values([\"Date\",\"Team\"], ascending=True)\n",
    "daily_torvik_2023_df.columns = ['season','date', 'team', 'rank', 'conf', 'games', 'adj_off_eff', 'adj_def_eff', 'barthag',\n",
    "       'efg_pct', 'efgd_pct', 'tor', 'tord', 'orb', 'drb', 'ftr', 'ftrd', 'two_pt_pct',\n",
    "       'two_pt_def_pct', 'three_pt_pct', 'three_pt_def_pct', 'three_pt_rt', 'three_pt_def_rt', 'adj_tempo', 'wab']\n",
    "\n",
    "csv_files_2024 = glob.glob(\"daily_csvs_2024/*.csv\")\n",
    "daily_torvik_2024_df = pd.concat((pd.read_csv(f) for f in csv_files_2024), ignore_index=True)\n",
    "daily_torvik_2024_df = daily_torvik_2024_df[daily_torvik_2024_df['Team'] != \"Team\"]\n",
    "daily_torvik_2024_df['Team'] = daily_torvik_2024_df['Team'].str.extract(r'^([A-Za-z\\s.&]+)')[0].str.strip()\n",
    "daily_torvik_2024_df['WAB'] = daily_torvik_2024_df['WAB'].str.replace(\"+\",\"\", regex=False).astype(\"float\")\n",
    "daily_torvik_2024_df['season'] = 2024\n",
    "daily_torvik_2024_df = daily_torvik_2024_df[['season','Date', 'Team', 'Rk', 'Conf', 'G', 'AdjOE', 'AdjDE', 'Barthag',\n",
    "       'EFG%', 'EFGD%', 'TOR', 'TORD', 'ORB', 'DRB', 'FTR', 'FTRD', '2P%',\n",
    "       '2P%D', '3P%', '3P%D', '3PR', '3PRD', 'Adj T.', 'WAB']].sort_values([\"Date\",\"Team\"], ascending=True)\n",
    "daily_torvik_2024_df.columns = ['season','date', 'team', 'rank', 'conf', 'games', 'adj_off_eff', 'adj_def_eff', 'barthag',\n",
    "       'efg_pct', 'efgd_pct', 'tor', 'tord', 'orb', 'drb', 'ftr', 'ftrd', 'two_pt_pct',\n",
    "       'two_pt_def_pct', 'three_pt_pct', 'three_pt_def_pct', 'three_pt_rt', 'three_pt_def_rt', 'adj_tempo', 'wab']\n",
    "\n",
    "csv_files_2025 = glob.glob(\"daily_csvs_2025/*.csv\")\n",
    "daily_torvik_2025_df = pd.concat((pd.read_csv(f) for f in csv_files_2025), ignore_index=True)\n",
    "daily_torvik_2025_df = daily_torvik_2025_df[daily_torvik_2025_df['Team'] != \"Team\"]\n",
    "daily_torvik_2025_df['Team'] = daily_torvik_2025_df['Team'].str.extract(r'^([A-Za-z\\s.&]+)')[0].str.strip()\n",
    "daily_torvik_2025_df['WAB'] = daily_torvik_2025_df['WAB'].str.replace(\"+\",\"\", regex=False).astype(\"float\")\n",
    "daily_torvik_2025_df['season'] = 2025\n",
    "daily_torvik_2025_df = daily_torvik_2025_df[['season','Date', 'Team', 'Rk', 'Conf', 'G', 'AdjOE', 'AdjDE', 'Barthag',\n",
    "       'EFG%', 'EFGD%', 'TOR', 'TORD', 'ORB', 'DRB', 'FTR', 'FTRD', '2P%',\n",
    "       '2P%D', '3P%', '3P%D', '3PR', '3PRD', 'Adj T.', 'WAB']].sort_values([\"Date\",\"Team\"], ascending=True)\n",
    "daily_torvik_2025_df.columns = ['season','date', 'team', 'rank', 'conf', 'games', 'adj_off_eff', 'adj_def_eff', 'barthag',\n",
    "       'efg_pct', 'efgd_pct', 'tor', 'tord', 'orb', 'drb', 'ftr', 'ftrd', 'two_pt_pct',\n",
    "       'two_pt_def_pct', 'three_pt_pct', 'three_pt_def_pct', 'three_pt_rt', 'three_pt_def_rt', 'adj_tempo', 'wab']\n",
    "\n",
    "assert len(set(daily_torvik_2021_df['team']) - set(team_map['team'])) == 0\n",
    "assert len(set(daily_torvik_2022_df['team']) - set(team_map['team'])) == 0\n",
    "assert len(set(daily_torvik_2023_df['team']) - set(team_map['team'])) == 0\n",
    "assert len(set(daily_torvik_2024_df['team']) - set(team_map['team'])) == 0\n",
    "assert len(set(daily_torvik_2025_df['team']) - set(team_map['team'])) == 0\n",
    "\n",
    "daily_torvik_df = pd.concat([daily_torvik_2021_df, daily_torvik_2022_df, daily_torvik_2023_df, daily_torvik_2024_df, daily_torvik_2025_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- unify date dtype to Int64 on both frames ---\n",
    "import re\n",
    "game_df['date_key'] = pd.to_numeric(game_df['date'], errors='coerce').astype('Int64')\n",
    "daily_torvik_df['date_key'] = pd.to_numeric(daily_torvik_df['date'], errors='coerce').astype('Int64')\n",
    "\n",
    "# --- normalize team names (strip seeds/suffixes) ---\n",
    "name_pat = r'^([A-Za-z\\s.&\\'-]+)'\n",
    "def clean_team(s):\n",
    "    if pd.isna(s): return s\n",
    "    m = re.match(name_pat, str(s))\n",
    "    base = m.group(1) if m else str(s)\n",
    "    return re.sub(r'\\s+', ' ', base).strip()\n",
    "\n",
    "game_df['home_key'] = game_df['home'].map(clean_team)\n",
    "game_df['away_key'] = game_df['away'].map(clean_team)\n",
    "daily_torvik_df['team_key'] = daily_torvik_df['team'].map(clean_team)\n",
    "\n",
    "right = daily_torvik_df.drop_duplicates(['date_key', 'team_key']).copy()\n",
    "\n",
    "# --- Build HOME version of the right table ---\n",
    "home_cols = [c for c in right.columns if c not in ['date_key', 'team_key']]\n",
    "torvik_home = right.rename(columns={'team_key': 'home_key', **{c: f'{c}_home' for c in home_cols}})\n",
    "\n",
    "# --- Merge HOME ---\n",
    "merged_df = game_df.merge(\n",
    "    torvik_home,\n",
    "    on=['date_key', 'home_key'],\n",
    "    how='left',\n",
    "    validate='many_to_one'\n",
    ")\n",
    "\n",
    "# --- Build AWAY version of the right table ---\n",
    "away_cols = [c for c in right.columns if c not in ['date_key', 'team_key']]\n",
    "torvik_away = right.rename(columns={'team_key': 'away_key', **{c: f'{c}_away' for c in away_cols}})\n",
    "\n",
    "# --- Merge AWAY ---\n",
    "merged_df = merged_df.merge(\n",
    "    torvik_away,\n",
    "    on=['date_key', 'away_key'],\n",
    "    how='left',\n",
    "    validate='many_to_one'\n",
    ")\n",
    "\n",
    "merged_df['season'] = np.where(merged_df['season_home'].isna(), merged_df['season_away'], merged_df['season_home'])\n",
    "merged_df['neutral_site'] = np.where(merged_df['neutral_site'] == True, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_grab(df):\n",
    "    \"\"\"\n",
    "    Assigns a season year based on date_utc.\n",
    "    Example: games before 2021-04-01 belong to season 2021,\n",
    "             games between 2021-04-01 and 2022-04-01 belong to 2022, etc.\n",
    "    \"\"\"\n",
    "    # Ensure date_utc is datetime\n",
    "    df = df.copy()\n",
    "    df['date_utc'] = pd.to_datetime(df['date_utc'])\n",
    "\n",
    "    # Define season cutoffs\n",
    "    bins = [\n",
    "        pd.Timestamp(\"1900-01-01\"),\n",
    "        pd.Timestamp(\"2021-04-01\"),\n",
    "        pd.Timestamp(\"2022-04-01\"),\n",
    "        pd.Timestamp(\"2023-04-01\"),\n",
    "        pd.Timestamp(\"2024-04-01\"),\n",
    "        pd.Timestamp(\"2025-04-01\"),\n",
    "        pd.Timestamp(\"2100-01-01\"),\n",
    "    ]\n",
    "    seasons = [2021, 2022, 2023, 2024, 2025, 2026]\n",
    "\n",
    "    # Use pandas cut to categorize efficiently\n",
    "    df['season'] = pd.cut(df['date_utc'], bins=bins, labels=seasons, right=False).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "merged_df = season_grab(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[[\n",
    "    'game_id',\n",
    "    'season',\n",
    "    'date',\n",
    "    'date_utc',\n",
    "    'time_utc',\n",
    "    'neutral_site',\n",
    "    'home',\n",
    "    'away',\n",
    "    'home_1h',\n",
    "    'away_1h',\n",
    "    'home_2h',\n",
    "    'away_2h',\n",
    "    'home_score',\n",
    "    'away_score',\n",
    "    'home_margin',\n",
    "    'away_margin',\n",
    "    'assists_home',\n",
    "    'fouls_home',\n",
    "    'technicalFouls_home',\n",
    "    'flagrantFouls_home',\n",
    "    'totalRebounds_home',\n",
    "    'offensiveRebounds_home',\n",
    "    'defensiveRebounds_home',\n",
    "    'pointsInPaint_home',\n",
    "    'turnovers_home',\n",
    "    'turnoverPoints_home',\n",
    "    'steals_home',\n",
    "    'blocks_home',\n",
    "    'fastBreakPoints_home',\n",
    "    'assists_away',\n",
    "    'fouls_away',\n",
    "    'technicalFouls_away',\n",
    "    'flagrantFouls_away',\n",
    "    'totalRebounds_away',\n",
    "    'offensiveRebounds_away',\n",
    "    'defensiveRebounds_away',\n",
    "    'pointsInPaint_away',\n",
    "    'turnovers_away',\n",
    "    'turnoverPoints_away',\n",
    "    'steals_away',\n",
    "    'blocks_away',\n",
    "    'fastBreakPoints_away',\n",
    "    'official_1',\n",
    "    'official_2',\n",
    "    'official_3',  \n",
    "    'rank_home',\n",
    "    'conf_home',\n",
    "    'games_home',\n",
    "    'adj_off_eff_home',\n",
    "    'adj_def_eff_home',\n",
    "    'barthag_home',\n",
    "    'efg_pct_home',\n",
    "    'efgd_pct_home',\n",
    "    'tor_home',\n",
    "    'tord_home',\n",
    "    'orb_home',\n",
    "    'drb_home',\n",
    "    'ftr_home',\n",
    "    'ftrd_home',\n",
    "    'two_pt_pct_home',\n",
    "    'two_pt_def_pct_home',\n",
    "    'three_pt_pct_home',\n",
    "    'three_pt_def_pct_home',\n",
    "    'three_pt_rt_home',\n",
    "    'three_pt_def_rt_home',\n",
    "    'adj_tempo_home',\n",
    "    'wab_home',\n",
    "    'rank_away',\n",
    "    'conf_away',\n",
    "    'games_away',\n",
    "    'adj_off_eff_away',\n",
    "    'adj_def_eff_away',\n",
    "    'barthag_away',\n",
    "    'efg_pct_away',\n",
    "    'efgd_pct_away',\n",
    "    'tor_away',\n",
    "    'tord_away',\n",
    "    'orb_away',\n",
    "    'drb_away',\n",
    "    'ftr_away',\n",
    "    'ftrd_away',\n",
    "    'two_pt_pct_away',\n",
    "    'two_pt_def_pct_away',\n",
    "    'three_pt_pct_away',\n",
    "    'three_pt_def_pct_away',\n",
    "    'three_pt_rt_away',\n",
    "    'three_pt_def_rt_away',\n",
    "    'adj_tempo_away',\n",
    "    'wab_away']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "del daily_torvik_2021_df\n",
    "del daily_torvik_2022_df\n",
    "del daily_torvik_2023_df\n",
    "del daily_torvik_2024_df\n",
    "del daily_torvik_2025_df\n",
    "del torvik_away\n",
    "del torvik_home\n",
    "del game_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df = merged_df[[\n",
    "    \"game_id\",\n",
    "    'home_1h',\n",
    "    'away_1h',\n",
    "    'home_2h',\n",
    "    'away_2h',\n",
    "    'home_score',\n",
    "    'away_score',\n",
    "    'home_margin',\n",
    "    'away_margin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, Optional, Iterable\n",
    "\n",
    "OFFICIAL_COLS = ['official_1', 'official_2', 'official_3']\n",
    "CONF_COLS = ['conf_home', 'conf_away']\n",
    "LOCAL_TZ = 'America/New_York'\n",
    "\n",
    "def _ensure_date_key_str(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalize YYYYMMDD to 8-char string.\"\"\"\n",
    "    return s.astype(str).str.extract(r'(\\d{8})')[0]\n",
    "\n",
    "def _build_tipoff_utc(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Build timezone-aware UTC datetime from (date_key + time_utc like '21:00Z').\n",
    "    Requires: 'date_key' (YYYYMMDD) and 'time_utc' ('HH:MMZ' or 'HH:MM').\n",
    "    \"\"\"\n",
    "    if 'date_key' not in df.columns:\n",
    "        raise KeyError(\"Expected 'date_key' (YYYYMMDD).\")\n",
    "    if 'time_utc' not in df.columns:\n",
    "        raise KeyError(\"Expected 'time_utc' like '21:00Z' or '21:00'.\")\n",
    "\n",
    "    date_key = _ensure_date_key_str(df['date'])\n",
    "    t = df['time_utc'].astype(str).str.strip()\n",
    "    t = np.where(t.str.endswith('Z'), t, t + 'Z')  # ensure trailing Z\n",
    "    iso = pd.to_datetime(date_key, format='%Y%m%d', errors='coerce').dt.strftime('%Y-%m-%d') + ' ' + t\n",
    "    tipoff_utc = pd.to_datetime(iso, utc=True, errors='coerce', infer_datetime_format=True)\n",
    "    return tipoff_utc\n",
    "\n",
    "def _time_features_from_dt(dt: pd.Series, prefix: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    From a timezone-aware datetime series, produce:\n",
    "      - {prefix}_hour, {prefix}_minute, {prefix}_second\n",
    "      - {prefix}_seconds_since_midnight\n",
    "      - {prefix}_hour_sin, {prefix}_hour_cos (cyclical)\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame(index=dt.index)\n",
    "    out[f'{prefix}_hour'] = dt.dt.hour.fillna(0).astype('int16')\n",
    "    out[f'{prefix}_minute'] = dt.dt.minute.fillna(0).astype('int16')\n",
    "    out[f'{prefix}_second'] = dt.dt.second.fillna(0).astype('int16')\n",
    "    out[f'{prefix}_seconds_since_midnight'] = (\n",
    "        out[f'{prefix}_hour'] * 3600 + out[f'{prefix}_minute'] * 60 + out[f'{prefix}_second']\n",
    "    ).astype('int32')\n",
    "\n",
    "    two_pi = 2 * np.pi\n",
    "    out[f'{prefix}_hour_sin'] = np.sin(two_pi * out[f'{prefix}_hour'] / 24.0)\n",
    "    out[f'{prefix}_hour_cos'] = np.cos(two_pi * out[f'{prefix}_hour'] / 24.0)\n",
    "    return out\n",
    "\n",
    "def _add_day_flags(local_dt: pd.Series, base_df: pd.DataFrame, prefix: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Day-of-week flags on LOCAL time:\n",
    "      - {prefix}_is_weekend (Sat/Sun)\n",
    "      - {prefix}_is_primetime (18:00–22:59)\n",
    "      - {prefix}_daypart_* one-hots: morning(5–11), afternoon(12–16), evening(17–21), late(other)\n",
    "    \"\"\"\n",
    "    out = base_df.copy()\n",
    "    dow = local_dt.dt.dayofweek  # Mon=0..Sun=6\n",
    "    out[f'{prefix}_is_weekend'] = dow.isin([5, 6]).fillna(False).astype('int8')\n",
    "\n",
    "    hour = local_dt.dt.hour.fillna(0).astype(int)\n",
    "    out[f'{prefix}_is_primetime'] = ((hour >= 18) & (hour <= 22)).astype('int8')\n",
    "\n",
    "    def daypart(h):\n",
    "        if 5 <= h <= 11:  return 'morning'\n",
    "        if 12 <= h <= 16: return 'afternoon'\n",
    "        if 17 <= h <= 21: return 'evening'\n",
    "        return 'late'\n",
    "\n",
    "    dp = hour.map(daypart).astype('category')\n",
    "    dummies = pd.get_dummies(dp, prefix=f'{prefix}_daypart', dtype='int8')\n",
    "    out = pd.concat([out, dummies], axis=1)\n",
    "    return out\n",
    "\n",
    "# ---------- Officials: shared encoding across all slots ----------\n",
    "def fit_official_encoder(df: pd.DataFrame, official_cols: Iterable[str] = OFFICIAL_COLS) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Fit a single shared encoding for officials across all official columns.\n",
    "    Reserve 0 for 'UNK' (missing/unknown).\n",
    "    \"\"\"\n",
    "    vals = []\n",
    "    for c in official_cols:\n",
    "        if c in df.columns:\n",
    "            vals.append(df[c].astype('string'))\n",
    "    if not vals:\n",
    "        return {'UNK': 0}\n",
    "    all_offs = pd.concat(vals, axis=0).dropna()\n",
    "    uniq = sorted(all_offs.unique())\n",
    "    mapping = {'UNK': 0}\n",
    "    mapping.update({name: i + 1 for i, name in enumerate(uniq)})\n",
    "    return mapping\n",
    "\n",
    "def transform_officials_with_map(df: pd.DataFrame, mapping: Dict[str, int],\n",
    "                                 official_cols: Iterable[str] = OFFICIAL_COLS) -> pd.DataFrame:\n",
    "    \"\"\"Apply the shared mapping to each official* column, creating *_code columns.\"\"\"\n",
    "    out = df.copy()\n",
    "    unk = mapping.get('UNK', 0)\n",
    "    for c in official_cols:\n",
    "        if c in out.columns:\n",
    "            s = out[c].astype('string')\n",
    "            out[f'{c}_code'] = s.map(mapping).fillna(unk).astype('int32')\n",
    "        else:\n",
    "            out[f'{c}_code'] = unk\n",
    "    return out\n",
    "\n",
    "# ---------- Conference encoders ----------\n",
    "def fit_label_encoder(series: pd.Series, fill_value: str = 'UNK') -> Dict[str, int]:\n",
    "    \"\"\"Simple label encoder fit: reserve 0 for UNK.\"\"\"\n",
    "    s = series.astype('string').fillna(fill_value)\n",
    "    uniq = sorted(s.unique())\n",
    "    mapping = {fill_value: 0}\n",
    "    # start real categories at 1, ensure UNK remains 0 even if present in uniques\n",
    "    idx = 1\n",
    "    for val in uniq:\n",
    "        if val == fill_value:\n",
    "            continue\n",
    "        mapping[val] = idx\n",
    "        idx += 1\n",
    "    return mapping\n",
    "\n",
    "def transform_with_map(series: pd.Series, mapping: Dict[str, int], fill_value: str = 'UNK') -> pd.Series:\n",
    "    \"\"\"Transform using a prefit mapping, unknowns go to 0.\"\"\"\n",
    "    return series.astype('string').fillna(fill_value).map(mapping).fillna(mapping.get(fill_value, 0)).astype('int32')\n",
    "\n",
    "# ------------------ PUBLIC ENTRY ------------------\n",
    "def build_time_officials_conference_features(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    add_et_features: bool = True,\n",
    "    make_conference_dummies: bool = False,\n",
    "    official_map: Optional[Dict[str, int]] = None,\n",
    "    conf_home_map: Optional[Dict[str, int]] = None,\n",
    "    conf_away_map: Optional[Dict[str, int]] = None,\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Dict[str, int]]]:\n",
    "    \"\"\"\n",
    "    STEP 1: Add time features + encode officials (shared) + encode conferences.\n",
    "\n",
    "    Fit/transform behavior:\n",
    "      - If official_map / conf_*_map are provided, they are USED to transform (no refit).\n",
    "      - If not provided, maps are FIT on the given df (OK for quick experiments; for strict ML practice,\n",
    "        fit on TRAIN ONLY and pass the maps for VAL/TEST to avoid distribution drift).\n",
    "\n",
    "    Returns:\n",
    "      features_1 (DataFrame), enc_maps (dict with 'official_map','conference_home_map','conference_away_map')\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # --- Compose UTC datetime from date_key + time_utc, then add UTC features ---\n",
    "    out['date_key'] = _ensure_date_key_str(out['date'])\n",
    "    out['tipoff_utc'] = _build_tipoff_utc(out)\n",
    "    utc_feats = _time_features_from_dt(out['tipoff_utc'], prefix='utc')\n",
    "    out = pd.concat([out, utc_feats], axis=1)\n",
    "\n",
    "    # --- ET (local) features + day flags ---\n",
    "    if add_et_features:\n",
    "        tipoff_et = out['tipoff_utc'].dt.tz_convert(LOCAL_TZ)\n",
    "        et_feats = _time_features_from_dt(tipoff_et, prefix='et')\n",
    "        out = pd.concat([out, et_feats], axis=1)\n",
    "        out = _add_day_flags(tipoff_et, out, prefix='et')\n",
    "\n",
    "    enc_maps: Dict[str, Dict[str, int]] = {}\n",
    "\n",
    "    # --- Officials (shared encoding across official1/2/3) ---\n",
    "    if official_map is None:\n",
    "        official_map = fit_official_encoder(out, OFFICIAL_COLS)\n",
    "    out = transform_officials_with_map(out, official_map, OFFICIAL_COLS)\n",
    "    enc_maps['official_map'] = official_map\n",
    "\n",
    "    # --- Conferences (label encodings, separate maps for home/away) ---\n",
    "    if 'conf_home' in out.columns:\n",
    "        if conf_home_map is None:\n",
    "            conf_home_map = fit_label_encoder(out['conf_home'])\n",
    "        out['conf_home_code'] = transform_with_map(out['conf_home'], conf_home_map)\n",
    "        enc_maps['conf_home_map'] = conf_home_map\n",
    "        if make_conference_dummies:\n",
    "            dummies = pd.get_dummies(out['conf_home'].astype('string').fillna('UNK'),\n",
    "                                     prefix='conf_home', dtype='int8')\n",
    "            out = pd.concat([out, dummies], axis=1)\n",
    "\n",
    "    if 'conf_away' in out.columns:\n",
    "        if conf_away_map is None:\n",
    "            conf_away_map = fit_label_encoder(out['conf_away'])\n",
    "        out['conf_away_code'] = transform_with_map(out['conf_away'], conf_away_map)\n",
    "        enc_maps['conf_away_map'] = conf_away_map\n",
    "        if make_conference_dummies:\n",
    "            dummies = pd.get_dummies(out['conf_away'].astype('string').fillna('UNK'),\n",
    "                                     prefix='conf_away', dtype='int8')\n",
    "            out = pd.concat([out, dummies], axis=1)\n",
    "\n",
    "    return out, enc_maps\n",
    "\n",
    "features_1, enc_maps = build_time_officials_conference_features(\n",
    "        merged_df,\n",
    "        add_et_features=True,\n",
    "        make_conference_dummies=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_1 = features_1[['game_id', 'utc_seconds_since_midnight', 'utc_hour_sin', 'utc_hour_cos',\n",
    "       'et_hour', 'et_minute', 'et_second', 'et_seconds_since_midnight',\n",
    "       'et_hour_sin', 'et_hour_cos', 'et_is_weekend', 'et_is_primetime',\n",
    "       'et_daypart_afternoon', 'et_daypart_evening', 'et_daypart_late',\n",
    "       'et_daypart_morning', 'official_1_code', 'official_2_code',\n",
    "       'official_3_code', 'conf_home_code', 'conf_away_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- assume your dataframe is named df ---\n",
    "df = merged_df[['game_id', 'season', 'date', 'date_utc', 'time_utc',\n",
    "       'neutral_site', 'home', 'away', 'home_1h', 'away_1h', 'home_2h',\n",
    "       'away_2h', 'home_score', 'away_score', 'home_margin',\n",
    "       'away_margin','rank_home', 'games_home',\n",
    "       'adj_off_eff_home', 'adj_def_eff_home', 'barthag_home',\n",
    "       'efg_pct_home', 'efgd_pct_home', 'tor_home', 'tord_home',\n",
    "       'orb_home', 'drb_home', 'ftr_home', 'ftrd_home', 'two_pt_pct_home',\n",
    "       'two_pt_def_pct_home', 'three_pt_pct_home',\n",
    "       'three_pt_def_pct_home', 'three_pt_rt_home',\n",
    "       'three_pt_def_rt_home', 'adj_tempo_home', 'wab_home', 'rank_away',\n",
    "       'games_away', 'adj_off_eff_away', 'adj_def_eff_away',\n",
    "       'barthag_away', 'efg_pct_away', 'efgd_pct_away', 'tor_away',\n",
    "       'tord_away', 'orb_away', 'drb_away', 'ftr_away', 'ftrd_away',\n",
    "       'two_pt_pct_away', 'two_pt_def_pct_away', 'three_pt_pct_away',\n",
    "       'three_pt_def_pct_away', 'three_pt_rt_away',\n",
    "       'three_pt_def_rt_away', 'adj_tempo_away', 'wab_away']].copy()\n",
    "\n",
    "# --- 1. Create a datetime for chronological sorting ---\n",
    "if 'date_utc' in df.columns:\n",
    "    if 'time_utc' in df.columns:\n",
    "        df['game_dt'] = pd.to_datetime(\n",
    "            df['date_utc'].astype(str).str.strip() + ' ' +\n",
    "            df['time_utc'].fillna('00:00:00').astype(str).str.strip(),\n",
    "            errors='coerce'\n",
    "        )\n",
    "    else:\n",
    "        df['game_dt'] = pd.to_datetime(df['date_utc'], errors='coerce')\n",
    "else:\n",
    "    df['game_dt'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# --- 2. Identify feature bases (shared between home/away) ---\n",
    "suffix_cols = [c for c in df.columns if c.endswith('_home') or c.endswith('_away')]\n",
    "bases = sorted({c.rsplit('_', 1)[0] for c in suffix_cols})\n",
    "\n",
    "# --- 3. Map column names for home/away to team/opponent versions ---\n",
    "home_to_team = {f'{b}_home': b for b in bases}\n",
    "away_to_team = {f'{b}_away': b for b in bases}\n",
    "home_to_opp  = {f'{b}_away': f'opp_{b}' for b in bases}\n",
    "away_to_opp  = {f'{b}_home': f'opp_{b}' for b in bases}\n",
    "\n",
    "# --- 4. Create long-format team-game table (home + away) ---\n",
    "id_cols = ['game_id', 'season', 'game_dt', 'neutral_site']\n",
    "id_cols = [c for c in id_cols if c in df.columns]\n",
    "\n",
    "# home perspective\n",
    "home_view = df[id_cols + ['home', 'away'] + suffix_cols].copy()\n",
    "home_view = home_view.rename(columns={'home': 'team', 'away': 'opponent'})\n",
    "home_view = home_view.rename(columns={**home_to_team, **home_to_opp})\n",
    "home_view['is_home'] = True\n",
    "\n",
    "# away perspective\n",
    "away_view = df[id_cols + ['home', 'away'] + suffix_cols].copy()\n",
    "away_view = away_view.rename(columns={'away': 'team', 'home': 'opponent'})\n",
    "away_view = away_view.rename(columns={**away_to_team, **away_to_opp})\n",
    "away_view['is_home'] = False\n",
    "\n",
    "# combine both\n",
    "team_games = pd.concat([home_view, away_view], ignore_index=True, sort=False)\n",
    "\n",
    "# --- 5. Sort games chronologically per team/season ---\n",
    "team_games = team_games.sort_values(['team', 'season', 'game_dt', 'game_id'], ignore_index=True)\n",
    "\n",
    "# --- 6. Convert all numeric/stat columns to floats safely ---\n",
    "all_stat_cols = [c for c in team_games.columns if any(x in c for x in [\n",
    "    'rank', 'games', 'adj_off_eff', 'adj_def_eff', 'barthag', 'efg_pct', 'efgd_pct',\n",
    "    'tor', 'tord', 'orb', 'drb', 'ftr', 'ftrd', 'two_pt_pct', 'two_pt_def_pct',\n",
    "    'three_pt_pct', 'three_pt_def_pct', 'three_pt_rt', 'three_pt_def_rt',\n",
    "    'adj_tempo', 'wab'\n",
    "])]\n",
    "\n",
    "def _to_num(s):\n",
    "    if s.dtype == 'O':\n",
    "        s = s.astype(str).str.strip().str.rstrip('%')\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "for c in all_stat_cols:\n",
    "    team_games[c] = _to_num(team_games[c])\n",
    "\n",
    "# --- 7. Lag features by 1 game (leakage-safe) ---\n",
    "team_feature_cols = [c for c in bases if c in team_games.columns]\n",
    "opp_feature_cols  = [f'opp_{b}' for b in bases if f'opp_{b}' in team_games.columns]\n",
    "\n",
    "# previous-game team stats\n",
    "if team_feature_cols:\n",
    "    team_games[[f'lag1_{c}' for c in team_feature_cols]] = (\n",
    "        team_games.groupby(['team', 'season'], dropna=False)[team_feature_cols].shift(1)\n",
    "    )\n",
    "\n",
    "# previous-game opponent stats (optional)\n",
    "if opp_feature_cols:\n",
    "    team_games[[f'lag1_{c}' for c in opp_feature_cols]] = (\n",
    "        team_games.groupby(['team', 'season'], dropna=False)[opp_feature_cols].shift(1)\n",
    "    )\n",
    "\n",
    "# --- 8. Rolling/cumulative opponent rank averages ---\n",
    "if 'opp_rank' in team_games.columns:\n",
    "    g = team_games.groupby(['team', 'season'], dropna=False)['opp_rank']\n",
    "\n",
    "    # Example-style (includes current opponent, as in your description)\n",
    "    team_games['opp_rank_cummean_incl'] = g.cumsum() / (g.cumcount() + 1)\n",
    "\n",
    "    # Leakage-safe (only prior opponents)\n",
    "    cum_sum_prev = g.cumsum().shift(1)\n",
    "    cnt_prev = g.cumcount()\n",
    "    team_games['opp_rank_cummean_pre'] = cum_sum_prev / cnt_prev.replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2 = team_games[['game_id', 'team', 'opponent',\n",
    "       'lag1_adj_def_eff', 'lag1_adj_off_eff',\n",
    "       'lag1_adj_tempo', 'lag1_barthag', 'lag1_drb', 'lag1_efg_pct',\n",
    "       'lag1_efgd_pct', 'lag1_ftr', 'lag1_ftrd', 'lag1_games', 'lag1_orb',\n",
    "       'lag1_rank', 'lag1_three_pt_def_pct', 'lag1_three_pt_def_rt',\n",
    "       'lag1_three_pt_pct', 'lag1_three_pt_rt', 'lag1_tor', 'lag1_tord',\n",
    "       'lag1_two_pt_def_pct', 'lag1_two_pt_pct', 'lag1_wab',\n",
    "       'lag1_opp_adj_def_eff', 'lag1_opp_adj_off_eff',\n",
    "       'lag1_opp_adj_tempo', 'lag1_opp_barthag', 'lag1_opp_drb',\n",
    "       'lag1_opp_efg_pct', 'lag1_opp_efgd_pct', 'lag1_opp_ftr',\n",
    "       'lag1_opp_ftrd', 'lag1_opp_games', 'lag1_opp_orb', 'lag1_opp_rank',\n",
    "       'lag1_opp_three_pt_def_pct', 'lag1_opp_three_pt_def_rt',\n",
    "       'lag1_opp_three_pt_pct', 'lag1_opp_three_pt_rt', 'lag1_opp_tor',\n",
    "       'lag1_opp_tord', 'lag1_opp_two_pt_def_pct', 'lag1_opp_two_pt_pct',\n",
    "       'lag1_opp_wab', 'opp_rank_cummean_incl', 'opp_rank_cummean_pre']]\n",
    "\n",
    "del df\n",
    "del team_games\n",
    "del home_view\n",
    "del away_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def build_cbb_features(raw_df, windows=(1,3,5,10), ewm_halflife=5):\n",
    "    df = raw_df.copy()\n",
    "\n",
    "    # --- 0) Parse dates and sort ---\n",
    "    date_col = 'date' if 'date' in df.columns else 'date_utc'\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df = df.sort_values([date_col, 'game_id']).reset_index(drop=True)\n",
    "\n",
    "    # --- 1) Identify base stat names ---\n",
    "    def _base_names(columns):\n",
    "        bases = set()\n",
    "        for c in columns:\n",
    "            if c.endswith('_home'):\n",
    "                bases.add(c[:-5])\n",
    "            elif c.endswith('_away'):\n",
    "                bases.add(c[:-5])\n",
    "        return sorted(bases)\n",
    "\n",
    "    stat_bases = _base_names(df.columns)\n",
    "\n",
    "    # --- 2) Long-format: one row per team per game ---\n",
    "    def _make_team_rows(side):\n",
    "        assert side in ('home','away')\n",
    "        other = 'away' if side == 'home' else 'home'\n",
    "\n",
    "        base_map = {}\n",
    "        for b in stat_bases:\n",
    "            team_col = f'{b}_{side}'\n",
    "            opp_col  = f'{b}_{other}'\n",
    "            if team_col in df.columns and opp_col in df.columns:\n",
    "                base_map[b] = (team_col, opp_col)\n",
    "\n",
    "        is_home_val = 1 if side == 'home' else 0\n",
    "\n",
    "        out = pd.DataFrame({\n",
    "            'game_id': df['game_id'],\n",
    "            'season' : df['season'],\n",
    "            'date'   : df[date_col],\n",
    "            'team'   : df[side],\n",
    "            'opponent': df[other],\n",
    "            'is_home': np.full(len(df), is_home_val, dtype=np.int8),\n",
    "        })\n",
    "\n",
    "        if f'{side}_score' in df.columns and f'{other}_score' in df.columns:\n",
    "            out['team_score'] = df[f'{side}_score']\n",
    "            out['opp_score']  = df[f'{other}_score']\n",
    "\n",
    "        if f'{side}_margin' in df.columns and f'{other}_margin' in df.columns:\n",
    "            out['team_margin'] = df[f'{side}_margin']\n",
    "            out['opp_margin']  = df[f'{other}_margin']\n",
    "\n",
    "        if 'neutral_site' in df.columns:\n",
    "            out['neutral_site'] = pd.Series(df['neutral_site']).fillna(0).astype(int)\n",
    "\n",
    "        for b,(tc,oc) in base_map.items():\n",
    "            out[b] = pd.to_numeric(df[tc], errors='coerce')\n",
    "            out[f'opp_{b}'] = pd.to_numeric(df[oc], errors='coerce')\n",
    "\n",
    "        # --- add 1H / 2H points as standardized columns ---\n",
    "        if f'{side}_1h' in df.columns and f'{other}_1h' in df.columns:\n",
    "            out['points_1h'] = pd.to_numeric(df[f'{side}_1h'], errors='coerce')\n",
    "            out['opp_points_1h'] = pd.to_numeric(df[f'{other}_1h'], errors='coerce')\n",
    "\n",
    "        if f'{side}_2h' in df.columns and f'{other}_2h' in df.columns:\n",
    "            out['points_2h'] = pd.to_numeric(df[f'{side}_2h'], errors='coerce')\n",
    "            out['opp_points_2h'] = pd.to_numeric(df[f'{other}_2h'], errors='coerce')\n",
    "\n",
    "        return out\n",
    "\n",
    "    long_home = _make_team_rows('home')\n",
    "    long_away = _make_team_rows('away')\n",
    "    team_games = pd.concat([long_home, long_away], ignore_index=True)\n",
    "    team_games = team_games.sort_values(['team','season','date','game_id']).reset_index(drop=True)\n",
    "\n",
    "    # --- 3) Compute rest days ---\n",
    "    team_games['prev_date'] = team_games.groupby(['team','season'])['date'].shift(1)\n",
    "    team_games['rest_days'] = (team_games['date'] - team_games['prev_date']).dt.days\n",
    "    team_games['rest_days'] = team_games['rest_days'].fillna(7)\n",
    "    for w in windows:\n",
    "        team_games[f'ra_rest_days_w{w}'] = (\n",
    "            team_games.groupby(['team','season'])['rest_days']\n",
    "            .transform(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
    "        )\n",
    "\n",
    "    # --- 4) Rolling averages for team & allowed stats ---\n",
    "    full_bases = [b for b in stat_bases if b in team_games.columns]\n",
    "\n",
    "    for b in full_bases:\n",
    "        for w in windows:\n",
    "            team_games[f'ra_{b}_w{w}'] = (\n",
    "                team_games.groupby(['team','season'])[b]\n",
    "                .transform(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
    "            )\n",
    "            opp_b = f'opp_{b}'\n",
    "            if opp_b in team_games.columns:\n",
    "                team_games[f'ra_allowed_{b}_w{w}'] = (\n",
    "                    team_games.groupby(['team','season'])[opp_b]\n",
    "                    .transform(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
    "                )\n",
    "\n",
    "        team_games[f'rstd_{b}_w5'] = (\n",
    "            team_games.groupby(['team','season'])[b]\n",
    "            .transform(lambda s: s.shift(1).rolling(5, min_periods=2).std())\n",
    "        )\n",
    "        team_games[f'ewm_{b}_hl{ewm_halflife}'] = (\n",
    "            team_games.groupby(['team','season'])[b]\n",
    "            .transform(lambda s: s.shift(1).ewm(halflife=ewm_halflife, min_periods=1, adjust=False).mean())\n",
    "        )\n",
    "\n",
    "        opp_b = f'opp_{b}'\n",
    "        if opp_b in team_games.columns:\n",
    "            team_games[f'rstd_allowed_{b}_w5'] = (\n",
    "                team_games.groupby(['team','season'])[opp_b]\n",
    "                .transform(lambda s: s.shift(1).rolling(5, min_periods=2).std())\n",
    "            )\n",
    "            team_games[f'ewm_allowed_{b}_hl{ewm_halflife}'] = (\n",
    "                team_games.groupby(['team','season'])[opp_b]\n",
    "                .transform(lambda s: s.shift(1).ewm(halflife=ewm_halflife, min_periods=1, adjust=False).mean())\n",
    "            )\n",
    "\n",
    "    # --- 4b) Rolling for 1H/2H points and allowed ---\n",
    "    for b in ['points_1h', 'points_2h']:\n",
    "        if b in team_games.columns:\n",
    "            for w in windows:\n",
    "                team_games[f'ra_{b}_w{w}'] = (\n",
    "                    team_games.groupby(['team','season'])[b]\n",
    "                    .transform(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
    "                )\n",
    "            opp_b = f'opp_{b}'\n",
    "            if opp_b in team_games.columns:\n",
    "                for w in windows:\n",
    "                    team_games[f'ra_allowed_{b}_w{w}'] = (\n",
    "                        team_games.groupby(['team','season'])[opp_b]\n",
    "                        .transform(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
    "                    )\n",
    "\n",
    "    # --- 5) Venue effects (home vs away trends) ---\n",
    "    if 'team_margin' in team_games.columns:\n",
    "        for w in windows:\n",
    "            team_games[f'ra_margin_homeonly_w{w}'] = (\n",
    "                team_games.groupby(['team','season','is_home'])['team_margin']\n",
    "                .transform(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
    "            )\n",
    "\n",
    "    # --- 6) Recent scoring form ---\n",
    "    if 'team_score' in team_games.columns:\n",
    "        for w in windows:\n",
    "            pf = team_games.groupby(['team','season'])['team_score'].transform(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
    "            pa = team_games.groupby(['team','season'])['opp_score'].transform(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
    "            team_games[f'ra_points_for_w{w}'] = pf\n",
    "            team_games[f'ra_points_against_w{w}'] = pa\n",
    "            team_games[f'ra_point_diff_w{w}'] = pf - pa\n",
    "\n",
    "    if 'team_margin' in team_games.columns:\n",
    "        for w in windows:\n",
    "            team_games[f'ra_margin_w{w}'] = (\n",
    "                team_games.groupby(['team','season'])['team_margin']\n",
    "                .transform(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
    "            )\n",
    "\n",
    "    # --- 7) Select pregame features (no leakage) ---\n",
    "    raw_stat_cols = full_bases + [f'opp_{b}' for b in full_bases] + [\n",
    "        'team_score','opp_score','team_margin','opp_margin','prev_date'\n",
    "    ]\n",
    "    # prevent leakage from raw current-game 1H/2H values\n",
    "    for leak_col in ['points_1h','opp_points_1h','points_2h','opp_points_2h']:\n",
    "        if leak_col in team_games.columns:\n",
    "            raw_stat_cols.append(leak_col)\n",
    "\n",
    "    raw_stat_cols = [c for c in raw_stat_cols if c in team_games.columns]\n",
    "    feature_cols = [c for c in team_games.columns if c not in (raw_stat_cols + ['opponent'])]\n",
    "    pregame_team_features = team_games[feature_cols + ['opponent']].copy()\n",
    "\n",
    "    # --- 8) Create wide per-game feature table (home vs away) ---\n",
    "    home_feat = pregame_team_features[pregame_team_features['is_home']==1].drop(columns=['team']).rename(\n",
    "        columns=lambda x: f'home_{x}' if x not in ['game_id','season','date','opponent'] else x\n",
    "    ).rename(columns={'opponent':'away_team'})\n",
    "\n",
    "    away_feat = pregame_team_features[pregame_team_features['is_home']==0].drop(columns=['team']).rename(\n",
    "        columns=lambda x: f'away_{x}' if x not in ['game_id','season','date','opponent'] else x\n",
    "    ).rename(columns={'opponent':'home_team'})\n",
    "\n",
    "    game_model_table = (\n",
    "        home_feat.merge(away_feat, on=['game_id','season','date'], suffixes=('',''))\n",
    "        .rename(columns={'home_team':'home','away_team':'away'})\n",
    "    )\n",
    "\n",
    "    # --- 9) Example matchup deltas ---\n",
    "    for w in windows:\n",
    "        pfx_h = f'home_ra_point_diff_w{w}'\n",
    "        pfx_a = f'away_ra_point_diff_w{w}'\n",
    "        if pfx_h in game_model_table.columns and pfx_a in game_model_table.columns:\n",
    "            game_model_table[f'diff_point_diff_w{w}'] = game_model_table[pfx_h] - game_model_table[pfx_a]\n",
    "\n",
    "    return pregame_team_features, game_model_table\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# USAGE\n",
    "# -----------------------\n",
    "features_3, game_model_table = build_cbb_features(merged_df[['game_id','season','date','date_utc','time_utc','neutral_site','home','away',\n",
    "               'home_1h','away_1h','home_2h','away_2h','home_score','away_score',\n",
    "               'home_margin','away_margin',\n",
    "               'assists_home','fouls_home','technicalFouls_home','flagrantFouls_home',\n",
    "               'totalRebounds_home','offensiveRebounds_home','defensiveRebounds_home',\n",
    "               'pointsInPaint_home','turnovers_home','turnoverPoints_home','steals_home',\n",
    "               'blocks_home','fastBreakPoints_home',\n",
    "               'assists_away','fouls_away','technicalFouls_away','flagrantFouls_away',\n",
    "               'totalRebounds_away','offensiveRebounds_away','defensiveRebounds_away',\n",
    "               'pointsInPaint_away','turnovers_away','turnoverPoints_away','steals_away',\n",
    "               'blocks_away','fastBreakPoints_away']])\n",
    "\n",
    "\n",
    "features_3 = features_3[['game_id', 'team', 'rest_days', 'ra_rest_days_w1', 'ra_rest_days_w3', 'ra_rest_days_w5', 'ra_rest_days_w10', 'ra_assists_w1', 'ra_allowed_assists_w1', 'ra_assists_w3', 'ra_allowed_assists_w3', 'ra_assists_w5', 'ra_allowed_assists_w5', 'ra_assists_w10', 'ra_allowed_assists_w10', 'rstd_assists_w5', 'ewm_assists_hl5', 'rstd_allowed_assists_w5', 'ewm_allowed_assists_hl5', 'ra_blocks_w1', 'ra_allowed_blocks_w1', 'ra_blocks_w3', 'ra_allowed_blocks_w3', 'ra_blocks_w5', 'ra_allowed_blocks_w5', 'ra_blocks_w10', 'ra_allowed_blocks_w10', 'rstd_blocks_w5', 'ewm_blocks_hl5', 'rstd_allowed_blocks_w5', 'ewm_allowed_blocks_hl5', 'ra_defensiveRebounds_w1', 'ra_allowed_defensiveRebounds_w1', 'ra_defensiveRebounds_w3', 'ra_allowed_defensiveRebounds_w3', 'ra_defensiveRebounds_w5', 'ra_allowed_defensiveRebounds_w5', 'ra_defensiveRebounds_w10', 'ra_allowed_defensiveRebounds_w10', 'rstd_defensiveRebounds_w5', 'ewm_defensiveRebounds_hl5', 'rstd_allowed_defensiveRebounds_w5', 'ewm_allowed_defensiveRebounds_hl5', 'ra_fastBreakPoints_w1', 'ra_allowed_fastBreakPoints_w1', 'ra_fastBreakPoints_w3', 'ra_allowed_fastBreakPoints_w3', 'ra_fastBreakPoints_w5', 'ra_allowed_fastBreakPoints_w5', 'ra_fastBreakPoints_w10', 'ra_allowed_fastBreakPoints_w10', 'rstd_fastBreakPoints_w5', 'ewm_fastBreakPoints_hl5', 'rstd_allowed_fastBreakPoints_w5', 'ewm_allowed_fastBreakPoints_hl5', 'ra_flagrantFouls_w1', 'ra_allowed_flagrantFouls_w1', 'ra_flagrantFouls_w3', 'ra_allowed_flagrantFouls_w3', 'ra_flagrantFouls_w5', 'ra_allowed_flagrantFouls_w5', 'ra_flagrantFouls_w10', 'ra_allowed_flagrantFouls_w10', 'rstd_flagrantFouls_w5', 'ewm_flagrantFouls_hl5', 'rstd_allowed_flagrantFouls_w5', 'ewm_allowed_flagrantFouls_hl5', 'ra_fouls_w1', 'ra_allowed_fouls_w1', 'ra_fouls_w3', 'ra_allowed_fouls_w3', 'ra_fouls_w5', 'ra_allowed_fouls_w5', 'ra_fouls_w10', 'ra_allowed_fouls_w10', 'rstd_fouls_w5', 'ewm_fouls_hl5', 'rstd_allowed_fouls_w5', 'ewm_allowed_fouls_hl5', 'ra_offensiveRebounds_w1', 'ra_allowed_offensiveRebounds_w1', 'ra_offensiveRebounds_w3', 'ra_allowed_offensiveRebounds_w3', 'ra_offensiveRebounds_w5', 'ra_allowed_offensiveRebounds_w5', 'ra_offensiveRebounds_w10', 'ra_allowed_offensiveRebounds_w10', 'rstd_offensiveRebounds_w5', 'ewm_offensiveRebounds_hl5', 'rstd_allowed_offensiveRebounds_w5', 'ewm_allowed_offensiveRebounds_hl5', 'ra_pointsInPaint_w1', 'ra_allowed_pointsInPaint_w1', 'ra_pointsInPaint_w3', 'ra_allowed_pointsInPaint_w3', 'ra_pointsInPaint_w5', 'ra_allowed_pointsInPaint_w5', 'ra_pointsInPaint_w10', 'ra_allowed_pointsInPaint_w10', 'rstd_pointsInPaint_w5', 'ewm_pointsInPaint_hl5', 'rstd_allowed_pointsInPaint_w5', 'ewm_allowed_pointsInPaint_hl5', 'ra_steals_w1', 'ra_allowed_steals_w1', 'ra_steals_w3', 'ra_allowed_steals_w3', 'ra_steals_w5', 'ra_allowed_steals_w5', 'ra_steals_w10', 'ra_allowed_steals_w10', 'rstd_steals_w5', 'ewm_steals_hl5', 'rstd_allowed_steals_w5', 'ewm_allowed_steals_hl5', 'ra_technicalFouls_w1', 'ra_allowed_technicalFouls_w1', 'ra_technicalFouls_w3', 'ra_allowed_technicalFouls_w3', 'ra_technicalFouls_w5', 'ra_allowed_technicalFouls_w5', 'ra_technicalFouls_w10', 'ra_allowed_technicalFouls_w10', 'rstd_technicalFouls_w5', 'ewm_technicalFouls_hl5', 'rstd_allowed_technicalFouls_w5', 'ewm_allowed_technicalFouls_hl5', 'ra_totalRebounds_w1', 'ra_allowed_totalRebounds_w1', 'ra_totalRebounds_w3', 'ra_allowed_totalRebounds_w3', 'ra_totalRebounds_w5', 'ra_allowed_totalRebounds_w5', 'ra_totalRebounds_w10', 'ra_allowed_totalRebounds_w10', 'rstd_totalRebounds_w5', 'ewm_totalRebounds_hl5', 'rstd_allowed_totalRebounds_w5', 'ewm_allowed_totalRebounds_hl5', 'ra_turnoverPoints_w1', 'ra_allowed_turnoverPoints_w1', 'ra_turnoverPoints_w3', 'ra_allowed_turnoverPoints_w3', 'ra_turnoverPoints_w5', 'ra_allowed_turnoverPoints_w5', 'ra_turnoverPoints_w10', 'ra_allowed_turnoverPoints_w10', 'rstd_turnoverPoints_w5', 'ewm_turnoverPoints_hl5', 'rstd_allowed_turnoverPoints_w5', 'ewm_allowed_turnoverPoints_hl5', 'ra_turnovers_w1', 'ra_allowed_turnovers_w1', 'ra_turnovers_w3', 'ra_allowed_turnovers_w3', 'ra_turnovers_w5', 'ra_allowed_turnovers_w5', 'ra_turnovers_w10', 'ra_allowed_turnovers_w10', 'rstd_turnovers_w5', 'ewm_turnovers_hl5', 'rstd_allowed_turnovers_w5', 'ewm_allowed_turnovers_hl5', 'ra_points_1h_w1', 'ra_points_1h_w3', 'ra_points_1h_w5', 'ra_points_1h_w10', 'ra_allowed_points_1h_w1', 'ra_allowed_points_1h_w3', 'ra_allowed_points_1h_w5', 'ra_allowed_points_1h_w10', 'ra_points_2h_w1', 'ra_points_2h_w3', 'ra_points_2h_w5', 'ra_points_2h_w10', 'ra_allowed_points_2h_w1', 'ra_allowed_points_2h_w3', 'ra_allowed_points_2h_w5', 'ra_allowed_points_2h_w10', 'ra_margin_homeonly_w1', 'ra_margin_homeonly_w3', 'ra_margin_homeonly_w5', 'ra_margin_homeonly_w10', 'ra_points_for_w1', 'ra_points_against_w1', 'ra_point_diff_w1', 'ra_points_for_w3', 'ra_points_against_w3', 'ra_point_diff_w3', 'ra_points_for_w5', 'ra_points_against_w5', 'ra_point_diff_w5', 'ra_points_for_w10', 'ra_points_against_w10', 'ra_point_diff_w10', 'ra_margin_w1', 'ra_margin_w3', 'ra_margin_w5', 'ra_margin_w10']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "del game_model_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_info = merged_df[['game_id', 'season', 'date', 'date_utc', 'time_utc', 'neutral_site',\n",
    "       'home', 'away', 'home_1h', 'away_1h', 'home_2h', 'away_2h',\n",
    "       'home_score', 'away_score', 'home_margin', 'away_margin']]\n",
    "\n",
    "del merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_info = game_info.merge(features_1, on=\"game_id\", how=\"left\")\n",
    "del features_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_merge = features_2.copy()\n",
    "home_merge = home_merge.rename(columns=lambda c: f\"{c}_home\" if c not in [\"game_id\", \"team\", \"opponent\"] else c)\n",
    "merged_home = game_info.merge(\n",
    "    home_merge,\n",
    "    left_on=[\"game_id\", \"home\"],\n",
    "    right_on=[\"game_id\", \"team\"],\n",
    "    how=\"left\",\n",
    "    validate=\"1:1\"\n",
    ").drop(columns=[\"team\", \"opponent\"])\n",
    "\n",
    "# --- AWAY TEAM MERGE ---\n",
    "away_merge = features_2.copy()\n",
    "away_merge = away_merge.rename(columns=lambda c: f\"{c}_away\" if c not in [\"game_id\", \"team\", \"opponent\"] else c)\n",
    "game_info = merged_home.merge(\n",
    "    away_merge,\n",
    "    left_on=[\"game_id\", \"away\"],\n",
    "    right_on=[\"game_id\", \"team\"],\n",
    "    how=\"left\",\n",
    "    validate=\"1:1\"\n",
    ").drop(columns=[\"team\", \"opponent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "del away_merge\n",
    "del home_merge\n",
    "del merged_home\n",
    "del features_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_cols = ['game_id', 'team']\n",
    "\n",
    "# --- HOME merge ---\n",
    "home_feats = features_3.copy()\n",
    "home_feats = home_feats.rename(columns=lambda c: f\"{c}_home\" if c not in key_cols else c)\n",
    "\n",
    "out = game_info.merge(\n",
    "    home_feats,\n",
    "    left_on=['game_id', 'home'],\n",
    "    right_on=['game_id', 'team'],\n",
    "    how='left',\n",
    "    validate='1:1'\n",
    ").drop(columns=['team'])\n",
    "\n",
    "# --- AWAY merge ---\n",
    "away_feats = features_3.copy()\n",
    "away_feats = away_feats.rename(columns=lambda c: f\"{c}_away\" if c not in key_cols else c)\n",
    "\n",
    "game_info = out.merge(\n",
    "    away_feats,\n",
    "    left_on=['game_id', 'away'],\n",
    "    right_on=['game_id', 'team'],\n",
    "    how='left',\n",
    "    validate='1:1'\n",
    ").drop(columns=['team'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "del away_feats\n",
    "del home_feats\n",
    "del out\n",
    "del features_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_info.to_csv(\"data/train/train.csv\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03eca27aa3e5b0c2bf98348f6751bc7dc08663828d24c367008019d5f5934307"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

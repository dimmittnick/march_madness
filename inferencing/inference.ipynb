{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  20251103: saved 225 rows to daily_csvs_2026/barttorvik_20251103.csv\n",
      "✔️  20251104: saved 267 rows to daily_csvs_2026/barttorvik_20251104.csv\n",
      "✔️  20251105: saved 284 rows to daily_csvs_2026/barttorvik_20251105.csv\n",
      "✔️  20251106: saved 299 rows to daily_csvs_2026/barttorvik_20251106.csv\n",
      "✔️  20251107: saved 332 rows to daily_csvs_2026/barttorvik_20251107.csv\n",
      "✔️  20251108: saved 363 rows to daily_csvs_2026/barttorvik_20251108.csv\n",
      "✔️  20251109: saved 363 rows to daily_csvs_2026/barttorvik_20251109.csv\n",
      "\n",
      "✅ Done! 2133 total rows saved across days.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Daily Barttovik Ratings ----------\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright, TimeoutError as PWTimeout\n",
    "\n",
    "BASE_URL = (\n",
    "    \"https://barttorvik.com/trank.php\"\n",
    "    \"?year={year}&sort=&hteam=&t2value=&conlimit=All&state=All\"\n",
    "    \"&begin={begin}&end={end}&top=0&revquad=0&quad=5&venue=All&type=All&mingames=0#\"\n",
    ")\n",
    "\n",
    "# ---------- HTML fetch ----------\n",
    "\n",
    "async def goto_and_get_html(page, url: str, table_selector: str = \"table\", timeout_ms: int = 30000) -> str:\n",
    "    await page.goto(url, wait_until=\"domcontentloaded\", timeout=timeout_ms)\n",
    "    try:\n",
    "        await page.wait_for_selector(table_selector, timeout=20000)\n",
    "    except PWTimeout:\n",
    "        await page.wait_for_load_state(\"networkidle\", timeout=10000)\n",
    "\n",
    "    # if we’re still on the verification page, wait a bit\n",
    "    for _ in range(6):\n",
    "        html = await page.content()\n",
    "        if \"Verifying your browser\" not in html and \"js_test_submitted\" not in html:\n",
    "            break\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "    return await page.content()\n",
    "\n",
    "# ---------- Table parsing ----------\n",
    "\n",
    "def parse_first_table(html: str) -> List[List[str]]:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.select_one(\"table\")\n",
    "    if not table:\n",
    "        return []\n",
    "    rows = []\n",
    "    for tr in table.select(\"tr\"):\n",
    "        cells = [c.get_text(strip=True) for c in tr.select(\"th, td\")]\n",
    "        if cells:\n",
    "            rows.append(cells)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def rows_to_dataframe(rows: List[List[str]]) -> pd.DataFrame:\n",
    "    \"\"\"Convert raw scraped rows into a DataFrame\"\"\"\n",
    "    if len(rows) < 3:\n",
    "        return pd.DataFrame()\n",
    "    columns = rows[1]  # second row = headers\n",
    "    data = rows[2:]\n",
    "    max_len = len(columns)\n",
    "    norm = [r[:max_len] + ([\"\"] * (max_len - len(r))) for r in data]\n",
    "    df = pd.DataFrame(norm, columns=columns)\n",
    "    return df\n",
    "\n",
    "# ---------- Orchestrator ----------\n",
    "\n",
    "async def scrape_barttorvik_daily(\n",
    "    year: int = 2021,\n",
    "    begin: str = \"20201101\",\n",
    "    end: str = \"20210313\",\n",
    "    output_dir: str = \"daily_csvs\",\n",
    "    master_csv: str = \"barttorvik_2021_all.csv\",\n",
    "    table_selector: str = \"table\",\n",
    "    headless: bool = True,\n",
    "    pause_sec: float = 3.8\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    start_dt = datetime.strptime(begin, \"%Y%m%d\")\n",
    "    final_dt = datetime.strptime(end, \"%Y%m%d\")\n",
    "\n",
    "    first_write = not os.path.exists(master_csv)\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(channel=\"chrome\", headless=headless)\n",
    "        context = await browser.new_context(\n",
    "            user_agent=(\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                        \"Chrome/119.0.0.0 Safari/537.36\"),\n",
    "            locale=\"en-US\",\n",
    "            timezone_id=\"America/New_York\",\n",
    "        )\n",
    "        page = await context.new_page()\n",
    "\n",
    "        dt = start_dt\n",
    "        total_rows = 0\n",
    "        while dt <= final_dt:\n",
    "            end_str = dt.strftime(\"%Y%m%d\")\n",
    "            url = BASE_URL.format(year=year, begin=begin, end=end_str)\n",
    "\n",
    "            try:\n",
    "                html = await goto_and_get_html(page, url, table_selector=table_selector)\n",
    "                rows = parse_first_table(html)\n",
    "                df = rows_to_dataframe(rows)\n",
    "\n",
    "                if not df.empty:\n",
    "                    df.insert(0, \"Date\", end_str)\n",
    "\n",
    "                    # write individual daily file\n",
    "                    daily_path = os.path.join(output_dir, f\"barttorvik_{end_str}.csv\")\n",
    "                    df.to_csv(daily_path, index=False)\n",
    "                    print(f\"✔️  {end_str}: saved {len(df)} rows to {daily_path}\")\n",
    "\n",
    "                    # append to master CSV\n",
    "                    if first_write:\n",
    "                        df.to_csv(master_csv, index=False)\n",
    "                        first_write = False\n",
    "                    else:\n",
    "                        df.to_csv(master_csv, mode=\"a\", header=False, index=False)\n",
    "\n",
    "                    total_rows += len(df)\n",
    "                else:\n",
    "                    print(f\"⚠️  {end_str}: no data (empty table)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {end_str}: ERROR {e}\")\n",
    "\n",
    "            await asyncio.sleep(pause_sec)\n",
    "            dt += timedelta(days=1)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    print(f\"\\n✅ Done! {total_rows} total rows saved across days.\")\n",
    "\n",
    "# ---------- Run ----------\n",
    "# In Jupyter or async environment:\n",
    "await scrape_barttorvik_daily(\n",
    "    year=2026,\n",
    "    begin=\"20251103\",\n",
    "    end=\"20251109\",\n",
    "    output_dir=\"daily_csvs_2026\",\n",
    "    master_csv=\"s3://collegebasketballinsiders/torvik/2026/team-ratings.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_torvik_2026_df = pd.read_csv(\"s3://collegebasketballinsiders/torvik/2026/team-ratings.csv\")\n",
    "\n",
    "daily_torvik_2026_df = daily_torvik_2026_df[daily_torvik_2026_df['Team'] != \"Team\"]\n",
    "daily_torvik_2026_df['Team'] = daily_torvik_2026_df['Team'].str.extract(r'^([A-Za-z\\s.&]+)')[0].str.strip()\n",
    "daily_torvik_2026_df['WAB'] = daily_torvik_2026_df['WAB'].str.replace(\"+\",\"\", regex=False).astype(\"float\")\n",
    "daily_torvik_2026_df['season'] = 2026\n",
    "daily_torvik_2026_df = daily_torvik_2026_df[['season','Date', 'Team', 'Rk', 'Conf', 'G', 'AdjOE', 'AdjDE', 'Barthag',\n",
    "       'EFG%', 'EFGD%', 'TOR', 'TORD', 'ORB', 'DRB', 'FTR', 'FTRD', '2P%',\n",
    "       '2P%D', '3P%', '3P%D', '3PR', '3PRD', 'Adj T.', 'WAB']].sort_values([\"Date\",\"Team\"], ascending=True)\n",
    "daily_torvik_2026_df.columns = ['season', 'date', 'team', 'rank', 'conf', 'games', 'adj_off_eff', 'adj_def_eff', 'barthag',\n",
    "       'efg_pct', 'efgd_pct', 'tor', 'tord', 'orb', 'drb', 'ftr', 'ftrd', 'two_pt_pct',\n",
    "       'two_pt_def_pct', 'three_pt_pct', 'three_pt_def_pct', 'three_pt_rt', 'three_pt_def_rt', 'adj_tempo', 'wab']\n",
    "\n",
    "daily_torvik_2026_df.to_csv(\"s3://collegebasketballinsiders/torvik/2026/team-ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import datetime as dt\n",
    "from typing import Iterable, Optional, Dict, Any, List\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Optional .env support\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "API_KEY = os.getenv(\"KENPOM_API_KEY\")\n",
    "\n",
    "API_BASE = \"https://kenpom.com/api.php\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "\n",
    "\n",
    "def session_with_retries(total=5, backoff=0.5):\n",
    "    s = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=total,\n",
    "        connect=total,\n",
    "        read=total,\n",
    "        backoff_factor=backoff,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"],\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    return s\n",
    "\n",
    "def season_date_window(y: int,\n",
    "                       start: Optional[str] = None,\n",
    "                       end: Optional[str] = None) -> Iterable[dt.date]:\n",
    "    # Default window: Oct 1 of previous year → Apr 15 of season’s end year\n",
    "    if start is None:\n",
    "        start = f\"{y-1}-11-01\"\n",
    "    if end is None:\n",
    "        end = f\"{y}-04-15\"\n",
    "    d0 = dt.date.fromisoformat(start)\n",
    "    d1 = dt.date.fromisoformat(end)\n",
    "    cur = d0\n",
    "    one = dt.timedelta(days=1)\n",
    "    while cur <= d1:\n",
    "        yield cur\n",
    "        cur += one\n",
    "\n",
    "def fetch_archive_for_date(s: requests.Session, day: dt.date) -> List[Dict[str, Any]]:\n",
    "    params = {\"endpoint\": \"archive\", \"d\": day.isoformat()}\n",
    "    r = s.get(API_BASE, params=params, headers=HEADERS, timeout=30)\n",
    "    if r.status_code == 404:\n",
    "        return []  # day not available\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    if not isinstance(data, list):\n",
    "        return []\n",
    "    return data\n",
    "\n",
    "def fetch_preseason_for_year(s: requests.Session, y: int) -> List[Dict[str, Any]]:\n",
    "    params = {\"endpoint\": \"archive\", \"preseason\": \"true\", \"y\": y}\n",
    "    r = s.get(API_BASE, params=params, headers=HEADERS, timeout=30)\n",
    "    if r.status_code == 404:\n",
    "        return []\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    if not isinstance(data, list):\n",
    "        return []\n",
    "    return data\n",
    "\n",
    "def fetch_teams_for_year(s: requests.Session, y: int) -> pd.DataFrame:\n",
    "    params = {\"endpoint\": \"teams\", \"y\": y}\n",
    "    r = s.get(API_BASE, params=params, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return pd.DataFrame(data) if data else pd.DataFrame(columns=[\"Season\",\"TeamName\",\"TeamID\",\"ConfShort\"])\n",
    "\n",
    "def normalize_rows(rows: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.json_normalize(rows)\n",
    "    # Ensure consistent columns; add ArchiveDate if missing (e.g., preseason response should include it per docs)\n",
    "    if \"ArchiveDate\" not in df.columns and \"DataThrough\" in df.columns:\n",
    "        df[\"ArchiveDate\"] = df[\"DataThrough\"]  # fallback\n",
    "    # Keep essential identifiers first\n",
    "    preferred = [\n",
    "        \"ArchiveDate\", \"Season\", \"TeamName\", \"Seed\", \"ConfShort\", \"Event\",\n",
    "        \"AdjEM\", \"RankAdjEM\",\n",
    "        \"AdjOE\", \"RankAdjOE\",\n",
    "        \"AdjDE\", \"RankAdjDE\",\n",
    "        \"AdjTempo\", \"RankAdjTempo\",\n",
    "        \"AdjEMFinal\", \"RankAdjEMFinal\",\n",
    "        \"AdjOEFinal\", \"RankAdjOEFinal\",\n",
    "        \"AdjDEFinal\", \"RankAdjDEFinal\",\n",
    "        \"AdjTempoFinal\", \"RankAdjTempoFinal\",\n",
    "        \"RankChg\", \"AdjEMChg\", \"AdjTChg\",\n",
    "        \"Preseason\"\n",
    "    ]\n",
    "    # Reorder if present\n",
    "    cols = [c for c in preferred if c in df.columns] + [c for c in df.columns if c not in preferred]\n",
    "    df = df[cols]\n",
    "    # Ensure date type\n",
    "    if \"ArchiveDate\" in df.columns:\n",
    "        df[\"ArchiveDate\"] = pd.to_datetime(df[\"ArchiveDate\"], errors=\"coerce\").dt.date\n",
    "    return df\n",
    "\n",
    "def scrape_daily_kenpom(y: int,\n",
    "                        include_preseason: bool = True,\n",
    "                        start: Optional[str] = None,\n",
    "                        end: Optional[str] = None,\n",
    "                        sleep_sec: float = 0.25) -> pd.DataFrame:\n",
    "    if not API_KEY:\n",
    "        raise RuntimeError(\"Set KENPOM_API_KEY in your environment (export KENPOM_API_KEY=...).\")\n",
    "    s = session_with_retries()\n",
    "\n",
    "    # Optional: team list (useful for mapping / validation)\n",
    "    teams_df = fetch_teams_for_year(s, y)\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    if include_preseason:\n",
    "        pre = fetch_preseason_for_year(s, y)\n",
    "        pdf = normalize_rows(pre)\n",
    "        if not pdf.empty:\n",
    "            # Mark as preseason if field not provided\n",
    "            if \"Preseason\" not in pdf.columns:\n",
    "                pdf[\"Preseason\"] = \"true\"\n",
    "            frames.append(pdf)\n",
    "\n",
    "    for day in season_date_window(y, start=start, end=end):\n",
    "        try:\n",
    "            rows = fetch_archive_for_date(s, day)\n",
    "        except requests.HTTPError as e:\n",
    "            # Skip on hard errors\n",
    "            continue\n",
    "        df = normalize_rows(rows)\n",
    "        if not df.empty:\n",
    "            # Ensure a season column exists\n",
    "            if \"Season\" not in df.columns:\n",
    "                df[\"Season\"] = y\n",
    "            frames.append(df)\n",
    "        time.sleep(sleep_sec)\n",
    "\n",
    "    if not frames:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    out = pd.concat(frames, ignore_index=True).drop_duplicates()\n",
    "\n",
    "    # Optional: join TeamID if you want a stable key\n",
    "    if \"TeamName\" in out.columns and not teams_df.empty:\n",
    "        out = out.merge(\n",
    "            teams_df[[\"TeamName\", \"TeamID\"]],\n",
    "            on=\"TeamName\",\n",
    "            how=\"left\",\n",
    "            validate=\"m:1\"\n",
    "        )\n",
    "\n",
    "    # Sort for readability\n",
    "    sort_cols = [c for c in [\"ArchiveDate\", \"Season\", \"RankAdjEM\", \"TeamName\"] if c in out.columns]\n",
    "    if sort_cols:\n",
    "        out = out.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "def _get_api_key() -> str:\n",
    "    key = os.getenv(\"KENPOM_API_KEY\")\n",
    "    if not key:\n",
    "        raise RuntimeError(\"KENPOM_API_KEY not set. Use a .env file or export it.\")\n",
    "    return key\n",
    "\n",
    "def _session_with_retries(total: int = 5, backoff: float = 0.5) -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    r = Retry(\n",
    "        total=total, connect=total, read=total,\n",
    "        backoff_factor=backoff,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"],\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=r))\n",
    "    return s\n",
    "\n",
    "def fetch_height_for_season(y: int, c: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return Height snapshot for season y (optionally filtered by conference short name c).\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {_get_api_key()}\"}\n",
    "    params = {\"endpoint\": \"height\", \"y\": y}\n",
    "    if c:\n",
    "        params[\"c\"] = c\n",
    "    s = _session_with_retries()\n",
    "    resp = s.get(API_BASE, params=params, headers=headers, timeout=45)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    df = pd.json_normalize(data) if isinstance(data, list) else pd.DataFrame()\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # Light normalization (no file I/O)\n",
    "    if \"DataThrough\" in df.columns:\n",
    "        df[\"DataThrough\"] = pd.to_datetime(df[\"DataThrough\"], errors=\"coerce\").dt.date\n",
    "    for col in [x for x in [\"TeamName\", \"ConfShort\"] if x in df.columns]:\n",
    "        df[col] = df[col].astype(\"string\")\n",
    "    if \"Season\" in df.columns:\n",
    "        df[\"Season\"] = pd.to_numeric(df[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    # Rank columns to Int64; metric columns to float64\n",
    "    for col in [c for c in df.columns if c.lower().endswith(\"rank\")]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "    for col in [\"AvgHgt\",\"HgtEff\",\"Hgt5\",\"Hgt4\",\"Hgt3\",\"Hgt2\",\"Hgt1\",\"Exp\",\"Bench\",\"Continuity\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"float64\")\n",
    "    return df\n",
    "\n",
    "def fetch_height_range(start_season: int, end_season: int, c: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loop seasons [start_season, end_season] inclusive and return one concatenated DataFrame.\n",
    "    \"\"\"\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    for y in range(start_season, end_season + 1):\n",
    "        h = fetch_height_for_season(y, c=c)\n",
    "        if not h.empty:\n",
    "            frames.append(h)\n",
    "    if not frames:\n",
    "        return pd.DataFrame()\n",
    "    out = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    # Nice column order (purely cosmetic)\n",
    "    preferred = [\n",
    "        \"Season\",\"TeamName\",\"ConfShort\",\"DataThrough\",\n",
    "        \"AvgHgt\",\"AvgHgtRank\",\"HgtEff\",\"HgtEffRank\",\n",
    "        \"Hgt5\",\"Hgt5Rank\",\"Hgt4\",\"Hgt4Rank\",\"Hgt3\",\"Hgt3Rank\",\"Hgt2\",\"Hgt2Rank\",\"Hgt1\",\"Hgt1Rank\",\n",
    "        \"Exp\",\"ExpRank\",\"Bench\",\"BenchRank\",\"Continuity\",\"RankContinuity\"\n",
    "    ]\n",
    "    cols = [c for c in preferred if c in out.columns] + [c for c in out.columns if c not in preferred]\n",
    "    return out.reindex(columns=cols)\n",
    "\n",
    "\n",
    "year = 2026\n",
    "df = scrape_daily_kenpom(year, include_preseason=True)\n",
    "df = df[['ArchiveDate','Season','TeamID','TeamName','Seed','ConfShort','Event',\n",
    "         'AdjEM','RankAdjEM','AdjOE','RankAdjOE','AdjDE','RankAdjDE',\n",
    "         'AdjTempo','RankAdjTempo']].copy()\n",
    "\n",
    "df.to_csv(f\"s3://collegebasketballinsiders/kenpom/{year}/team-ratings.csv\")\n",
    "\n",
    "df = fetch_height_range(year,year)\n",
    "df.to_csv(f\"s3://collegebasketballinsiders/kenpom/{year}/team-height.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— 20251102: no games\n",
      "✅ 20251103: saved 169 games → s3://collegebasketballinsiders/daily-box-score-ids/20251103/game_ids.csv and s3://collegebasketballinsiders/daily-box-score-ids/20251103/game_ids.txt\n",
      "✅ 20251104: saved 36 games → s3://collegebasketballinsiders/daily-box-score-ids/20251104/game_ids.csv and s3://collegebasketballinsiders/daily-box-score-ids/20251104/game_ids.txt\n",
      "✅ 20251105: saved 35 games → s3://collegebasketballinsiders/daily-box-score-ids/20251105/game_ids.csv and s3://collegebasketballinsiders/daily-box-score-ids/20251105/game_ids.txt\n",
      "✅ 20251106: saved 44 games → s3://collegebasketballinsiders/daily-box-score-ids/20251106/game_ids.csv and s3://collegebasketballinsiders/daily-box-score-ids/20251106/game_ids.txt\n",
      "✅ 20251107: saved 76 games → s3://collegebasketballinsiders/daily-box-score-ids/20251107/game_ids.csv and s3://collegebasketballinsiders/daily-box-score-ids/20251107/game_ids.txt\n",
      "✅ 20251108: saved 69 games → s3://collegebasketballinsiders/daily-box-score-ids/20251108/game_ids.csv and s3://collegebasketballinsiders/daily-box-score-ids/20251108/game_ids.txt\n",
      "✅ 20251109: saved 34 games → s3://collegebasketballinsiders/daily-box-score-ids/20251109/game_ids.csv and s3://collegebasketballinsiders/daily-box-score-ids/20251109/game_ids.txt\n",
      "\n",
      "Done. Wrote 463 games across 7 days into 's3://collegebasketballinsiders/daily-box-score-ids/<YYYYMMDD>/' folders.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Game Ids (dated subfolders) ----------\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Optional import: only needed if OUT_DIR is s3://...\n",
    "try:\n",
    "    import s3fs\n",
    "except Exception:\n",
    "    s3fs = None\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "START_DATE = \"20251102\"  # inclusive\n",
    "END_DATE   = \"20251109\"  # inclusive\n",
    "GROUP = 50               # 50 = NCAA Division I\n",
    "OUT_DIR = \"s3://collegebasketballinsiders/daily-box-score-ids\"\n",
    "OVERWRITE = False        # True to overwrite existing daily files\n",
    "PAUSE_SECONDS = 3.4      # be polite (optional)\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/119.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def is_s3_path(path: str) -> bool:\n",
    "    return isinstance(path, str) and path.lower().startswith(\"s3://\")\n",
    "\n",
    "def s3_join(base: str, *parts: str) -> str:\n",
    "    \"\"\"Join S3 URI segments without mangling the scheme/bucket.\"\"\"\n",
    "    base = base.rstrip(\"/\")\n",
    "    tail = \"/\".join(p.strip(\"/\") for p in parts if p is not None)\n",
    "    return f\"{base}/{tail}\" if tail else base\n",
    "\n",
    "class FS:\n",
    "    \"\"\"\n",
    "    Tiny facade over local fs vs S3 so the rest of the code stays clean.\n",
    "    \"\"\"\n",
    "    def __init__(self, root: str):\n",
    "        self.root = root\n",
    "        self._is_s3 = is_s3_path(root)\n",
    "        self._fs = None\n",
    "        if self._is_s3:\n",
    "            if s3fs is None:\n",
    "                raise RuntimeError(\n",
    "                    \"s3fs is required for S3 paths. Install with: pip install s3fs\"\n",
    "                )\n",
    "            # Uses standard AWS credential chain (env vars, ~/.aws/credentials, IAM role, etc.)\n",
    "            self._fs = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    def join(self, *parts: str) -> str:\n",
    "        if self._is_s3:\n",
    "            return s3_join(*parts)\n",
    "        return os.path.join(*parts)\n",
    "\n",
    "    def mkdir(self, path: str):\n",
    "        if self._is_s3:\n",
    "            # No-op: S3 prefixes are virtual; objects create \"folders\"\n",
    "            return\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    def exists(self, path: str) -> bool:\n",
    "        if self._is_s3:\n",
    "            return self._fs.exists(path)\n",
    "        return os.path.exists(path)\n",
    "\n",
    "    def open_write_text(self, path: str):\n",
    "        if self._is_s3:\n",
    "            # text mode with utf-8 encoding\n",
    "            return self._fs.open(path, \"w\")\n",
    "        # ensure local dir exists\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        return open(path, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    def to_csv(self, df: pd.DataFrame, path: str):\n",
    "        if self._is_s3:\n",
    "            with self._fs.open(path, \"w\") as f:\n",
    "                df.to_csv(f, index=False)\n",
    "        else:\n",
    "            # ensure local dir exists\n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "            df.to_csv(path, index=False)\n",
    "\n",
    "# ---------------- Data fetchers ----------------\n",
    "def fetch_games_api(date_yyyymmdd: str, group: int = GROUP) -> pd.DataFrame:\n",
    "    \"\"\"Preferred: ESPN public JSON API (no HTML parsing).\"\"\"\n",
    "    url = (\n",
    "        \"https://site.api.espn.com/apis/v2/sports/basketball/mens-college-basketball/\"\n",
    "        f\"scoreboard?dates={date_yyyymmdd}&groups={group}\"\n",
    "    )\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    rows = []\n",
    "    for e in data.get(\"events\", []):\n",
    "        gid = e.get(\"id\")\n",
    "        comp = (e.get(\"competitions\") or [{}])[0]\n",
    "        comps = comp.get(\"competitors\", [])\n",
    "        home = next((c for c in comps if c.get(\"homeAway\") == \"home\"), {})\n",
    "        away = next((c for c in comps if c.get(\"homeAway\") == \"away\"), {})\n",
    "        rows.append({\n",
    "            \"game_id\": gid,\n",
    "            \"home_team\": (home.get(\"team\") or {}).get(\"displayName\"),\n",
    "            \"away_team\": (away.get(\"team\") or {}).get(\"displayName\"),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def fetch_scoreboard_html(date_yyyymmdd: str, group: int = GROUP) -> str:\n",
    "    \"\"\"Fallback: fetch the scoreboard HTML for the date/group.\"\"\"\n",
    "    url = f\"https://www.espn.com/mens-college-basketball/scoreboard/_/date/{date_yyyymmdd}/group/{group}\"\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def parse_games_from_html(html: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse game blocks from server-rendered DOM.\n",
    "    <section class=\"Scoreboard\" id=\"<game_id>\">…</section>\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    games = []\n",
    "\n",
    "    # Primary: section blocks with game id\n",
    "    for sec in soup.select(\"section.Scoreboard[id]\"):\n",
    "        gid = sec.get(\"id\")\n",
    "        away = sec.select_one(\".ScoreboardScoreCell__Item--away .ScoreCell__TeamName--shortDisplayName\")\n",
    "        home = sec.select_one(\".ScoreboardScoreCell__Item--home .ScoreCell__TeamName--shortDisplayName\")\n",
    "        games.append({\n",
    "            \"game_id\": gid,\n",
    "            \"home_team\": home.get_text(strip=True) if home else None,\n",
    "            \"away_team\": away.get_text(strip=True) if away else None,\n",
    "        })\n",
    "\n",
    "    # Secondary: backup to anchor pattern if nothing found\n",
    "    if not games:\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            m = re.search(r\"/mens-college-basketball/game/_/gameId/(\\d+)\", a[\"href\"])\n",
    "            if m:\n",
    "                games.append({\"game_id\": m.group(1), \"home_team\": None, \"away_team\": None})\n",
    "\n",
    "    return pd.DataFrame(games)\n",
    "\n",
    "def get_games_for_date(date_yyyymmdd: str, group: int = GROUP) -> pd.DataFrame:\n",
    "    \"\"\"Try API first; if empty/error, fall back to HTML.\"\"\"\n",
    "    try:\n",
    "        df_api = fetch_games_api(date_yyyymmdd, group=group)\n",
    "        if not df_api.empty:\n",
    "            df_api.insert(0, \"date\", date_yyyymmdd)\n",
    "            return df_api\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        html = fetch_scoreboard_html(date_yyyymmdd, group=group)\n",
    "        df_html = parse_games_from_html(html)\n",
    "        if not df_html.empty:\n",
    "            df_html.insert(0, \"date\", date_yyyymmdd)\n",
    "        return df_html\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def daterange(start_yyyymmdd: str, end_yyyymmdd: str):\n",
    "    start = datetime.strptime(start_yyyymmdd, \"%Y%m%d\")\n",
    "    end = datetime.strptime(end_yyyymmdd, \"%Y%m%d\")\n",
    "    d = start\n",
    "    while d <= end:\n",
    "        yield d.strftime(\"%Y%m%d\")\n",
    "        d += timedelta(days=1)\n",
    "\n",
    "# ---------------- Main ----------------\n",
    "def main():\n",
    "    fs = FS(OUT_DIR)\n",
    "\n",
    "    total_days = 0\n",
    "    total_games = 0\n",
    "\n",
    "    for day in daterange(START_DATE, END_DATE):\n",
    "        # Make a dated subfolder like: daily-box-score-ids/20251103/\n",
    "        if is_s3_path(OUT_DIR):\n",
    "            day_dir = s3_join(OUT_DIR, day)\n",
    "        else:\n",
    "            day_dir = os.path.join(OUT_DIR, day)\n",
    "            fs.mkdir(day_dir)  # local only\n",
    "\n",
    "        # Files we’ll write inside that folder\n",
    "        csv_path = fs.join(day_dir, \"game_ids.csv\") if not is_s3_path(OUT_DIR) else s3_join(day_dir, \"game_ids.csv\")\n",
    "        txt_path = fs.join(day_dir, \"game_ids.txt\") if not is_s3_path(OUT_DIR) else s3_join(day_dir, \"game_ids.txt\")\n",
    "\n",
    "        if (not OVERWRITE) and fs.exists(csv_path):\n",
    "            print(f\"⏭️  {day}: {csv_path} exists, skipping (set OVERWRITE=True to redo)\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = get_games_for_date(day, group=GROUP)\n",
    "            if df is None or df.empty:\n",
    "                print(f\"— {day}: no games\")\n",
    "            else:\n",
    "                # Normalize columns/order\n",
    "                df = df[[\"date\", \"game_id\", \"home_team\", \"away_team\"]]\n",
    "\n",
    "                # Write CSV\n",
    "                fs.to_csv(df, csv_path)\n",
    "\n",
    "                # Also write a plain text list of IDs (one per line)\n",
    "                with fs.open_write_text(txt_path) as f:\n",
    "                    for gid in df[\"game_id\"].astype(str):\n",
    "                        f.write(gid + \"\\n\")\n",
    "\n",
    "                total_days += 1\n",
    "                total_games += len(df)\n",
    "                print(f\"✅ {day}: saved {len(df)} games → {csv_path} and {txt_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {day}: ERROR {e}\")\n",
    "\n",
    "        # be polite to the server\n",
    "        if PAUSE_SECONDS:\n",
    "            try:\n",
    "                time.sleep(PAUSE_SECONDS)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    print(f\"\\nDone. Wrote {total_games} games across {total_days} days into '{OUT_DIR}/<YYYYMMDD>/' folders.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: saving 401812600\n",
      "2: saving 401828296\n",
      "3: saving 401826732\n",
      "4: saving 401823022\n",
      "5: saving 401822758\n",
      "6: saving 401812574\n",
      "7: saving 401828576\n",
      "8: saving 401823485\n",
      "9: saving 401823417\n",
      "10: saving 401823393\n",
      "11: saving 401823256\n",
      "12: saving 401820542\n",
      "13: saving 401818548\n",
      "14: saving 401813320\n",
      "15: saving 401813287\n",
      "16: saving 401813254\n",
      "17: saving 401811103\n",
      "18: saving 401826896\n",
      "19: saving 401825577\n",
      "20: saving 401819916\n",
      "21: saving 401819814\n",
      "22: saving 401828912\n",
      "23: saving 401823515\n",
      "24: saving 401828276\n",
      "25: saving 401824089\n",
      "26: saving 401823555\n",
      "27: saving 401827088\n",
      "28: saving 401826772\n",
      "29: saving 401823858\n",
      "30: saving 401823808\n",
      "31: saving 401827257\n",
      "32: saving 401829390\n",
      "33: saving 401819824\n",
      "34: saving 401824896\n"
     ]
    }
   ],
   "source": [
    "# ---------- Game Box Scores (S3-ready) ----------\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "\n",
    "try:\n",
    "    import s3fs\n",
    "except Exception:\n",
    "    s3fs = None\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "SUMMARY_URL = \"https://site.api.espn.com/apis/site/v2/sports/basketball/mens-college-basketball/summary?event={gid}\"\n",
    "UA_HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# Where to SAVE box score CSVs (can be s3://bucket/prefix or local)\n",
    "OUT_DIR = \"s3://collegebasketballinsiders/boxscores-2026\"\n",
    "\n",
    "# Where to READ daily game-id CSVs (can be s3://bucket/prefix or local)\n",
    "# Example matches output from your first script: s3://.../daily-box-score-ids/<YYYYMMDD>/*.csv\n",
    "IDS_DIR = \"s3://collegebasketballinsiders/daily-box-score-ids\"\n",
    "IDS_DATE = \"20251109\"  # which day’s game_id CSVs to ingest\n",
    "\n",
    "# ----------------------------\n",
    "# Simple FS facade (local vs S3)\n",
    "# ----------------------------\n",
    "def is_s3_path(path: str) -> bool:\n",
    "    return isinstance(path, str) and path.lower().startswith(\"s3://\")\n",
    "\n",
    "def s3_join(base: str, *parts: str) -> str:\n",
    "    base = base.rstrip(\"/\")\n",
    "    tail = \"/\".join(p.strip(\"/\") for p in parts if p is not None)\n",
    "    return f\"{base}/{tail}\" if tail else base\n",
    "# --- Add this helper near your FS code ---\n",
    "def _ensure_s3_scheme(p: str) -> str:\n",
    "    if p and not p.lower().startswith(\"s3://\"):\n",
    "        return f\"s3://{p}\"\n",
    "    return p\n",
    "\n",
    "class FS:\n",
    "    def __init__(self, root_hint: Optional[str] = None):\n",
    "        self._is_s3 = is_s3_path(root_hint) if root_hint else False\n",
    "        self._fs = None\n",
    "        if self._is_s3:\n",
    "            if s3fs is None:\n",
    "                raise RuntimeError(\"s3fs is required for S3 paths. Install: pip install s3fs\")\n",
    "            self._fs = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    def join(self, *parts: str) -> str:\n",
    "        if self._is_s3:\n",
    "            return s3_join(*parts)\n",
    "        return os.path.join(*parts)\n",
    "\n",
    "    def mkdirs_for_file(self, path: str):\n",
    "        if not self._is_s3:\n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        # S3: no-op, prefixes are virtual\n",
    "\n",
    "    def open_text_write(self, path: str):\n",
    "        if self._is_s3:\n",
    "            return self._fs.open(path, \"w\")\n",
    "        self.mkdirs_for_file(path)\n",
    "        return open(path, \"w\", newline=\"\", encoding=\"utf-8\")\n",
    "\n",
    "    def open_binary_read(self, path: str):\n",
    "        if self._is_s3:\n",
    "            return self._fs.open(path, \"rb\")\n",
    "        return open(path, \"rb\")\n",
    "\n",
    "    def exists(self, path: str) -> bool:\n",
    "        if self._is_s3:\n",
    "            return self._fs.exists(path)\n",
    "        return os.path.exists(path)\n",
    "\n",
    "    def glob(self, pattern: str) -> List[str]:\n",
    "        if self._is_s3:\n",
    "            results = self._fs.glob(pattern)\n",
    "            # Some s3fs versions return keys without the scheme. Normalize.\n",
    "            return [_ensure_s3_scheme(p) for p in results]\n",
    "        return glob.glob(pattern)\n",
    "\n",
    "# Instantiate two facades: one for OUT_DIR and one for IDS_DIR\n",
    "fs_out = FS(OUT_DIR)\n",
    "fs_ids = FS(IDS_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# Fetch\n",
    "# ----------------------------\n",
    "def _get_summary(game_id: str) -> Dict[str, Any]:\n",
    "    r = requests.get(SUMMARY_URL.format(gid=game_id), headers=UA_HEADERS, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _first_comp(summary: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    return (summary.get(\"header\", {}).get(\"competitions\") or [{}])[0]\n",
    "\n",
    "def _competitors(summary: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    return _first_comp(summary).get(\"competitors\") or []\n",
    "\n",
    "def _home_comp(comp_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    return next((c for c in comp_list if c.get(\"homeAway\") == \"home\"), comp_list[0] if comp_list else {})\n",
    "\n",
    "def _away_comp(comp_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    return next((c for c in comp_list if c.get(\"homeAway\") == \"away\"), comp_list[1] if len(comp_list) > 1 else {})\n",
    "\n",
    "def _team_name(team_obj: Dict[str, Any]) -> str:\n",
    "    return team_obj.get(\"displayName\") or team_obj.get(\"name\") or team_obj.get(\"location\") or \"\"\n",
    "\n",
    "def _parse_value_to_int(val: Any) -> Optional[int]:\n",
    "    if val is None:\n",
    "        return None\n",
    "    try:\n",
    "        return int(val)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return int(str(val).split(\".\")[0])\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def _extract_period_num(item: Dict[str, Any]) -> Optional[int]:\n",
    "    p = item.get(\"period\")\n",
    "    if isinstance(p, dict):\n",
    "        return _parse_value_to_int(p.get(\"number\"))\n",
    "    return _parse_value_to_int(p)\n",
    "\n",
    "def _extract_score_from_item(item: Dict[str, Any]) -> Optional[int]:\n",
    "    for k in (\"value\", \"displayValue\", \"score\"):\n",
    "        if k in item:\n",
    "            return _parse_value_to_int(item[k])\n",
    "    return None\n",
    "\n",
    "def _half_scores(competitor: Dict[str, Any]) -> Tuple[Optional[int], Optional[int]]:\n",
    "    lines = competitor.get(\"linescores\") or competitor.get(\"scoreByPeriod\") or []\n",
    "    if not isinstance(lines, list) or not lines:\n",
    "        return (None, None)\n",
    "\n",
    "    by_period: Dict[int, int] = {}\n",
    "    fallback_order_vals: List[int] = []\n",
    "\n",
    "    for it in lines:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        val = _extract_score_from_item(it)\n",
    "        if val is not None:\n",
    "            fallback_order_vals.append(val)\n",
    "        pnum = _extract_period_num(it)\n",
    "        if pnum is not None and val is not None:\n",
    "            by_period[pnum] = val\n",
    "\n",
    "    h1 = by_period.get(1)\n",
    "    h2 = by_period.get(2)\n",
    "    if h1 is not None or h2 is not None:\n",
    "        return h1, h2\n",
    "\n",
    "    if len(fallback_order_vals) >= 2:\n",
    "        return fallback_order_vals[0], fallback_order_vals[1]\n",
    "\n",
    "    return (None, None)\n",
    "\n",
    "def _write_csv(path: str, rows: List[Dict[str, Any]], header: List[str] = None):\n",
    "    \"\"\"\n",
    "    S3/local-aware CSV writer (uses fs_out).\n",
    "    \"\"\"\n",
    "    fs_out.mkdirs_for_file(path)\n",
    "\n",
    "    if not rows:\n",
    "        if header:\n",
    "            with fs_out.open_text_write(path) as f:\n",
    "                csv.DictWriter(f, fieldnames=header).writeheader()\n",
    "        else:\n",
    "            # create empty file\n",
    "            with fs_out.open_text_write(path) as f:\n",
    "                pass\n",
    "        return\n",
    "\n",
    "    cols = header or list({k for r in rows for k in r.keys()})\n",
    "    with fs_out.open_text_write(path) as f:\n",
    "        w = csv.DictWriter(f, fieldnames=cols)\n",
    "        w.writeheader()\n",
    "        w.writerows(rows)\n",
    "\n",
    "# ----------------------------\n",
    "# Parsers (summary-only)\n",
    "# ----------------------------\n",
    "def parse_game_info(summary: Dict[str, Any], game_id: str) -> List[Dict[str, Any]]:\n",
    "    header = summary.get(\"header\", {}) or {}\n",
    "    comp = _first_comp(summary)\n",
    "    comps = _competitors(summary)\n",
    "\n",
    "    home = _home_comp(comps)\n",
    "    away = _away_comp(comps)\n",
    "\n",
    "    dt_iso = (header.get(\"competitions\", [{}])[0].get(\"date\")\n",
    "              or header.get(\"date\")\n",
    "              or \"\")\n",
    "    date_utc, time_utc = \"\", \"\"\n",
    "    if \"T\" in dt_iso:\n",
    "        date_utc, rest = dt_iso.split(\"T\", 1)\n",
    "        time_utc = rest\n",
    "    else:\n",
    "        date_utc = dt_iso\n",
    "\n",
    "    home_score = _parse_value_to_int(home.get(\"score\"))\n",
    "    away_score = _parse_value_to_int(away.get(\"score\"))\n",
    "\n",
    "    home_1h, home_2h = _half_scores(home)\n",
    "    away_1h, away_2h = _half_scores(away)\n",
    "\n",
    "    row = {\n",
    "        \"game_id\": game_id,\n",
    "        \"date_utc\": date_utc,\n",
    "        \"time_utc\": time_utc,\n",
    "        \"neutral_site\": bool(comp.get(\"neutralSite\")),\n",
    "        \"home_team\": _team_name((home.get(\"team\") or {})),\n",
    "        \"away_team\": _team_name((away.get(\"team\") or {})),\n",
    "        \"home_1h\": home_1h,\n",
    "        \"away_1h\": away_1h,\n",
    "        \"home_2h\": home_2h,\n",
    "        \"away_2h\": away_2h,\n",
    "        \"home_score\": home_score,\n",
    "        \"away_score\": away_score,\n",
    "    }\n",
    "    return [row]\n",
    "\n",
    "def parse_team_stats(summary: Dict[str, Any], game_id: str) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    teams = (summary.get(\"boxscore\", {}) or {}).get(\"teams\") or []\n",
    "    for t in teams:\n",
    "        team_obj = t.get(\"team\", {}) or {}\n",
    "        stats = t.get(\"statistics\") or []\n",
    "        row = {\n",
    "            \"game_id\": game_id,\n",
    "            \"team_id\": team_obj.get(\"id\"),\n",
    "            \"team\": _team_name(team_obj),\n",
    "            \"abbreviation\": team_obj.get(\"abbreviation\"),\n",
    "            \"homeAway\": t.get(\"homeAway\"),\n",
    "            \"displayOrder\": t.get(\"displayOrder\"),\n",
    "        }\n",
    "        for s in stats:\n",
    "            key = s.get(\"name\") or s.get(\"label\")\n",
    "            if key:\n",
    "                row[key] = s.get(\"displayValue\")\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def parse_player_stats(summary: Dict[str, Any], game_id: str) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    players_blocks = (summary.get(\"boxscore\", {}) or {}).get(\"players\") or []\n",
    "\n",
    "    for team_block in players_blocks:\n",
    "        team_obj = team_block.get(\"team\", {}) or {}\n",
    "        team_id = team_obj.get(\"id\")\n",
    "        team_name = _team_name(team_obj)\n",
    "        team_abbr = team_obj.get(\"abbreviation\")\n",
    "\n",
    "        for stats_pack in team_block.get(\"statistics\") or []:\n",
    "            keys = stats_pack.get(\"keys\") or []\n",
    "            for ath in stats_pack.get(\"athletes\") or []:\n",
    "                athlete = ath.get(\"athlete\", {}) or {}\n",
    "                values = ath.get(\"stats\") or []\n",
    "                row = {\n",
    "                    \"game_id\": game_id,\n",
    "                    \"team_id\": team_id,\n",
    "                    \"team\": team_name,\n",
    "                    \"abbreviation\": team_abbr,\n",
    "                    \"athlete_id\": athlete.get(\"id\"),\n",
    "                    \"athlete_name\": athlete.get(\"displayName\"),\n",
    "                    \"jersey\": athlete.get(\"jersey\"),\n",
    "                    \"position\": (athlete.get(\"position\") or {}).get(\"abbreviation\") or (athlete.get(\"position\") or {}).get(\"displayName\"),\n",
    "                    \"starter\": ath.get(\"starter\"),\n",
    "                    \"didNotPlay\": ath.get(\"didNotPlay\"),\n",
    "                    \"ejected\": ath.get(\"ejected\"),\n",
    "                }\n",
    "                for k, v in zip(keys, values):\n",
    "                    row[k] = v\n",
    "                rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def parse_officials(summary: Dict[str, Any], game_id: str) -> List[Dict[str, Any]]:\n",
    "    officials = (\n",
    "        summary.get(\"officials\")\n",
    "        or summary.get(\"gameInfo\", {}).get(\"officials\")\n",
    "        or _first_comp(summary).get(\"officials\")\n",
    "        or []\n",
    "    )\n",
    "    out = []\n",
    "    for off in officials:\n",
    "        name = off.get(\"fullName\") or off.get(\"displayName\")\n",
    "        out.append({\"game_id\": game_id, \"official_name\": name})\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Public: single-game saver\n",
    "# ----------------------------\n",
    "def save_single_game(game_id: str, outdir: str = OUT_DIR) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Writes FOUR CSVs using only the summary endpoint to S3/local:\n",
    "      1) {gid}_game_info.csv\n",
    "      2) {gid}_team_stats.csv\n",
    "      3) {gid}_player_stats.csv\n",
    "      4) {gid}_officials.csv\n",
    "    \"\"\"\n",
    "    summary = _get_summary(game_id)\n",
    "\n",
    "    game_info_rows  = parse_game_info(summary, game_id)\n",
    "    team_stats_rows = parse_team_stats(summary, game_id)\n",
    "    player_rows     = parse_player_stats(summary, game_id)\n",
    "    officials_rows  = parse_officials(summary, game_id)\n",
    "\n",
    "    # Subfolders (adjust year tag as desired)\n",
    "    game_info_path    = (s3_join(outdir, f\"game-info-2026/{game_id}_game_info.csv\")\n",
    "                         if is_s3_path(outdir) else os.path.join(outdir, f\"game-info-2026/{game_id}_game_info.csv\"))\n",
    "    team_stats_path   = (s3_join(outdir, f\"team-stats-2026/{game_id}_team_stats.csv\")\n",
    "                         if is_s3_path(outdir) else os.path.join(outdir, f\"team-stats-2026/{game_id}_team_stats.csv\"))\n",
    "    player_stats_path = (s3_join(outdir, f\"player-stats-2026/{game_id}_player_stats.csv\")\n",
    "                         if is_s3_path(outdir) else os.path.join(outdir, f\"player-stats-2026/{game_id}_player_stats.csv\"))\n",
    "    officials_path    = (s3_join(outdir, f\"officials-2026/{game_id}_officials.csv\")\n",
    "                         if is_s3_path(outdir) else os.path.join(outdir, f\"officials-2026/{game_id}_officials.csv\"))\n",
    "\n",
    "    _write_csv(\n",
    "        game_info_path,\n",
    "        game_info_rows,\n",
    "        header=[\n",
    "            \"game_id\",\"date_utc\",\"time_utc\",\"neutral_site\",\n",
    "            \"home_team\",\"away_team\",\n",
    "            \"home_1h\",\"away_1h\",\"home_2h\",\"away_2h\",\n",
    "            \"home_score\",\"away_score\",\n",
    "        ],\n",
    "    )\n",
    "    _write_csv(team_stats_path, team_stats_rows)\n",
    "    _write_csv(player_stats_path, player_rows)\n",
    "    _write_csv(officials_path, officials_rows, header=[\"game_id\",\"official_name\"])\n",
    "\n",
    "    return {\n",
    "        \"game_info_csv\": game_info_path,\n",
    "        \"team_stats_csv\": team_stats_path,\n",
    "        \"player_stats_csv\": player_stats_path,\n",
    "        \"officials_csv\": officials_path,\n",
    "    }\n",
    "\n",
    "# ----------------------------\n",
    "# Pull IDs and run\n",
    "# ----------------------------\n",
    "def _read_ids_from_folder(ids_dir: str, ids_date: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Read all game_id CSVs under <ids_dir>/<ids_date>/*.csv from S3/local.\n",
    "    Expects a 'game_id' column.\n",
    "    \"\"\"\n",
    "    pattern = (s3_join(ids_dir, ids_date, \"*.csv\") if is_s3_path(ids_dir)\n",
    "               else os.path.join(ids_dir, ids_date, \"*.csv\"))\n",
    "\n",
    "    files = fs_ids.glob(pattern)\n",
    "    if not files:\n",
    "        print(f\"⚠️  No CSV files found at {pattern}\")\n",
    "        return []\n",
    "\n",
    "    dfs = []\n",
    "    for fpath in files:\n",
    "        try:\n",
    "            # Use the fs (not string inspection) to decide how to open\n",
    "            if fs_ids._is_s3:  # S3-backed\n",
    "                with fs_ids.open_binary_read(fpath) as fh:\n",
    "                    dfs.append(pd.read_csv(fh))\n",
    "            else:              # local\n",
    "                dfs.append(pd.read_csv(fpath))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Skipping unreadable file {fpath}: {e}\")\n",
    "\n",
    "    if not dfs:\n",
    "        return []\n",
    "\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    if \"game_id\" not in combined.columns:\n",
    "        print(\"⚠️  Combined CSVs missing 'game_id' column.\")\n",
    "        return []\n",
    "\n",
    "    return [str(x) for x in combined[\"game_id\"].dropna().astype(str).tolist()]\n",
    "\n",
    "def main():\n",
    "    game_ids = _read_ids_from_folder(IDS_DIR, IDS_DATE)\n",
    "    if not game_ids:\n",
    "        print(\"No game IDs to process.\")\n",
    "        return\n",
    "\n",
    "    count = 1\n",
    "    for gid in game_ids:\n",
    "        print(f\"{count}: saving {gid}\")\n",
    "        try:\n",
    "            save_single_game(gid, OUT_DIR)\n",
    "        except Exception as e:\n",
    "            print(f\"❌  {gid}: ERROR {e}\")\n",
    "        time.sleep(0.5)\n",
    "        count += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 463 files\n",
      "Found 463 files\n"
     ]
    }
   ],
   "source": [
    "import s3fs\n",
    "import pandas as pd\n",
    "from pandas.errors import EmptyDataError, ParserError\n",
    "\n",
    "# Initialize S3 filesystem (uses your AWS credentials)\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "# List all CSVs\n",
    "files = fs.glob(\"collegebasketballinsiders/boxscores-2026/team-stats-2026/*.csv\")\n",
    "print(f\"Found {len(files)} files\")\n",
    "\n",
    "dfs = []\n",
    "for path in files:\n",
    "    s3_url = f\"s3://{path}\"\n",
    "    try:\n",
    "        df_part = pd.read_csv(s3_url, storage_options={\"anon\": False})\n",
    "        if df_part.empty:\n",
    "            print(f\"[skip] Empty file: {s3_url}\")\n",
    "            continue\n",
    "        dfs.append(df_part)\n",
    "    except (EmptyDataError, ParserError, UnicodeDecodeError) as e:\n",
    "        print(f\"[skip] Could not read {s3_url}: {e}\")\n",
    "        continue\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No valid CSV files found.\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df.to_csv(\"s3://collegebasketballinsiders/box-scores/2026/teams/team-stats.csv\")\n",
    "\n",
    "\n",
    "# List all CSVs\n",
    "files = fs.glob(\"collegebasketballinsiders/boxscores-2026/game-info-2026/*.csv\")\n",
    "print(f\"Found {len(files)} files\")\n",
    "\n",
    "dfs = []\n",
    "for path in files:\n",
    "    s3_url = f\"s3://{path}\"\n",
    "    try:\n",
    "        df_part = pd.read_csv(s3_url, storage_options={\"anon\": False})\n",
    "        if df_part.empty:\n",
    "            print(f\"[skip] Empty file: {s3_url}\")\n",
    "            continue\n",
    "        dfs.append(df_part)\n",
    "    except (EmptyDataError, ParserError, UnicodeDecodeError) as e:\n",
    "        print(f\"[skip] Could not read {s3_url}: {e}\")\n",
    "        continue\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No valid CSV files found.\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df.to_csv(\"s3://collegebasketballinsiders/box-scores/2026/game-info/game-info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torvik_df = pd.read_csv(\"s3://collegebasketballinsiders/torvik/2026/team-ratings.csv\")\n",
    "map_df = pd.read_csv(\"s3://collegebasketballinsiders/general/map.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torvik_df = torvik_df[['season', 'date', 'team', 'rank', 'conf', 'games',\n",
    "       'adj_off_eff', 'adj_def_eff', 'barthag', 'efg_pct', 'efgd_pct', 'tor',\n",
    "       'tord', 'orb', 'drb', 'ftr', 'ftrd', 'two_pt_pct', 'two_pt_def_pct',\n",
    "       'three_pt_pct', 'three_pt_def_pct', 'three_pt_rt', 'three_pt_def_rt',\n",
    "       'adj_tempo', 'wab']]\n",
    "torvik_df[\"date\"] = pd.to_datetime(torvik_df[\"date\"], format=\"%Y%m%d\")\n",
    "\n",
    "torvik_df = torvik_df.merge(map_df[[\"team_id\", \"torrvik\"]], left_on=\"team\", right_on=\"torrvik\").drop(\"torrvik\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "kenpom_df = pd.read_csv(\"s3://collegebasketballinsiders/kenpom/2026/team-ratings.csv\")\n",
    "kenpom_df = kenpom_df[['ArchiveDate', 'Season', 'TeamID', 'TeamName',\n",
    "       'ConfShort', 'AdjEM', 'RankAdjEM', 'AdjOE', 'RankAdjOE',\n",
    "       'AdjDE', 'RankAdjDE', 'AdjTempo', 'RankAdjTempo']]\n",
    "kenpom_df.columns = ['date', 'season', 'team_id', 'team',\n",
    "       'conf', 'adj_em', 'adj_em_rank', 'adj_oe', 'adj_oe_rank',\n",
    "       'adj_def', 'adj_def_rank', 'adj_tempo', 'adj_tempo_rank']\n",
    "kenpom_df[\"date\"] = pd.to_datetime(kenpom_df[\"date\"])\n",
    "height_df = pd.read_csv(\"s3://collegebasketballinsiders/kenpom/2026/team-height.csv\")\n",
    "height_df = height_df.merge(map_df[[\"team_id\", \"kenpom\"]], left_on=\"TeamName\", right_on=\"kenpom\")\n",
    "height_df = height_df[['Season', 'team_id', 'TeamName', 'ConfShort', \n",
    "       'AvgHgt', 'AvgHgtRank', 'HgtEff', 'HgtEffRank', 'Hgt5', 'Hgt5Rank',\n",
    "       'Hgt4', 'Hgt4Rank', 'Hgt3', 'Hgt3Rank', 'Hgt2', 'Hgt2Rank', 'Hgt1',\n",
    "       'Hgt1Rank', 'Exp', 'ExpRank', 'Bench', 'BenchRank', 'Continuity',\n",
    "       'RankContinuity']]\n",
    "height_df.columns = ['season', 'team_id', 'team', 'conf',\n",
    "       'avg_height', 'avg_height_rank', 'eff_height', 'eff_height_rank', 'height_5', 'height_5_rank',\n",
    "       'height_4', 'height_4_rank', 'height_3', 'height_3_rank', 'height_2', 'height_2_rank', 'height_1',\n",
    "       'height_1_rank', 'exp', 'exp_rank', 'bench', 'bench_rank', 'continuity',\n",
    "       'continuity_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = kenpom_df.merge(torvik_df, on=['date', 'team_id'], how=\"left\")\n",
    "rating_df['season'] = rating_df['season_x']\n",
    "rating_df['conf'] = rating_df['conf_x']\n",
    "rating_df['team'] = rating_df['team_x']\n",
    "rating_df['adj_tempo_km'] = rating_df['adj_tempo_x']\n",
    "rating_df['adj_tempo_tvk'] = rating_df['adj_tempo_y']\n",
    "rating_df = rating_df[['date', 'season', 'team_id', 'team', 'conf', 'adj_em',\n",
    "       'adj_em_rank', 'adj_oe', 'adj_oe_rank', 'adj_def', 'adj_def_rank',\n",
    "       'adj_tempo_km', 'adj_tempo_rank','rank', \n",
    "       'games', 'adj_off_eff', 'adj_def_eff', 'barthag', 'efg_pct', 'efgd_pct',\n",
    "       'tor', 'tord', 'orb', 'drb', 'ftr', 'ftrd', 'two_pt_pct',\n",
    "       'two_pt_def_pct', 'three_pt_pct', 'three_pt_def_pct', 'three_pt_rt',\n",
    "       'three_pt_def_rt', 'adj_tempo_tvk', 'wab']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = rating_df.merge(height_df, on=['season', 'team_id'], how=\"left\")\n",
    "rating_df['team'] = rating_df['team_x']\n",
    "rating_df['conf'] = rating_df['conf_x']\n",
    "rating_df = rating_df[['date', 'season', 'team_id', 'team', 'conf', 'adj_em',\n",
    "       'adj_em_rank', 'adj_oe', 'adj_oe_rank', 'adj_def', 'adj_def_rank',\n",
    "       'adj_tempo_km', 'adj_tempo_rank', 'rank', 'games', 'adj_off_eff',\n",
    "       'adj_def_eff', 'barthag', 'efg_pct', 'efgd_pct', 'tor', 'tord', 'orb',\n",
    "       'drb', 'ftr', 'ftrd', 'two_pt_pct', 'two_pt_def_pct', 'three_pt_pct',\n",
    "       'three_pt_def_pct', 'three_pt_rt', 'three_pt_def_rt', 'adj_tempo_tvk',\n",
    "       'wab', 'avg_height', 'avg_height_rank',\n",
    "       'eff_height', 'eff_height_rank', 'height_5', 'height_5_rank',\n",
    "       'height_4', 'height_4_rank', 'height_3', 'height_3_rank', 'height_2',\n",
    "       'height_2_rank', 'height_1', 'height_1_rank', 'exp', 'exp_rank',\n",
    "       'bench', 'bench_rank', 'continuity', 'continuity_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_ratings_one_game(rating_df: pd.DataFrame, make_new_cols=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Shift per-game rating/stat columns back by 1 within (season, team_id),\n",
    "    while leaving height/exp/bench/continuity + ID/meta columns unshifted.\n",
    "\n",
    "    If make_new_cols=True, creates *_lag1 columns instead of overwriting.\n",
    "    \"\"\"\n",
    "    df = rating_df.copy()\n",
    "\n",
    "    # ---- ID/meta (never shift)\n",
    "    id_cols = [\"date\", \"season\", \"team_id\", \"team\", \"conf\"]\n",
    "\n",
    "    # ---- Non-shift stat families (explicit names from your schema)\n",
    "    no_shift_cols = id_cols + [\n",
    "        \"avg_height\", \"avg_height_rank\",\n",
    "        \"eff_height\", \"eff_height_rank\",\n",
    "        \"height_5\", \"height_5_rank\",\n",
    "        \"height_4\", \"height_4_rank\",\n",
    "        \"height_3\", \"height_3_rank\",\n",
    "        \"height_2\", \"height_2_rank\",\n",
    "        \"height_1\", \"height_1_rank\",\n",
    "        \"exp\", \"exp_rank\",\n",
    "        \"bench\", \"bench_rank\",\n",
    "        \"continuity\", \"continuity_rank\",\n",
    "    ]\n",
    "\n",
    "    # ---- Columns to shift = everything else that exists in the df\n",
    "    shift_cols = [c for c in df.columns if c not in set(no_shift_cols)]\n",
    "\n",
    "    # Ensure date is datetime and sort for proper game order\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values([\"season\", \"team_id\", \"date\"])\n",
    "\n",
    "    # Grouped lag\n",
    "    lagged = df.groupby([\"season\", \"team_id\"], dropna=False)[shift_cols].shift(1)\n",
    "\n",
    "    if make_new_cols:\n",
    "        df[[f\"{c}_lag1\" for c in shift_cols]] = lagged\n",
    "    else:\n",
    "        df[shift_cols] = lagged\n",
    "\n",
    "    return df\n",
    "\n",
    "kp_tvk_features_df = shift_ratings_one_game(rating_df=rating_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_tvk_features_df = kp_tvk_features_df[~kp_tvk_features_df['adj_em'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESPN GAME STATS\n",
    "game_info_df = pd.read_csv(\"s3://collegebasketballinsiders/box-scores/2026/game-info/game-info.csv\")\n",
    "team_stats_df = pd.read_csv(\"s3://collegebasketballinsiders/box-scores/2026/teams/team-stats.csv\")\n",
    "todays_games_df = game_info_df[game_info_df['date_utc'] == \"2025-11-09\"]\n",
    "\n",
    "todays_games_df = todays_games_df.merge(map_df[[\"espn_2\", \"team_id\"]], left_on=\"home_team\", right_on=\"espn_2\")\n",
    "todays_games_df['home_team_id'] = todays_games_df['team_id']\n",
    "todays_games_df = todays_games_df[['game_id', 'date_utc', 'time_utc', 'neutral_site',\n",
    "       'home_team', 'home_team_id', 'away_team', 'home_1h', 'away_1h', 'home_2h', 'away_2h',\n",
    "       'home_score', 'away_score']]\n",
    "todays_games_df = todays_games_df.merge(map_df[[\"espn_2\", \"team_id\"]], left_on=\"away_team\", right_on=\"espn_2\")\n",
    "todays_games_df['away_team_id'] = todays_games_df['team_id']\n",
    "todays_games_df = todays_games_df[['game_id', 'date_utc', 'time_utc', 'neutral_site',\n",
    "       'home_team', 'home_team_id', 'away_team', 'away_team_id', 'home_1h', 'away_1h', 'home_2h', 'away_2h',\n",
    "       'home_score', 'away_score']]\n",
    "todays_games_df['season'] = 2026\n",
    "todays_games_df = todays_games_df[['game_id', 'season', 'date_utc', 'time_utc', 'neutral_site', 'home_team',\n",
    "       'home_team_id', 'away_team', 'away_team_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_info_df = game_info_df.merge(map_df[[\"espn_2\", \"team_id\"]], left_on=\"home_team\", right_on=\"espn_2\")\n",
    "game_info_df['home_team_id'] = game_info_df['team_id']\n",
    "game_info_df = game_info_df[['game_id', 'date_utc', 'time_utc', 'neutral_site',\n",
    "       'home_team', 'home_team_id', 'away_team', 'home_1h', 'away_1h', 'home_2h', 'away_2h',\n",
    "       'home_score', 'away_score']]\n",
    "game_info_df = game_info_df.merge(map_df[[\"espn_2\", \"team_id\"]], left_on=\"away_team\", right_on=\"espn_2\")\n",
    "game_info_df['away_team_id'] = game_info_df['team_id']\n",
    "game_info_df = game_info_df[['game_id', 'date_utc', 'time_utc', 'neutral_site',\n",
    "       'home_team', 'home_team_id', 'away_team', 'away_team_id', 'home_1h', 'away_1h', 'home_2h', 'away_2h',\n",
    "       'home_score', 'away_score']]\n",
    "game_info_df = game_info_df[~game_info_df['home_2h'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_stats_df = team_stats_df.merge(map_df[[\"espn_2\", \"team_id\"]], left_on=\"team\", right_on=\"espn_2\")\n",
    "team_stats_df['team_id'] = team_stats_df['team_id_y']\n",
    "team_stats_df = team_stats_df[['game_id', 'team', 'team_id', 'assists', 'defensiveRebounds', 'fouls',\n",
    "       'totalRebounds', \n",
    "       'pointsInPaint', 'technicalFouls',\n",
    "       'offensiveRebounds',  'turnoverPoints', 'steals', 'blocks', 'fastBreakPoints',\n",
    "       'turnovers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_grab(df):\n",
    "    \"\"\"\n",
    "    Assigns a season year based on date_utc.\n",
    "    Example: games before 2021-04-01 belong to season 2021,\n",
    "             games between 2021-04-01 and 2022-04-01 belong to 2022, etc.\n",
    "    \"\"\"\n",
    "    # Ensure date_utc is datetime\n",
    "    df = df.copy()\n",
    "    df['date_utc'] = pd.to_datetime(df['date_utc'])\n",
    "\n",
    "    # Define season cutoffs\n",
    "    bins = [\n",
    "        pd.Timestamp(\"1900-01-01\"),\n",
    "        pd.Timestamp(\"2021-04-21\"),\n",
    "        pd.Timestamp(\"2022-04-21\"),\n",
    "        pd.Timestamp(\"2023-04-21\"),\n",
    "        pd.Timestamp(\"2024-04-21\"),\n",
    "        pd.Timestamp(\"2025-04-21\"),\n",
    "        pd.Timestamp(\"2100-01-21\"),\n",
    "    ]\n",
    "    seasons = [2021, 2022, 2023, 2024, 2025, 2026]\n",
    "\n",
    "    # Use pandas cut to categorize efficiently\n",
    "    df['season'] = pd.cut(df['date_utc'], bins=bins, labels=seasons, right=False).astype(int)\n",
    "\n",
    "    return df\n",
    "game_info_df = season_grab(game_info_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ======================\n",
    "# Your existing config\n",
    "# ======================\n",
    "BASE_STATS = [\n",
    "    'assists', 'defensiveRebounds', 'fouls', 'totalRebounds',\n",
    "    'pointsInPaint', 'technicalFouls', 'offensiveRebounds',\n",
    "    'turnoverPoints', 'steals', 'blocks', 'fastBreakPoints', 'turnovers'\n",
    "]\n",
    "ROLL_WINDOWS = [1, 3, 5, 10]\n",
    "\n",
    "# --- Robust datetime combiner (handles \"HHZ\", \"HH:MMZ\", \"HH:MM:SSZ\" or without Z):\n",
    "def _combine_utc_datetime(date_series: pd.Series, time_series: pd.Series) -> pd.Series:\n",
    "    d = date_series.astype(str).str.strip()\n",
    "    t = time_series.astype(str).str.strip().str.upper()\n",
    "    # default blank -> noon UTC\n",
    "    t = t.mask((t.eq(\"\")) | t.isna(), \"12:00:00Z\")\n",
    "    # strip Z, normalize to HH:MM:SS, then add Z back\n",
    "    s = pd.Series(t.str.replace(\"Z\", \"\", regex=False))\n",
    "    s = s.where(~s.str.match(r\"^\\d{1,2}$\"), s + \":00:00\")   # HH -> HH:00:00\n",
    "    s = s.where(~s.str.match(r\"^\\d{1,2}:\\d{2}$\"), s + \":00\")# HH:MM -> HH:MM:00\n",
    "    t_full = s.astype(str) + \"Z\"\n",
    "    return pd.to_datetime(d + \" \" + t_full, errors=\"coerce\", utc=True)\n",
    "\n",
    "# ======================\n",
    "# Your existing builders\n",
    "# ======================\n",
    "\n",
    "def build_team_stats_features_no_dup(\n",
    "    game_info_df: pd.DataFrame,\n",
    "    team_stats_df: pd.DataFrame,\n",
    "    windows: list[int] = ROLL_WINDOWS,\n",
    "    local_time_zone: str | None = None,\n",
    "    agg: str | dict = \"sum\",\n",
    ") -> pd.DataFrame:\n",
    "    req_game = ['game_id','date_utc','time_utc','season',\n",
    "                'home_team','home_team_id','away_team','away_team_id']\n",
    "    missing = [c for c in req_game if c not in game_info_df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing in game_info_df: {missing}\")\n",
    "\n",
    "    if not {'game_id','team_id'}.issubset(team_stats_df.columns):\n",
    "        raise KeyError(\"team_stats_df must include 'game_id' and 'team_id'.\")\n",
    "\n",
    "    stats = [s for s in BASE_STATS if s in team_stats_df.columns]\n",
    "    if not stats:\n",
    "        out = game_info_df.copy()\n",
    "        out[\"game_datetime_utc\"] = _combine_utc_datetime(out[\"date_utc\"], out[\"time_utc\"])\n",
    "        out[\"game_datetime_local\"] = (\n",
    "            out[\"game_datetime_utc\"].dt.tz_convert(local_time_zone)\n",
    "            if local_time_zone else out[\"game_datetime_utc\"]\n",
    "        )\n",
    "        return out\n",
    "\n",
    "    agg_map = {s: agg if isinstance(agg, str) else agg.get(s, \"sum\") for s in stats}\n",
    "    ts_clean = (\n",
    "        team_stats_df\n",
    "        .groupby([\"game_id\",\"team_id\"], as_index=False)\n",
    "        .agg(agg_map)\n",
    "    )\n",
    "    assert not ts_clean.duplicated([\"game_id\",\"team_id\"]).any(), \"Aggregation failed to ensure uniqueness.\"\n",
    "\n",
    "    gif = game_info_df.copy()\n",
    "    gif[\"game_datetime_utc\"] = _combine_utc_datetime(gif[\"date_utc\"], gif[\"time_utc\"])\n",
    "    gif[\"game_datetime_local\"] = (\n",
    "        gif[\"game_datetime_utc\"].dt.tz_convert(local_time_zone)\n",
    "        if local_time_zone else gif[\"game_datetime_utc\"]\n",
    "    )\n",
    "\n",
    "    home_stats = ts_clean.rename(columns={\"team_id\":\"home_team_id\", **{s: f\"{s}_home\" for s in stats}})\n",
    "    away_stats = ts_clean.rename(columns={\"team_id\":\"away_team_id\", **{s: f\"{s}_away\" for s in stats}})\n",
    "    merged = (\n",
    "        gif.merge(home_stats, on=[\"game_id\",\"home_team_id\"], how=\"left\")\n",
    "           .merge(away_stats, on=[\"game_id\",\"away_team_id\"], how=\"left\")\n",
    "    )\n",
    "\n",
    "    home_payload = {\n",
    "        'game_id': merged['game_id'],\n",
    "        'season': merged['season'],\n",
    "        'game_dt': merged['game_datetime_utc'],\n",
    "        'team_id': merged['home_team_id'],\n",
    "        'side': 'home'\n",
    "    }\n",
    "    for s in stats:\n",
    "        home_payload[s] = merged.get(f\"{s}_home\")\n",
    "        home_payload[f\"allowed_{s}\"] = merged.get(f\"{s}_away\")\n",
    "\n",
    "    away_payload = {\n",
    "        'game_id': merged['game_id'],\n",
    "        'season': merged['season'],\n",
    "        'game_dt': merged['game_datetime_utc'],\n",
    "        'team_id': merged['away_team_id'],\n",
    "        'side': 'away'\n",
    "    }\n",
    "    for s in stats:\n",
    "        away_payload[s] = merged.get(f\"{s}_away\")\n",
    "        away_payload[f\"allowed_{s}\"] = merged.get(f\"{s}_home\")\n",
    "\n",
    "    long_team = pd.concat([pd.DataFrame(home_payload), pd.DataFrame(away_payload)], ignore_index=True)\n",
    "    long_team = long_team.sort_values(['team_id','season','game_dt'], kind='mergesort')\n",
    "    g = long_team.groupby(['team_id','season'], group_keys=False)\n",
    "\n",
    "    for s in stats:\n",
    "        for col in (s, f\"allowed_{s}\"):\n",
    "            shifted = g[col].shift(1)  # use only past games\n",
    "            for w in windows:\n",
    "                long_team[f\"ra{w}_{col}\"] = (\n",
    "                    long_team.assign(_s=shifted)\n",
    "                             .groupby(['team_id','season'], group_keys=False)['_s']\n",
    "                             .rolling(window=w, min_periods=1)\n",
    "                             .mean()\n",
    "                             .reset_index(level=[0,1], drop=True)\n",
    "                )\n",
    "\n",
    "    roll_cols = [c for c in long_team.columns if c.startswith(\"ra\")]\n",
    "    home_feats = (\n",
    "        long_team[long_team['side']=='home'][['game_id','team_id'] + roll_cols]\n",
    "        .rename(columns={'team_id':'home_team_id', **{c: f\"home_{c}\" for c in roll_cols}})\n",
    "    )\n",
    "    away_feats = (\n",
    "        long_team[long_team['side']=='away'][['game_id','team_id'] + roll_cols]\n",
    "        .rename(columns={'team_id':'away_team_id', **{c: f\"away_{c}\" for c in roll_cols}})\n",
    "    )\n",
    "\n",
    "    out = (\n",
    "        merged.merge(home_feats, on=['game_id','home_team_id'], how='left')\n",
    "              .merge(away_feats, on=['game_id','away_team_id'], how='left')\n",
    "    )\n",
    "\n",
    "    id_cols = ['game_id','date_utc','time_utc','season','home_team','home_team_id','away_team','away_team_id']\n",
    "    roll_outs = [c for c in out.columns if c.startswith('home_ra') or c.startswith('away_ra')]\n",
    "    ordered = [c for c in id_cols if c in out.columns] + roll_outs\n",
    "    ordered += [c for c in out.columns if c not in ordered]\n",
    "    return out[ordered]\n",
    "\n",
    "# --- Game-level rolling (points/totals/margins/time features) you shared:\n",
    "REQUIRED_COLS = [\n",
    "    \"game_id\", \"date_utc\", \"time_utc\", \"neutral_site\",\n",
    "    \"home_team\", \"home_team_id\", \"away_team\", \"away_team_id\",\n",
    "    \"home_1h\", \"away_1h\", \"home_2h\", \"away_2h\",\n",
    "    \"home_score\", \"away_score\", \"season\",\n",
    "    \"total\", \"1h_total\", \"2h_total\",\n",
    "    \"margin\", \"1h_margin\", \"2h_margin\",\n",
    "]\n",
    "\n",
    "def _to_flag(x) -> int:\n",
    "    if pd.isna(x):\n",
    "        return 0\n",
    "    if isinstance(x, (int, float)) and not pd.isna(x):\n",
    "        return int(x != 0)\n",
    "    s = str(x).strip().lower()\n",
    "    return int(s in {\"1\",\"true\",\"t\",\"y\",\"yes\",\"neutral\",\"neutral_site\"})\n",
    "\n",
    "def build_cbb_features_multiroll(\n",
    "    games_df: pd.DataFrame,\n",
    "    windows: list[int] = [1, 3, 5, 10],\n",
    "    local_time_zone: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    missing = [c for c in REQUIRED_COLS if c not in games_df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    df = games_df.copy()\n",
    "    df[\"game_datetime_utc\"] = _combine_utc_datetime(df[\"date_utc\"], df[\"time_utc\"])\n",
    "    df[\"game_datetime_local\"] = (\n",
    "        df[\"game_datetime_utc\"].dt.tz_convert(local_time_zone)\n",
    "        if local_time_zone else df[\"game_datetime_utc\"]\n",
    "    )\n",
    "\n",
    "    gdl = \"game_datetime_local\"\n",
    "    df[\"game_dow\"]        = df[gdl].dt.weekday\n",
    "    df[\"game_month\"]      = df[gdl].dt.month\n",
    "    df[\"game_dayofyear\"]  = df[gdl].dt.dayofyear\n",
    "    df[\"game_weekofyear\"] = df[gdl].dt.isocalendar().week.astype(int)\n",
    "    df[\"game_hour\"]       = df[gdl].dt.hour\n",
    "    df[\"is_weekend\"]      = df[\"game_dow\"].isin([5, 6]).astype(int)\n",
    "    df[\"neutral_site_flag\"] = df[\"neutral_site\"].apply(_to_flag).astype(int)\n",
    "\n",
    "    if \"1h_total\" not in df or df[\"1h_total\"].isna().all():\n",
    "        df[\"1h_total\"] = df[\"home_1h\"] + df[\"away_1h\"]\n",
    "    if \"2h_total\" not in df or df[\"2h_total\"].isna().all():\n",
    "        df[\"2h_total\"] = df[\"home_2h\"] + df[\"away_2h\"]\n",
    "    if \"total\" not in df or df[\"total\"].isna().all():\n",
    "        df[\"total\"] = df[\"home_score\"] + df[\"away_score\"]\n",
    "    if \"1h_margin\" not in df or df[\"1h_margin\"].isna().all():\n",
    "        df[\"1h_margin\"] = df[\"home_1h\"] - df[\"away_1h\"]\n",
    "    if \"2h_margin\" not in df or df[\"2h_margin\"].isna().all():\n",
    "        df[\"2h_margin\"] = df[\"home_2h\"] - df[\"away_2h\"]\n",
    "    if \"margin\" not in df or df[\"margin\"].isna().all():\n",
    "        df[\"margin\"] = df[\"home_score\"] - df[\"away_score\"]\n",
    "\n",
    "    home_side = pd.DataFrame({\n",
    "        \"game_id\":     df[\"game_id\"],\n",
    "        \"season\":      df[\"season\"],\n",
    "        \"game_dt\":     df[\"game_datetime_utc\"],\n",
    "        \"team\":        df[\"home_team\"],\n",
    "        \"team_id\":     df[\"home_team_id\"],\n",
    "        \"opponent\":    df[\"away_team\"],\n",
    "        \"opponent_id\": df[\"away_team_id\"],\n",
    "        \"side\":        \"home\",\n",
    "        \"pts_1h\":      df[\"home_1h\"],\n",
    "        \"pts_2h\":      df[\"home_2h\"],\n",
    "        \"pts_g\":       df[\"home_score\"],\n",
    "        \"opp_1h\":      df[\"away_1h\"],\n",
    "        \"opp_2h\":      df[\"away_2h\"],\n",
    "        \"opp_g\":       df[\"away_score\"],\n",
    "        \"is_neutral\":  df[\"neutral_site_flag\"].astype(int),\n",
    "    })\n",
    "    away_side = pd.DataFrame({\n",
    "        \"game_id\":     df[\"game_id\"],\n",
    "        \"season\":      df[\"season\"],\n",
    "        \"game_dt\":     df[\"game_datetime_utc\"],\n",
    "        \"team\":        df[\"away_team\"],\n",
    "        \"team_id\":     df[\"away_team_id\"],\n",
    "        \"opponent\":    df[\"home_team\"],\n",
    "        \"opponent_id\": df[\"home_team_id\"],\n",
    "        \"side\":        \"away\",\n",
    "        \"pts_1h\":      df[\"away_1h\"],\n",
    "        \"pts_2h\":      df[\"away_2h\"],\n",
    "        \"pts_g\":       df[\"away_score\"],\n",
    "        \"opp_1h\":      df[\"home_1h\"],\n",
    "        \"opp_2h\":      df[\"home_2h\"],\n",
    "        \"opp_g\":       df[\"home_score\"],\n",
    "        \"is_neutral\":  df[\"neutral_site_flag\"].astype(int),\n",
    "    })\n",
    "    long_team = pd.concat([home_side, away_side], ignore_index=True)\n",
    "\n",
    "    long_team[\"mgn_1h\"] = long_team[\"pts_1h\"] - long_team[\"opp_1h\"]\n",
    "    long_team[\"mgn_2h\"] = long_team[\"pts_2h\"] - long_team[\"opp_2h\"]\n",
    "    long_team[\"mgn_g\"]  = long_team[\"pts_g\"]  - long_team[\"opp_g\"]\n",
    "    long_team[\"tot_1h\"] = long_team[\"pts_1h\"] + long_team[\"opp_1h\"]\n",
    "    long_team[\"tot_g\"]  = long_team[\"pts_g\"]  + long_team[\"opp_g\"]\n",
    "\n",
    "    long_team = long_team.sort_values([\"team_id\", \"season\", \"game_dt\"], kind=\"mergesort\")\n",
    "\n",
    "    # days since last (this will compute rest for today's rows too)\n",
    "    long_team[\"days_since_last_game\"] = (\n",
    "        long_team.groupby([\"team_id\", \"season\"], group_keys=False)[\"game_dt\"]\n",
    "                 .diff()\n",
    "                 .dt.total_seconds()\n",
    "                 .div(86400.0)\n",
    "    )\n",
    "\n",
    "    base_feats = {\n",
    "        \"pts_1h\": \"ra_pts_1h\",\n",
    "        \"pts_2h\": \"ra_pts_2h\",\n",
    "        \"pts_g\":  \"ra_pts_g\",\n",
    "        \"opp_1h\": \"ra_allowed_1h\",\n",
    "        \"opp_2h\": \"ra_allowed_2h\",\n",
    "        \"opp_g\":  \"ra_allowed_g\",\n",
    "        \"mgn_1h\": \"ra_mgn_1h\",\n",
    "        \"mgn_2h\": \"ra_mgn_2h\",\n",
    "        \"mgn_g\":  \"ra_mgn_g\",\n",
    "        \"tot_1h\": \"ra_tot_1h\",\n",
    "        \"tot_g\":  \"ra_tot_g\",\n",
    "    }\n",
    "    g = long_team.groupby([\"team_id\", \"season\"], group_keys=False)\n",
    "    for src_col, base_name in base_feats.items():\n",
    "        shifted = g[src_col].shift(1)  # <-- leakage-safe\n",
    "        long_team[f\"__shifted__{src_col}\"] = shifted\n",
    "        for w in windows:\n",
    "            long_team[f\"{base_name}_{w}\"] = (\n",
    "                long_team.groupby([\"team_id\", \"season\"], group_keys=False)[f\"__shifted__{src_col}\"]\n",
    "                        .rolling(window=w, min_periods=1)\n",
    "                        .mean()\n",
    "                        .reset_index(level=[0,1], drop=True)\n",
    "            )\n",
    "    long_team.drop(columns=[c for c in long_team.columns if c.startswith(\"__shifted__\")], inplace=True)\n",
    "\n",
    "    roll_cols = [c for c in long_team.columns if c.startswith(\"ra_\")]\n",
    "    keep_cols = [\"game_id\", \"team\", \"team_id\", \"side\", \"days_since_last_game\", \"is_neutral\"] + roll_cols\n",
    "\n",
    "    home_feats = (\n",
    "        long_team[long_team[\"side\"] == \"home\"][keep_cols]\n",
    "        .drop(columns=[\"side\"])\n",
    "        .rename(columns={\n",
    "            \"team\": \"home_team\",\n",
    "            \"team_id\": \"home_team_id\",\n",
    "            \"days_since_last_game\": \"home_days_since_last\",\n",
    "            \"is_neutral\": \"home_is_neutral\",\n",
    "            **{c: f\"home_{c}\" for c in roll_cols}\n",
    "        })\n",
    "    )\n",
    "    away_feats = (\n",
    "        long_team[long_team[\"side\"] == \"away\"][keep_cols]\n",
    "        .drop(columns=[\"side\"])\n",
    "        .rename(columns={\n",
    "            \"team\": \"away_team\",\n",
    "            \"team_id\": \"away_team_id\",\n",
    "            \"days_since_last_game\": \"away_days_since_last\",\n",
    "            \"is_neutral\": \"away_is_neutral\",\n",
    "            **{c: f\"away_{c}\" for c in roll_cols}\n",
    "        })\n",
    "    )\n",
    "\n",
    "    out = (\n",
    "        df.merge(home_feats, on=[\"game_id\", \"home_team\", \"home_team_id\"], how=\"left\")\n",
    "          .merge(away_feats, on=[\"game_id\", \"away_team\", \"away_team_id\"], how=\"left\")\n",
    "    )\n",
    "\n",
    "    id_cols   = [\n",
    "        \"game_id\",\"date_utc\",\"time_utc\",\"game_datetime_utc\",\"game_datetime_local\",\n",
    "        \"season\",\"neutral_site\",\"neutral_site_flag\",\n",
    "        \"home_team\",\"home_team_id\",\"away_team\",\"away_team_id\"\n",
    "    ]\n",
    "    time_cols = [\"game_dow\",\"game_month\",\"game_dayofyear\",\"game_weekofyear\",\"game_hour\",\"is_weekend\"]\n",
    "    spacing   = [\"home_days_since_last\",\"away_days_since_last\",\"home_is_neutral\",\"away_is_neutral\"]\n",
    "    roll_outs = [c for c in out.columns if c.startswith(\"home_ra_\") or c.startswith(\"away_ra_\")]\n",
    "    ordered = [c for c in id_cols + time_cols + spacing + roll_outs if c in out.columns]\n",
    "    ordered += [c for c in out.columns if c not in ordered]\n",
    "    return out[ordered]\n",
    "\n",
    "# ============================================\n",
    "# Minimal-tweak inference via simple append\n",
    "# ============================================\n",
    "def build_inference_features_via_append(\n",
    "    todays_games_df: pd.DataFrame,\n",
    "    hist_games_df: pd.DataFrame,\n",
    "    team_stats_df: pd.DataFrame,\n",
    "    local_time_zone: str | None = \"America/New_York\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Append today's games (with targets/boxscore stats set to NaN) to history,\n",
    "    run the SAME builders (which already use shift(1)), then return rows for today.\n",
    "    No asof merges needed.\n",
    "    \"\"\"\n",
    "    # 1) Ensure today's has the columns training code expects\n",
    "    today = todays_games_df.copy()\n",
    "    today = today[['game_id','season','date_utc','time_utc','neutral_site',\n",
    "                   'home_team','home_team_id','away_team','away_team_id']]\n",
    "\n",
    "    # add target columns (NaN) so build_cbb_features_multiroll can run without error\n",
    "    add_cols = [\"home_1h\",\"away_1h\",\"home_2h\",\"away_2h\",\"home_score\",\"away_score\",\n",
    "                \"total\",\"1h_total\",\"2h_total\",\"margin\",\"1h_margin\",\"2h_margin\"]\n",
    "    for c in add_cols:\n",
    "        if c not in today.columns:\n",
    "            today[c] = np.nan\n",
    "\n",
    "    # 2) Align history to required cols (keep originals in hist)\n",
    "    hist = hist_games_df.copy()\n",
    "    # make sure all required columns exist in hist (if any of total/margins missing, builder recomputes anyway)\n",
    "    for c in [\"total\",\"1h_total\",\"2h_total\",\"margin\",\"1h_margin\",\"2h_margin\"]:\n",
    "        if c not in hist.columns:\n",
    "            hist[c] = np.nan\n",
    "\n",
    "    # 3) Concatenate history + today (today last so time order is natural)\n",
    "    combined = pd.concat([hist, today], ignore_index=True, sort=False)\n",
    "\n",
    "    # 4) Build game-level rolling/time features on the combined frame\n",
    "    game_feats_all = build_cbb_features_multiroll(combined, windows=ROLL_WINDOWS, local_time_zone=local_time_zone)\n",
    "\n",
    "    # 5) Build team boxscore rolling features on the combined schedule\n",
    "    # (team_stats_df only contains historical rows; today's games simply won't match -> NaN FOR/AGAINST, which is fine)\n",
    "    ts_feats_all = build_team_stats_features_no_dup(\n",
    "        game_info_df=combined[['game_id','date_utc','time_utc','season','home_team','home_team_id','away_team','away_team_id']],\n",
    "        team_stats_df=team_stats_df,\n",
    "        windows=ROLL_WINDOWS,\n",
    "        local_time_zone=local_time_zone\n",
    "    )\n",
    "\n",
    "    # 6) Extract today's rows (by game_id) and merge both feature sets\n",
    "    today_ids = set(today['game_id'].tolist())\n",
    "    gf_today = game_feats_all[game_feats_all['game_id'].isin(today_ids)].copy()\n",
    "    ts_today = ts_feats_all[ts_feats_all['game_id'].isin(today_ids)].copy()\n",
    "\n",
    "    # Keep only the rolling cols from team-stats frame (avoid duplicate id/time cols)\n",
    "    ts_roll_cols = [c for c in ts_today.columns if c.startswith('home_ra') or c.startswith('away_ra')]\n",
    "    out = gf_today.merge(\n",
    "        ts_today[['game_id','home_team_id','away_team_id'] + ts_roll_cols],\n",
    "        on=['game_id','home_team_id','away_team_id'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # 7) Return one row per today's game with all rolling/time features\n",
    "    out = out.sort_values('game_datetime_utc').reset_index(drop=True)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_features_df = build_inference_features_via_append(todays_games_df, game_info_df, team_stats_df).drop_duplicates(subset=[\"home_team\", \"away_team\"])\n",
    "game_features_df = game_features_df[game_features_df['game_id'].isin(list(todays_games_df['game_id']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_features = game_features_df[[\n",
    "       ## GAME INFO\n",
    "       'game_id', 'date_utc', 'season', 'neutral_site', 'home_team',\n",
    "       'home_team_id', 'away_team', 'away_team_id', \n",
    "       ## TARGETS\n",
    "       'home_1h', 'away_1h','home_2h', 'away_2h', 'home_score', 'away_score', 'total',\n",
    "       '1h_total', '2h_total', 'margin', '1h_margin', '2h_margin',\n",
    "       ## GAME INFO FEATURES\n",
    "       'neutral_site_flag', 'game_dow', 'game_month', 'game_dayofyear',\n",
    "       'game_weekofyear', 'game_hour', 'is_weekend',\n",
    "       ## TEAM GAME INFO FEATURES\n",
    "       'home_days_since_last', 'away_days_since_last', 'home_ra_pts_1h_1',\n",
    "       'home_ra_pts_1h_3', 'home_ra_pts_1h_5', 'home_ra_pts_1h_10',\n",
    "       'home_ra_pts_2h_1', 'home_ra_pts_2h_3', 'home_ra_pts_2h_5',\n",
    "       'home_ra_pts_2h_10', 'home_ra_pts_g_1', 'home_ra_pts_g_3',\n",
    "       'home_ra_pts_g_5', 'home_ra_pts_g_10', 'home_ra_allowed_1h_1',\n",
    "       'home_ra_allowed_1h_3', 'home_ra_allowed_1h_5',\n",
    "       'home_ra_allowed_1h_10', 'home_ra_allowed_2h_1',\n",
    "       'home_ra_allowed_2h_3', 'home_ra_allowed_2h_5',\n",
    "       'home_ra_allowed_2h_10', 'home_ra_allowed_g_1',\n",
    "       'home_ra_allowed_g_3', 'home_ra_allowed_g_5',\n",
    "       'home_ra_allowed_g_10', 'home_ra_mgn_1h_1', 'home_ra_mgn_1h_3',\n",
    "       'home_ra_mgn_1h_5', 'home_ra_mgn_1h_10', 'home_ra_mgn_2h_1',\n",
    "       'home_ra_mgn_2h_3', 'home_ra_mgn_2h_5', 'home_ra_mgn_2h_10',\n",
    "       'home_ra_mgn_g_1', 'home_ra_mgn_g_3', 'home_ra_mgn_g_5',\n",
    "       'home_ra_mgn_g_10', 'home_ra_tot_1h_1', 'home_ra_tot_1h_3',\n",
    "       'home_ra_tot_1h_5', 'home_ra_tot_1h_10', 'home_ra_tot_g_1',\n",
    "       'home_ra_tot_g_3', 'home_ra_tot_g_5', 'home_ra_tot_g_10',\n",
    "       'away_ra_pts_1h_1', 'away_ra_pts_1h_3', 'away_ra_pts_1h_5',\n",
    "       'away_ra_pts_1h_10', 'away_ra_pts_2h_1', 'away_ra_pts_2h_3',\n",
    "       'away_ra_pts_2h_5', 'away_ra_pts_2h_10', 'away_ra_pts_g_1',\n",
    "       'away_ra_pts_g_3', 'away_ra_pts_g_5', 'away_ra_pts_g_10',\n",
    "       'away_ra_allowed_1h_1', 'away_ra_allowed_1h_3',\n",
    "       'away_ra_allowed_1h_5', 'away_ra_allowed_1h_10',\n",
    "       'away_ra_allowed_2h_1', 'away_ra_allowed_2h_3',\n",
    "       'away_ra_allowed_2h_5', 'away_ra_allowed_2h_10',\n",
    "       'away_ra_allowed_g_1', 'away_ra_allowed_g_3',\n",
    "       'away_ra_allowed_g_5', 'away_ra_allowed_g_10', 'away_ra_mgn_1h_1',\n",
    "       'away_ra_mgn_1h_3', 'away_ra_mgn_1h_5', 'away_ra_mgn_1h_10',\n",
    "       'away_ra_mgn_2h_1', 'away_ra_mgn_2h_3', 'away_ra_mgn_2h_5',\n",
    "       'away_ra_mgn_2h_10', 'away_ra_mgn_g_1', 'away_ra_mgn_g_3',\n",
    "       'away_ra_mgn_g_5', 'away_ra_mgn_g_10', 'away_ra_tot_1h_1',\n",
    "       'away_ra_tot_1h_3', 'away_ra_tot_1h_5', 'away_ra_tot_1h_10',\n",
    "       'away_ra_tot_g_1', 'away_ra_tot_g_3', 'away_ra_tot_g_5',\n",
    "       'away_ra_tot_g_10', \n",
    "       ## TEAM GAME STATS FEATURES\n",
    "       'home_ra1_assists', 'home_ra3_assists',\n",
    "       'home_ra5_assists', 'home_ra10_assists',\n",
    "       'home_ra1_allowed_assists', 'home_ra3_allowed_assists',\n",
    "       'home_ra5_allowed_assists', 'home_ra10_allowed_assists',\n",
    "       'home_ra1_defensiveRebounds', 'home_ra3_defensiveRebounds',\n",
    "       'home_ra5_defensiveRebounds', 'home_ra10_defensiveRebounds',\n",
    "       'home_ra1_allowed_defensiveRebounds',\n",
    "       'home_ra3_allowed_defensiveRebounds',\n",
    "       'home_ra5_allowed_defensiveRebounds',\n",
    "       'home_ra10_allowed_defensiveRebounds', 'home_ra1_fouls',\n",
    "       'home_ra3_fouls', 'home_ra5_fouls', 'home_ra10_fouls',\n",
    "       'home_ra1_allowed_fouls', 'home_ra3_allowed_fouls',\n",
    "       'home_ra5_allowed_fouls', 'home_ra10_allowed_fouls',\n",
    "       'home_ra1_totalRebounds', 'home_ra3_totalRebounds',\n",
    "       'home_ra5_totalRebounds', 'home_ra10_totalRebounds',\n",
    "       'home_ra1_allowed_totalRebounds', 'home_ra3_allowed_totalRebounds',\n",
    "       'home_ra5_allowed_totalRebounds',\n",
    "       'home_ra10_allowed_totalRebounds', 'home_ra1_pointsInPaint',\n",
    "       'home_ra3_pointsInPaint', 'home_ra5_pointsInPaint',\n",
    "       'home_ra10_pointsInPaint', 'home_ra1_allowed_pointsInPaint',\n",
    "       'home_ra3_allowed_pointsInPaint', 'home_ra5_allowed_pointsInPaint',\n",
    "       'home_ra10_allowed_pointsInPaint', 'home_ra1_technicalFouls',\n",
    "       'home_ra3_technicalFouls', 'home_ra5_technicalFouls',\n",
    "       'home_ra10_technicalFouls', 'home_ra1_allowed_technicalFouls',\n",
    "       'home_ra3_allowed_technicalFouls',\n",
    "       'home_ra5_allowed_technicalFouls',\n",
    "       'home_ra10_allowed_technicalFouls', 'home_ra1_offensiveRebounds',\n",
    "       'home_ra3_offensiveRebounds', 'home_ra5_offensiveRebounds',\n",
    "       'home_ra10_offensiveRebounds',\n",
    "       'home_ra1_allowed_offensiveRebounds',\n",
    "       'home_ra3_allowed_offensiveRebounds',\n",
    "       'home_ra5_allowed_offensiveRebounds',\n",
    "       'home_ra10_allowed_offensiveRebounds', 'home_ra1_turnoverPoints',\n",
    "       'home_ra3_turnoverPoints', 'home_ra5_turnoverPoints',\n",
    "       'home_ra10_turnoverPoints', 'home_ra1_allowed_turnoverPoints',\n",
    "       'home_ra3_allowed_turnoverPoints',\n",
    "       'home_ra5_allowed_turnoverPoints',\n",
    "       'home_ra10_allowed_turnoverPoints', 'home_ra1_steals',\n",
    "       'home_ra3_steals', 'home_ra5_steals', 'home_ra10_steals',\n",
    "       'home_ra1_allowed_steals', 'home_ra3_allowed_steals',\n",
    "       'home_ra5_allowed_steals', 'home_ra10_allowed_steals',\n",
    "       'home_ra1_blocks', 'home_ra3_blocks', 'home_ra5_blocks',\n",
    "       'home_ra10_blocks', 'home_ra1_allowed_blocks',\n",
    "       'home_ra3_allowed_blocks', 'home_ra5_allowed_blocks',\n",
    "       'home_ra10_allowed_blocks', 'home_ra1_fastBreakPoints',\n",
    "       'home_ra3_fastBreakPoints', 'home_ra5_fastBreakPoints',\n",
    "       'home_ra10_fastBreakPoints', 'home_ra1_allowed_fastBreakPoints',\n",
    "       'home_ra3_allowed_fastBreakPoints',\n",
    "       'home_ra5_allowed_fastBreakPoints',\n",
    "       'home_ra10_allowed_fastBreakPoints', 'home_ra1_turnovers',\n",
    "       'home_ra3_turnovers', 'home_ra5_turnovers', 'home_ra10_turnovers',\n",
    "       'home_ra1_allowed_turnovers', 'home_ra3_allowed_turnovers',\n",
    "       'home_ra5_allowed_turnovers', 'home_ra10_allowed_turnovers',\n",
    "       'away_ra1_assists', 'away_ra3_assists', 'away_ra5_assists',\n",
    "       'away_ra10_assists', 'away_ra1_allowed_assists',\n",
    "       'away_ra3_allowed_assists', 'away_ra5_allowed_assists',\n",
    "       'away_ra10_allowed_assists', 'away_ra1_defensiveRebounds',\n",
    "       'away_ra3_defensiveRebounds', 'away_ra5_defensiveRebounds',\n",
    "       'away_ra10_defensiveRebounds',\n",
    "       'away_ra1_allowed_defensiveRebounds',\n",
    "       'away_ra3_allowed_defensiveRebounds',\n",
    "       'away_ra5_allowed_defensiveRebounds',\n",
    "       'away_ra10_allowed_defensiveRebounds', 'away_ra1_fouls',\n",
    "       'away_ra3_fouls', 'away_ra5_fouls', 'away_ra10_fouls',\n",
    "       'away_ra1_allowed_fouls', 'away_ra3_allowed_fouls',\n",
    "       'away_ra5_allowed_fouls', 'away_ra10_allowed_fouls',\n",
    "       'away_ra1_totalRebounds', 'away_ra3_totalRebounds',\n",
    "       'away_ra5_totalRebounds', 'away_ra10_totalRebounds',\n",
    "       'away_ra1_allowed_totalRebounds', 'away_ra3_allowed_totalRebounds',\n",
    "       'away_ra5_allowed_totalRebounds',\n",
    "       'away_ra10_allowed_totalRebounds', 'away_ra1_pointsInPaint',\n",
    "       'away_ra3_pointsInPaint', 'away_ra5_pointsInPaint',\n",
    "       'away_ra10_pointsInPaint', 'away_ra1_allowed_pointsInPaint',\n",
    "       'away_ra3_allowed_pointsInPaint', 'away_ra5_allowed_pointsInPaint',\n",
    "       'away_ra10_allowed_pointsInPaint', 'away_ra1_technicalFouls',\n",
    "       'away_ra3_technicalFouls', 'away_ra5_technicalFouls',\n",
    "       'away_ra10_technicalFouls', 'away_ra1_allowed_technicalFouls',\n",
    "       'away_ra3_allowed_technicalFouls',\n",
    "       'away_ra5_allowed_technicalFouls',\n",
    "       'away_ra10_allowed_technicalFouls', 'away_ra1_offensiveRebounds',\n",
    "       'away_ra3_offensiveRebounds', 'away_ra5_offensiveRebounds',\n",
    "       'away_ra10_offensiveRebounds',\n",
    "       'away_ra1_allowed_offensiveRebounds',\n",
    "       'away_ra3_allowed_offensiveRebounds',\n",
    "       'away_ra5_allowed_offensiveRebounds',\n",
    "       'away_ra10_allowed_offensiveRebounds', 'away_ra1_turnoverPoints',\n",
    "       'away_ra3_turnoverPoints', 'away_ra5_turnoverPoints',\n",
    "       'away_ra10_turnoverPoints', 'away_ra1_allowed_turnoverPoints',\n",
    "       'away_ra3_allowed_turnoverPoints',\n",
    "       'away_ra5_allowed_turnoverPoints',\n",
    "       'away_ra10_allowed_turnoverPoints', 'away_ra1_steals',\n",
    "       'away_ra3_steals', 'away_ra5_steals', 'away_ra10_steals',\n",
    "       'away_ra1_allowed_steals', 'away_ra3_allowed_steals',\n",
    "       'away_ra5_allowed_steals', 'away_ra10_allowed_steals',\n",
    "       'away_ra1_blocks', 'away_ra3_blocks', 'away_ra5_blocks',\n",
    "       'away_ra10_blocks', 'away_ra1_allowed_blocks',\n",
    "       'away_ra3_allowed_blocks', 'away_ra5_allowed_blocks',\n",
    "       'away_ra10_allowed_blocks', 'away_ra1_fastBreakPoints',\n",
    "       'away_ra3_fastBreakPoints', 'away_ra5_fastBreakPoints',\n",
    "       'away_ra10_fastBreakPoints', 'away_ra1_allowed_fastBreakPoints',\n",
    "       'away_ra3_allowed_fastBreakPoints',\n",
    "       'away_ra5_allowed_fastBreakPoints',\n",
    "       'away_ra10_allowed_fastBreakPoints', 'away_ra1_turnovers',\n",
    "       'away_ra3_turnovers', 'away_ra5_turnovers', 'away_ra10_turnovers',\n",
    "       'away_ra1_allowed_turnovers', 'away_ra3_allowed_turnovers',\n",
    "       'away_ra5_allowed_turnovers', 'away_ra10_allowed_turnovers']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_tvk_features_df = kp_tvk_features_df.sort_values([\"date\",\"team_id\"], ascending=True).drop_duplicates(subset=\"team_id\", keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_team_ratings(features_df: pd.DataFrame, team_ratings_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # --- Merge home team ratings\n",
    "    merged = features_df.merge(\n",
    "        team_ratings_df.add_suffix(\"_home\"),\n",
    "        how=\"left\",\n",
    "        left_on=\"home_team_id\",\n",
    "        right_on=\"team_id_home\"\n",
    "    )\n",
    "\n",
    "    # --- Merge away team ratings\n",
    "    merged = merged.merge(\n",
    "        team_ratings_df.add_suffix(\"_away\"),\n",
    "        how=\"left\",\n",
    "        left_on=\"away_team_id\",\n",
    "        right_on=\"team_id_away\"\n",
    "    )\n",
    "\n",
    "    # --- Optional: drop duplicate key columns introduced by the merges\n",
    "    drop_cols = [\n",
    "        \"team_id_home\",\"team_id_away\"\n",
    "    ]\n",
    "    drop_cols = [c for c in drop_cols if c in merged.columns]\n",
    "    merged = merged.drop(columns=drop_cols)\n",
    "\n",
    "    return merged\n",
    "\n",
    "inference_df = merge_team_ratings(game_features, kp_tvk_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = inference_df[[\n",
    "       ## GAME INFO\n",
    "       'game_id', 'date_utc', 'season', 'neutral_site', 'home_team',\n",
    "       'home_team_id', 'conf_home', 'away_team', 'away_team_id', 'conf_away',\n",
    "       ## GAME INFO FEATURES\n",
    "       'neutral_site_flag', 'game_dow', 'game_month', 'game_dayofyear',\n",
    "       'game_weekofyear', 'game_hour', 'is_weekend',\n",
    "       ## TEAM GAME INFO FEATURES\n",
    "       'home_days_since_last', 'away_days_since_last', 'home_ra_pts_1h_1',\n",
    "       'home_ra_pts_1h_3', 'home_ra_pts_1h_5', 'home_ra_pts_1h_10',\n",
    "       'home_ra_pts_2h_1', 'home_ra_pts_2h_3', 'home_ra_pts_2h_5',\n",
    "       'home_ra_pts_2h_10', 'home_ra_pts_g_1', 'home_ra_pts_g_3',\n",
    "       'home_ra_pts_g_5', 'home_ra_pts_g_10', 'home_ra_allowed_1h_1',\n",
    "       'home_ra_allowed_1h_3', 'home_ra_allowed_1h_5',\n",
    "       'home_ra_allowed_1h_10', 'home_ra_allowed_2h_1',\n",
    "       'home_ra_allowed_2h_3', 'home_ra_allowed_2h_5',\n",
    "       'home_ra_allowed_2h_10', 'home_ra_allowed_g_1',\n",
    "       'home_ra_allowed_g_3', 'home_ra_allowed_g_5',\n",
    "       'home_ra_allowed_g_10', 'home_ra_mgn_1h_1', 'home_ra_mgn_1h_3',\n",
    "       'home_ra_mgn_1h_5', 'home_ra_mgn_1h_10', 'home_ra_mgn_2h_1',\n",
    "       'home_ra_mgn_2h_3', 'home_ra_mgn_2h_5', 'home_ra_mgn_2h_10',\n",
    "       'home_ra_mgn_g_1', 'home_ra_mgn_g_3', 'home_ra_mgn_g_5',\n",
    "       'home_ra_mgn_g_10', 'home_ra_tot_1h_1', 'home_ra_tot_1h_3',\n",
    "       'home_ra_tot_1h_5', 'home_ra_tot_1h_10', 'home_ra_tot_g_1',\n",
    "       'home_ra_tot_g_3', 'home_ra_tot_g_5', 'home_ra_tot_g_10',\n",
    "       'away_ra_pts_1h_1', 'away_ra_pts_1h_3', 'away_ra_pts_1h_5',\n",
    "       'away_ra_pts_1h_10', 'away_ra_pts_2h_1', 'away_ra_pts_2h_3',\n",
    "       'away_ra_pts_2h_5', 'away_ra_pts_2h_10', 'away_ra_pts_g_1',\n",
    "       'away_ra_pts_g_3', 'away_ra_pts_g_5', 'away_ra_pts_g_10',\n",
    "       'away_ra_allowed_1h_1', 'away_ra_allowed_1h_3',\n",
    "       'away_ra_allowed_1h_5', 'away_ra_allowed_1h_10',\n",
    "       'away_ra_allowed_2h_1', 'away_ra_allowed_2h_3',\n",
    "       'away_ra_allowed_2h_5', 'away_ra_allowed_2h_10',\n",
    "       'away_ra_allowed_g_1', 'away_ra_allowed_g_3',\n",
    "       'away_ra_allowed_g_5', 'away_ra_allowed_g_10', 'away_ra_mgn_1h_1',\n",
    "       'away_ra_mgn_1h_3', 'away_ra_mgn_1h_5', 'away_ra_mgn_1h_10',\n",
    "       'away_ra_mgn_2h_1', 'away_ra_mgn_2h_3', 'away_ra_mgn_2h_5',\n",
    "       'away_ra_mgn_2h_10', 'away_ra_mgn_g_1', 'away_ra_mgn_g_3',\n",
    "       'away_ra_mgn_g_5', 'away_ra_mgn_g_10', 'away_ra_tot_1h_1',\n",
    "       'away_ra_tot_1h_3', 'away_ra_tot_1h_5', 'away_ra_tot_1h_10',\n",
    "       'away_ra_tot_g_1', 'away_ra_tot_g_3', 'away_ra_tot_g_5',\n",
    "       'away_ra_tot_g_10', \n",
    "       ## TEAM GAME STATS FEATURES\n",
    "       'home_ra1_assists', 'home_ra3_assists',\n",
    "       'home_ra5_assists', 'home_ra10_assists',\n",
    "       'home_ra1_allowed_assists', 'home_ra3_allowed_assists',\n",
    "       'home_ra5_allowed_assists', 'home_ra10_allowed_assists',\n",
    "       'home_ra1_defensiveRebounds', 'home_ra3_defensiveRebounds',\n",
    "       'home_ra5_defensiveRebounds', 'home_ra10_defensiveRebounds',\n",
    "       'home_ra1_allowed_defensiveRebounds',\n",
    "       'home_ra3_allowed_defensiveRebounds',\n",
    "       'home_ra5_allowed_defensiveRebounds',\n",
    "       'home_ra10_allowed_defensiveRebounds', 'home_ra1_fouls',\n",
    "       'home_ra3_fouls', 'home_ra5_fouls', 'home_ra10_fouls',\n",
    "       'home_ra1_allowed_fouls', 'home_ra3_allowed_fouls',\n",
    "       'home_ra5_allowed_fouls', 'home_ra10_allowed_fouls',\n",
    "       'home_ra1_totalRebounds', 'home_ra3_totalRebounds',\n",
    "       'home_ra5_totalRebounds', 'home_ra10_totalRebounds',\n",
    "       'home_ra1_allowed_totalRebounds', 'home_ra3_allowed_totalRebounds',\n",
    "       'home_ra5_allowed_totalRebounds',\n",
    "       'home_ra10_allowed_totalRebounds', 'home_ra1_pointsInPaint',\n",
    "       'home_ra3_pointsInPaint', 'home_ra5_pointsInPaint',\n",
    "       'home_ra10_pointsInPaint', 'home_ra1_allowed_pointsInPaint',\n",
    "       'home_ra3_allowed_pointsInPaint', 'home_ra5_allowed_pointsInPaint',\n",
    "       'home_ra10_allowed_pointsInPaint', 'home_ra1_technicalFouls',\n",
    "       'home_ra3_technicalFouls', 'home_ra5_technicalFouls',\n",
    "       'home_ra10_technicalFouls', 'home_ra1_allowed_technicalFouls',\n",
    "       'home_ra3_allowed_technicalFouls',\n",
    "       'home_ra5_allowed_technicalFouls',\n",
    "       'home_ra10_allowed_technicalFouls', 'home_ra1_offensiveRebounds',\n",
    "       'home_ra3_offensiveRebounds', 'home_ra5_offensiveRebounds',\n",
    "       'home_ra10_offensiveRebounds',\n",
    "       'home_ra1_allowed_offensiveRebounds',\n",
    "       'home_ra3_allowed_offensiveRebounds',\n",
    "       'home_ra5_allowed_offensiveRebounds',\n",
    "       'home_ra10_allowed_offensiveRebounds', 'home_ra1_turnoverPoints',\n",
    "       'home_ra3_turnoverPoints', 'home_ra5_turnoverPoints',\n",
    "       'home_ra10_turnoverPoints', 'home_ra1_allowed_turnoverPoints',\n",
    "       'home_ra3_allowed_turnoverPoints',\n",
    "       'home_ra5_allowed_turnoverPoints',\n",
    "       'home_ra10_allowed_turnoverPoints', 'home_ra1_steals',\n",
    "       'home_ra3_steals', 'home_ra5_steals', 'home_ra10_steals',\n",
    "       'home_ra1_allowed_steals', 'home_ra3_allowed_steals',\n",
    "       'home_ra5_allowed_steals', 'home_ra10_allowed_steals',\n",
    "       'home_ra1_blocks', 'home_ra3_blocks', 'home_ra5_blocks',\n",
    "       'home_ra10_blocks', 'home_ra1_allowed_blocks',\n",
    "       'home_ra3_allowed_blocks', 'home_ra5_allowed_blocks',\n",
    "       'home_ra10_allowed_blocks', 'home_ra1_fastBreakPoints',\n",
    "       'home_ra3_fastBreakPoints', 'home_ra5_fastBreakPoints',\n",
    "       'home_ra10_fastBreakPoints', 'home_ra1_allowed_fastBreakPoints',\n",
    "       'home_ra3_allowed_fastBreakPoints',\n",
    "       'home_ra5_allowed_fastBreakPoints',\n",
    "       'home_ra10_allowed_fastBreakPoints', 'home_ra1_turnovers',\n",
    "       'home_ra3_turnovers', 'home_ra5_turnovers', 'home_ra10_turnovers',\n",
    "       'home_ra1_allowed_turnovers', 'home_ra3_allowed_turnovers',\n",
    "       'home_ra5_allowed_turnovers', 'home_ra10_allowed_turnovers',\n",
    "       'away_ra1_assists', 'away_ra3_assists', 'away_ra5_assists',\n",
    "       'away_ra10_assists', 'away_ra1_allowed_assists',\n",
    "       'away_ra3_allowed_assists', 'away_ra5_allowed_assists',\n",
    "       'away_ra10_allowed_assists', 'away_ra1_defensiveRebounds',\n",
    "       'away_ra3_defensiveRebounds', 'away_ra5_defensiveRebounds',\n",
    "       'away_ra10_defensiveRebounds',\n",
    "       'away_ra1_allowed_defensiveRebounds',\n",
    "       'away_ra3_allowed_defensiveRebounds',\n",
    "       'away_ra5_allowed_defensiveRebounds',\n",
    "       'away_ra10_allowed_defensiveRebounds', 'away_ra1_fouls',\n",
    "       'away_ra3_fouls', 'away_ra5_fouls', 'away_ra10_fouls',\n",
    "       'away_ra1_allowed_fouls', 'away_ra3_allowed_fouls',\n",
    "       'away_ra5_allowed_fouls', 'away_ra10_allowed_fouls',\n",
    "       'away_ra1_totalRebounds', 'away_ra3_totalRebounds',\n",
    "       'away_ra5_totalRebounds', 'away_ra10_totalRebounds',\n",
    "       'away_ra1_allowed_totalRebounds', 'away_ra3_allowed_totalRebounds',\n",
    "       'away_ra5_allowed_totalRebounds',\n",
    "       'away_ra10_allowed_totalRebounds', 'away_ra1_pointsInPaint',\n",
    "       'away_ra3_pointsInPaint', 'away_ra5_pointsInPaint',\n",
    "       'away_ra10_pointsInPaint', 'away_ra1_allowed_pointsInPaint',\n",
    "       'away_ra3_allowed_pointsInPaint', 'away_ra5_allowed_pointsInPaint',\n",
    "       'away_ra10_allowed_pointsInPaint', 'away_ra1_technicalFouls',\n",
    "       'away_ra3_technicalFouls', 'away_ra5_technicalFouls',\n",
    "       'away_ra10_technicalFouls', 'away_ra1_allowed_technicalFouls',\n",
    "       'away_ra3_allowed_technicalFouls',\n",
    "       'away_ra5_allowed_technicalFouls',\n",
    "       'away_ra10_allowed_technicalFouls', 'away_ra1_offensiveRebounds',\n",
    "       'away_ra3_offensiveRebounds', 'away_ra5_offensiveRebounds',\n",
    "       'away_ra10_offensiveRebounds',\n",
    "       'away_ra1_allowed_offensiveRebounds',\n",
    "       'away_ra3_allowed_offensiveRebounds',\n",
    "       'away_ra5_allowed_offensiveRebounds',\n",
    "       'away_ra10_allowed_offensiveRebounds', 'away_ra1_turnoverPoints',\n",
    "       'away_ra3_turnoverPoints', 'away_ra5_turnoverPoints',\n",
    "       'away_ra10_turnoverPoints', 'away_ra1_allowed_turnoverPoints',\n",
    "       'away_ra3_allowed_turnoverPoints',\n",
    "       'away_ra5_allowed_turnoverPoints',\n",
    "       'away_ra10_allowed_turnoverPoints', 'away_ra1_steals',\n",
    "       'away_ra3_steals', 'away_ra5_steals', 'away_ra10_steals',\n",
    "       'away_ra1_allowed_steals', 'away_ra3_allowed_steals',\n",
    "       'away_ra5_allowed_steals', 'away_ra10_allowed_steals',\n",
    "       'away_ra1_blocks', 'away_ra3_blocks', 'away_ra5_blocks',\n",
    "       'away_ra10_blocks', 'away_ra1_allowed_blocks',\n",
    "       'away_ra3_allowed_blocks', 'away_ra5_allowed_blocks',\n",
    "       'away_ra10_allowed_blocks', 'away_ra1_fastBreakPoints',\n",
    "       'away_ra3_fastBreakPoints', 'away_ra5_fastBreakPoints',\n",
    "       'away_ra10_fastBreakPoints', 'away_ra1_allowed_fastBreakPoints',\n",
    "       'away_ra3_allowed_fastBreakPoints',\n",
    "       'away_ra5_allowed_fastBreakPoints',\n",
    "       'away_ra10_allowed_fastBreakPoints', 'away_ra1_turnovers',\n",
    "       'away_ra3_turnovers', 'away_ra5_turnovers', 'away_ra10_turnovers',\n",
    "       'away_ra1_allowed_turnovers', 'away_ra3_allowed_turnovers',\n",
    "       'away_ra5_allowed_turnovers', 'away_ra10_allowed_turnovers',\n",
    "       ## RATING FEATURES\n",
    "       'adj_em_home','adj_em_rank_home', 'adj_oe_home', 'adj_oe_rank_home',\n",
    "       'adj_def_home', 'adj_def_rank_home', 'adj_tempo_km_home',\n",
    "       'adj_tempo_rank_home', 'rank_home', 'games_home',\n",
    "       'adj_off_eff_home', 'adj_def_eff_home', 'barthag_home',\n",
    "       'efg_pct_home', 'efgd_pct_home', 'tor_home', 'tord_home',\n",
    "       'orb_home', 'drb_home', 'ftr_home', 'ftrd_home', 'two_pt_pct_home',\n",
    "       'two_pt_def_pct_home', 'three_pt_pct_home',\n",
    "       'three_pt_def_pct_home', 'three_pt_rt_home',\n",
    "       'three_pt_def_rt_home', 'adj_tempo_tvk_home', 'wab_home',\n",
    "       'avg_height_home', 'avg_height_rank_home', 'eff_height_home',\n",
    "       'eff_height_rank_home', 'height_5_home', 'height_5_rank_home',\n",
    "       'height_4_home', 'height_4_rank_home', 'height_3_home',\n",
    "       'height_3_rank_home', 'height_2_home', 'height_2_rank_home',\n",
    "       'height_1_home', 'height_1_rank_home', 'exp_home', 'exp_rank_home',\n",
    "       'bench_home', 'bench_rank_home', 'continuity_home',\n",
    "       'continuity_rank_home','adj_em_away', 'adj_em_rank_away', 'adj_oe_away',\n",
    "       'adj_oe_rank_away', 'adj_def_away', 'adj_def_rank_away',\n",
    "       'adj_tempo_km_away', 'adj_tempo_rank_away', 'rank_away',\n",
    "       'games_away', 'adj_off_eff_away', 'adj_def_eff_away',\n",
    "       'barthag_away', 'efg_pct_away', 'efgd_pct_away', 'tor_away',\n",
    "       'tord_away', 'orb_away', 'drb_away', 'ftr_away', 'ftrd_away',\n",
    "       'two_pt_pct_away', 'two_pt_def_pct_away', 'three_pt_pct_away',\n",
    "       'three_pt_def_pct_away', 'three_pt_rt_away',\n",
    "       'three_pt_def_rt_away', 'adj_tempo_tvk_away', 'wab_away',\n",
    "       'avg_height_away', 'avg_height_rank_away', 'eff_height_away',\n",
    "       'eff_height_rank_away', 'height_5_away', 'height_5_rank_away',\n",
    "       'height_4_away', 'height_4_rank_away', 'height_3_away',\n",
    "       'height_3_rank_away', 'height_2_away', 'height_2_rank_away',\n",
    "       'height_1_away', 'height_1_rank_away', 'exp_away', 'exp_rank_away',\n",
    "       'bench_away', 'bench_rank_away', 'continuity_away',\n",
    "       'continuity_rank_away']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load as joblib_load\n",
    "from typing import List\n",
    "\n",
    "# Where your per-target artifacts live\n",
    "OUT_DIR = \"../predicting/training_reports\"  # where models & preprocessors were saved\n",
    "\n",
    "TARGETS: List[str] = [\n",
    "    \"home_1h\",\"away_1h\",\"home_2h\",\"away_2h\",\"home_score\",\"away_score\",\n",
    "    \"total\",\"1h_total\",\"2h_total\",\"margin\",\"1h_margin\",\"2h_margin\"\n",
    "]\n",
    "\n",
    "# Final view columns you requested\n",
    "ID_VIEW = [\"game_id\",\"date_utc\",\"home_team\",\"home_team_id\",\"away_team\",\"away_team_id\"]\n",
    "\n",
    "# Columns that should never be fed as raw features (mirrors your training)\n",
    "DATE_COL = \"date_utc\"\n",
    "RAW_TIME_COLS_TO_EXCLUDE = [\n",
    "    DATE_COL, \"season\", \"game_datetime_utc\", \"game_datetime_local\",\n",
    "    \"date_home\", \"date_away\"\n",
    "]\n",
    "ID_COLS = [\"game_id\", \"home_team_id\", \"away_team_id\"]\n",
    "\n",
    "def _robust_joblib_load(path: str):\n",
    "    \"\"\"\n",
    "    Load an artifact and gracefully handle sklearn private symbol changes by\n",
    "    monkey-patching missing names commonly hit when unpickling ColumnTransformer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return joblib_load(path)\n",
    "    except AttributeError as e:\n",
    "        # Attempt to patch common missing private symbol(s)\n",
    "        try:\n",
    "            from sklearn.compose import _column_transformer as _ct\n",
    "            # Newer sklearn introduced _RemainderColsList; on older versions it's missing.\n",
    "            if not hasattr(_ct, \"_RemainderColsList\"):\n",
    "                class _RemainderColsList(list):\n",
    "                    pass\n",
    "                _ct._RemainderColsList = _RemainderColsList\n",
    "            # Retry after patch\n",
    "            return joblib_load(path)\n",
    "        except Exception:\n",
    "            # Re-raise original for clarity if patch fails\n",
    "            raise e\n",
    "\n",
    "def _load_artifact(target: str):\n",
    "    path = os.path.join(OUT_DIR, target, f\"{target}_final_model.joblib\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Missing artifact for {target}: {path}\")\n",
    "    art = _robust_joblib_load(path)\n",
    "    for k in [\"preprocessor\",\"model\",\"features\"]:\n",
    "        if k not in art:\n",
    "            raise ValueError(f\"Artifact for {target} missing key: '{k}'\")\n",
    "    return art\n",
    "\n",
    "def _ensure_columns(df: pd.DataFrame, expected_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Add missing expected columns as NaN, drop extras, enforce expected order.\"\"\"\n",
    "    X = df.copy()\n",
    "    for c in expected_cols:\n",
    "        if c not in X.columns:\n",
    "            X[c] = np.nan\n",
    "    return X[expected_cols]\n",
    "\n",
    "def predict_with_artifacts(inference_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Guard: ID columns for final view\n",
    "    missing_ids = [c for c in ID_VIEW if c not in inference_df.columns]\n",
    "    if missing_ids:\n",
    "        raise KeyError(f\"inference_df missing ID/display columns: {missing_ids}\")\n",
    "\n",
    "    preds = inference_df[ID_VIEW].copy()\n",
    "\n",
    "    for target in TARGETS:\n",
    "        print(f\"[predict] {target} …\")\n",
    "        art = _load_artifact(target)\n",
    "        pre = art[\"preprocessor\"]       # fitted ColumnTransformer from training\n",
    "        model = art[\"model\"]            # fitted LightGBMRegressor\n",
    "        feat_cols = art[\"features\"]     # exact raw feature list used to fit pre\n",
    "\n",
    "        # Align inference to training feature list\n",
    "        X_raw = _ensure_columns(inference_df, feat_cols)\n",
    "\n",
    "        # Transform & predict\n",
    "        X_t = pre.transform(X_raw)\n",
    "\n",
    "        # Optional sanity: check feature count if model exposes it\n",
    "        n_model = getattr(model, \"n_features_in_\", None)\n",
    "        if n_model is not None and X_t.shape[1] != n_model:\n",
    "            raise ValueError(\n",
    "                f\"[{target}] Feature count mismatch: model expects {n_model}, got {X_t.shape[1]}.\\n\"\n",
    "                f\"Check that you are using the same preprocessor and feature set as training.\"\n",
    "            )\n",
    "\n",
    "        preds[f\"pred_{target}\"] = model.predict(X_t).astype(float)\n",
    "\n",
    "    # Sort by date for readability (best-effort)\n",
    "    try:\n",
    "        preds = preds.sort_values(\"date_utc\").reset_index(drop=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[predict] home_1h …\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.442322575734447e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.442322575734447e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9045551296476547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9045551296476547\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7296890365747699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7296890365747699\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.891030945934824e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.891030945934824e-07\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[predict] away_1h …\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.442322575734447e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.442322575734447e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9045551296476547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9045551296476547\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7296890365747699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7296890365747699\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.891030945934824e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.891030945934824e-07\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[predict] home_2h …\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.442322575734447e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.442322575734447e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9045551296476547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9045551296476547\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7296890365747699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7296890365747699\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.891030945934824e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.891030945934824e-07\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[predict] away_2h …\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.442322575734447e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.442322575734447e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9045551296476547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9045551296476547\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7296890365747699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7296890365747699\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.891030945934824e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.891030945934824e-07\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[predict] home_score …\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.442322575734447e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.442322575734447e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9045551296476547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9045551296476547\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7296890365747699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7296890365747699\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.891030945934824e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.891030945934824e-07\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[predict] away_score …\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.442322575734447e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.442322575734447e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9045551296476547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9045551296476547\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7296890365747699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7296890365747699\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.891030945934824e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.891030945934824e-07\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[predict] total …\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.442322575734447e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.442322575734447e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9045551296476547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9045551296476547\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7296890365747699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7296890365747699\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.891030945934824e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.891030945934824e-07\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[predict] 1h_total …\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.442322575734447e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.442322575734447e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9045551296476547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9045551296476547\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7296890365747699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7296890365747699\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.891030945934824e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.891030945934824e-07\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[predict] 2h_total …\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.442322575734447e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.442322575734447e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9045551296476547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9045551296476547\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7296890365747699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7296890365747699\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.891030945934824e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.891030945934824e-07\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[predict] margin …\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.442322575734447e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.442322575734447e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9045551296476547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9045551296476547\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7296890365747699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7296890365747699\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.891030945934824e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.891030945934824e-07\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[predict] 1h_margin …\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.442322575734447e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.442322575734447e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9045551296476547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9045551296476547\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7296890365747699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7296890365747699\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.891030945934824e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.891030945934824e-07\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[predict] 2h_margin …\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.442322575734447e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.442322575734447e-07\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9045551296476547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9045551296476547\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7296890365747699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7296890365747699\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.891030945934824e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.891030945934824e-07\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n"
     ]
    }
   ],
   "source": [
    "final_predictions = predict_with_artifacts(inference_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"20251109\"\n",
    "game_ids = list(pd.read_csv(f\"s3://collegebasketballinsiders/daily-box-score-ids/{date}/game_ids.csv\")['game_id'])\n",
    "\n",
    "final_predictions = final_predictions[final_predictions['game_id'].isin(game_ids)]\n",
    "final_predictions.to_csv(f\"s3://collegebasketballinsiders/predictions/{date}/preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7d2c12d5ae1b736389bb589262edf81367667a744944aadb10838f171dc1188"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
